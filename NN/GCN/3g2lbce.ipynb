{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [15:44:41] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "[15:44:41] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# sklearn, condusion matrix, mcc and auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "# build dataset\n",
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch_geometric.utils import from_networkx\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# random\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jgl/2Tdisk/som/Zaretzki/GCN\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = SummaryWriter(\"./runs/3g2lbceloss/train\")\n",
    "val_writer = SummaryWriter(\"./runs/3g2lbceloss/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = {\n",
    "    'C':[1,0,0,0,0,0,0,0,0,0],\n",
    "    'N':[0,1,0,0,0,0,0,0,0,0],\n",
    "    'O':[0,0,1,0,0,0,0,0,0,0],\n",
    "    'F':[0,0,0,1,0,0,0,0,0,0],\n",
    "    'P':[0,0,0,0,1,0,0,0,0,0],\n",
    "    'S':[0,0,0,0,0,1,0,0,0,0],\n",
    "    'Cl':[0,0,0,0,0,0,1,0,0,0],\n",
    "    'Br':[0,0,0,0,0,0,0,1,0,0],\n",
    "    'I':[0,0,0,0,0,0,0,0,1,0],\n",
    "    'other':[0,0,0,0,0,0,0,0,0,1],\n",
    "}\n",
    "\n",
    "zero_five = {\n",
    "    0:[1,0,0,0,0,0],\n",
    "    1:[0,1,0,0,0,0],\n",
    "    2:[0,0,1,0,0,0],\n",
    "    3:[0,0,0,1,0,0],\n",
    "    4:[0,0,0,0,1,0],\n",
    "    5:[0,0,0,0,0,1]\n",
    "}\n",
    "\n",
    "num_H = {\n",
    "    0:[1,0,0,0,0],\n",
    "    1:[0,1,0,0,0],\n",
    "    2:[0,0,1,0,0],\n",
    "    3:[0,0,0,1,0],\n",
    "    4:[0,0,0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph(mol):\n",
    "    # mol = Chem.MolFromSmiles(smiles)\n",
    "    # mol = add_atom_index(mol)\n",
    "    # graph\n",
    "    g = nx.Graph()\n",
    "    for atom in mol.GetAtoms():\n",
    "        # atom number\n",
    "        idx = atom.GetIdx()\n",
    "        # print(idx)\n",
    "        feature = []\n",
    "        # identity one-hot 10\n",
    "        feature.extend(identity.get(atom.GetSymbol(),[0,0,0,0,0,0,0,0,0,1]))\n",
    "        # degree of atom one-hot 6\n",
    "        feature.extend(zero_five[atom.GetDegree()])\n",
    "        # number of hydrogen atoms attached one-hot 5\n",
    "        feature.extend(num_H[atom.GetNumImplicitHs()])\n",
    "        # implicit valence electrons one-hot 6\n",
    "        feature.extend(zero_five[atom.GetImplicitValence()])\n",
    "        # aromatic 0 or 1\n",
    "        if atom.GetIsAromatic():\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "        # total feature 28d\n",
    "        g.add_node(idx, feature=feature)\n",
    "    # add edge\n",
    "    bonds_info = [(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()) for bond in mol.GetBonds()]\n",
    "    # add self_loop\n",
    "    for atom in mol.GetAtoms():\n",
    "        bonds_info.append((atom.GetIdx(), atom.GetIdx()))\n",
    "    g.add_edges_from(bonds_info)\n",
    "    # print(g.nodes.data)\n",
    "    return g\n",
    "\n",
    "\n",
    "def mol2y(mol):\n",
    "    _y = []\n",
    "    som = ['PRIMARY_SOM_1A2', 'PRIMARY_SOM_2A6','PRIMARY_SOM_2B6','PRIMARY_SOM_2C8','PRIMARY_SOM_2C9','PRIMARY_SOM_2C19','PRIMARY_SOM_2D6','PRIMARY_SOM_2E1','PRIMARY_SOM_3A4',\n",
    "           'SECONDARY_SOM_1A2', 'SECONDARY_SOM_2A6','SECONDARY_SOM_2B6','SECONDARY_SOM_2C8','SECONDARY_SOM_2C9','SECONDARY_SOM_2C19','SECONDARY_SOM_2D6','SECONDARY_SOM_2E1','SECONDARY_SOM_3A4',\n",
    "           'TERTIARY_SOM_1A2', 'TERTIARY_SOM_2A6','TERTIARY_SOM_2B6','TERTIARY_SOM_2C8','TERTIARY_SOM_2C9','TERTIARY_SOM_2C19','TERTIARY_SOM_2D6','TERTIARY_SOM_2E1','TERTIARY_SOM_3A4'\n",
    "          ]\n",
    "    result = []\n",
    "    for k in som:\n",
    "        try:\n",
    "            _res = mol.GetProp(k)\n",
    "            if ' ' in _res:\n",
    "                res = _res.split(' ')\n",
    "                for s in res:\n",
    "                    result.append(int(s))\n",
    "                # res = [int(temp) for temp in res]\n",
    "            else:\n",
    "                # res = [int(_res)]\n",
    "                result.append(int(_res))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for data in result:\n",
    "        _y.append(data)\n",
    "    _y = list(set(_y))\n",
    "\n",
    "    y = np.zeros(len(mol.GetAtoms()))\n",
    "    for i in _y:\n",
    "        y[i-1] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = Chem.SDMolSupplier('../../raw_database/merged.sdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for mol in mols:\n",
    "    g = mol2graph(mol)\n",
    "    y = mol2y(mol)\n",
    "    graph = from_networkx(g)\n",
    "    graph.feature = graph.feature.float()\n",
    "    label = torch.tensor(y, dtype=torch.float)\n",
    "    dataset.append((g, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed('42')\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(dataset)\n",
    "ratio = 0.8\n",
    "training_set = dataset[:int(total * 0.8)]\n",
    "test_set = dataset[int(total * 0.8):]\n",
    "validation_set = training_set[int(len(training_set) * 0.8):]\n",
    "training_set = training_set[:int(len(training_set) * 0.8)]\n",
    "# test_set = lea[int(len(lea) * 0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 136, 109)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set), len(test_set), len(validation_set)\n",
    "# , len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntimecrossvalidation(n=5):\n",
    "    seeds = [1,2,3,4,5]\n",
    "    all_records = []\n",
    "    for seed in seeds:\n",
    "        records = crossvalidation(args, training_set, 5, seed)\n",
    "        all_records.append(records)\n",
    "    pickle.dump(all_records, open('./all_records.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(args, trainingSet, k, seed):\n",
    "    # split training set to k fold\n",
    "    random.seed(seed)\n",
    "    random.shuffle(trainingSet)\n",
    "    splits = [0, 109, 218, 327, 436, 545]\n",
    "    fold0 = trainingSet[:109]\n",
    "    fold1 = trainingSet[109:218]\n",
    "    fold2 = trainingSet[218:327]\n",
    "    fold3 = trainingSet[327:436]\n",
    "    fold4 = trainingSet[436:]\n",
    "    folds = [fold0, fold1, fold2, fold3, fold4]\n",
    "    records = []\n",
    "    for i in range(5):\n",
    "        val_set = folds[i]\n",
    "        _tr_set = folds[0:i] + folds[i+1:]\n",
    "        tr_set = [y for x in _tr_set for y in x]\n",
    "        record = main(args, tr_set, val_set, seed, i)\n",
    "        records.append(record)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = GCNConv(28, 1024)\n",
    "        self.conv2 = GCNConv(1024, 1024)\n",
    "        self.conv3 = GCNConv(1024, 1024)\n",
    "        self.linear1 = nn.Linear(1024, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "# self.conv4 = GCNConv(1024, 1024)\n",
    "    # self.linear2 = nn.Linear(224, 1)\n",
    "    def forward(self, mol):\n",
    "        res = self.conv1(mol.feature, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv2(res, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv3(res, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        # res = self.relu(self.conv4(res, mol.edge_index))\n",
    "        # res = self.relu(self.conv5(res, mol.edge_index))\n",
    "        res = self.linear1(res)\n",
    "        # res = self.relu(res)\n",
    "        # # res = self.drop1(res)\n",
    "        # res = self.linear2(res)\n",
    "        # res = self.dropout(res)\n",
    "        return res\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.2)\n",
    "        # self.ln1 = nn.LayerNorm(1024)\n",
    "        # self.ln2 = nn.LayerNorm(1024)\n",
    "        # self.ln3 =nn.LayerNorm(1024)\n",
    "        # self.bn3 = nn.BatchNorm1d(112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def top2(output, label):\n",
    "    preds = torch.sigmoid(output)\n",
    "    _, indices = torch.topk(preds, 2)\n",
    "    pos_index = []\n",
    "    for i in range(label.shape[0]):\n",
    "        if label[i] == 1:\n",
    "            pos_index.append(i)\n",
    "    # print(pos_index)      \n",
    "    for li in pos_index:\n",
    "        if li in indices:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def MCC(output, label):\n",
    "    tn,fp,fn,tp=confusion_matrix(label, output).ravel()\n",
    "    print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "    up = (tp * tn) - (fp * fn)\n",
    "    down = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    return up / down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, training_set, optimizer, criterion, epoch, record):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    for mol, target in training_set:\n",
    "        mol, target = mol.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        # squeeze\n",
    "        output = torch.squeeze(output)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        total_loss += loss.item()\n",
    "        all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "        all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    train_writer.add_scalar('Ave Loss', total_loss / len(training_set), epoch)\n",
    "    train_writer.add_scalar('ACC', accuracy_score(all_labels, all_pred), epoch)\n",
    "    train_writer.add_scalar('Top2', top2n / len(training_set), epoch)\n",
    "    train_writer.add_scalar('AUC', roc_auc_score(all_labels, all_pred_raw), epoch)\n",
    "    train_writer.add_scalar('MCC', mcc, epoch)\n",
    "    record['train_loss'].append(total_loss / len(training_set))\n",
    "    record['train_acc'].append(accuracy_score(all_labels, all_pred))\n",
    "    record['train_top2'].append(top2n / len(training_set))\n",
    "    record['train_auc'].append(roc_auc_score(all_labels, all_pred_raw))\n",
    "    record['train_mcc'].append(mcc)\n",
    "    # loss_record['train'].append(total_loss / len(training_set))\n",
    "    print(f'Train Epoch: {epoch}, Ave Loss: {total_loss / len(training_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(training_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, model, device, val_set, optimizer, criterion, epoch, record):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    for mol, target in val_set:\n",
    "        mol, target = mol.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        # squeeze\n",
    "        output = torch.squeeze(output)\n",
    "        loss = criterion(output, target)\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        total_loss += loss.item()\n",
    "        all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "        all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    val_writer.add_scalar('Ave Loss', total_loss / len(val_set), epoch)\n",
    "    val_writer.add_scalar('ACC', accuracy_score(all_labels, all_pred), epoch)\n",
    "    val_writer.add_scalar('Top2', top2n / len(val_set), epoch)\n",
    "    val_writer.add_scalar('AUC', roc_auc_score(all_labels, all_pred_raw), epoch)\n",
    "    val_writer.add_scalar('MCC', mcc, epoch)\n",
    "    record['val_loss'].append(total_loss / len(val_set))\n",
    "    record['val_acc'].append(accuracy_score(all_labels, all_pred))\n",
    "    record['val_top2'].append(top2n / len(val_set))\n",
    "    record['val_auc'].append(roc_auc_score(all_labels, all_pred_raw))\n",
    "    record['val_mcc'].append(mcc)\n",
    "    # loss_record['dev'].append(total_loss / len(val_set))\n",
    "    print(f'Val Epoch: {epoch}, Ave Loss: {total_loss / len(val_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(val_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')\n",
    "    return top2n / len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_set, record):\n",
    "    model.eval()\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    with torch.no_grad():\n",
    "        for mol, target in test_set:\n",
    "            mol, target = mol.to(device), target.to(device)\n",
    "            output = model(mol)\n",
    "            # squeeze\n",
    "            output = torch.squeeze(output)\n",
    "            # tracking\n",
    "            top2n += top2(output, target)\n",
    "            all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "            all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "            all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    record['test_acc'].append(accuracy_score(all_labels, all_pred))\n",
    "    record['test_top2'].append(top2n / len(test_set))\n",
    "    record['test_auc'].append(roc_auc_score(all_labels, all_pred_raw))\n",
    "    record['test_mcc'].append(mcc)\n",
    "    print(f'ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(test_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, training_set, validation_set, seed, i):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(args['seed'])\n",
    "    model = Model().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'], weight_decay=args['weight_decay'])\n",
    "    criterion = nn.BCEWithLogitsLoss(torch.tensor(args['pos_weight']).to(device))\n",
    "    max_top2 = 0\n",
    "    record = {\n",
    "    'train_loss':[],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_top2':[],\n",
    "    'val_top2':[],\n",
    "    'train_auc':[],\n",
    "    'val_auc':[],\n",
    "    'train_mcc':[],\n",
    "    'val_mcc':[],\n",
    "    'test_acc':[],\n",
    "    'test_top2':[],\n",
    "    'test_auc':[],\n",
    "    'test_mcc':[]\n",
    "}\n",
    "    for epoch in range(1, args['epoch'] + 1):\n",
    "        train(args, model, device, training_set, optimizer, criterion, epoch, record)\n",
    "        top2acc = val(args, model, device, validation_set, optimizer, criterion, epoch, record)\n",
    "        if top2acc > max_top2:\n",
    "            max_top2 = top2acc\n",
    "            print('Saving model (epoch = {:4d}, top2acc = {:.4f})'\n",
    "                .format(epoch, max_top2))\n",
    "            torch.save(model.state_dict(), args['save_path'] + '_' + str(seed) + '_' + str(i))\n",
    "    model = Model().to(device)\n",
    "    model.load_state_dict(torch.load(args['save_path'] + '_' + str(seed) + '_' + str(i)))\n",
    "    test(model, device, test_set, record)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 0.01,\n",
    "    'epoch': 1000,\n",
    "    'seed': 100,\n",
    "    'save_path': './model/model',\n",
    "    'momentum':0.9,\n",
    "    'weight_decay': 1e-6,\n",
    "    'pos_weight': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntimecrossvalidation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jgl/2Tdisk/som/Zaretzki/GCN\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, model, device, val_set, optimizer, criterion, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    for mol, target in val_set:\n",
    "        mol, target = mol.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        # squeeze\n",
    "        output = torch.squeeze(output)\n",
    "        # output.squeeze_(1)\n",
    "        loss = criterion(output, target)\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        total_loss += loss.item()\n",
    "        all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "        all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    val_writer.add_scalar('Ave Loss', total_loss / len(val_set), epoch)\n",
    "    val_writer.add_scalar('ACC', accuracy_score(all_labels, all_pred), epoch)\n",
    "    val_writer.add_scalar('Top2', top2n / len(val_set), epoch)\n",
    "    val_writer.add_scalar('AUC', roc_auc_score(all_labels, all_pred_raw), epoch)\n",
    "    val_writer.add_scalar('MCC', mcc, epoch)\n",
    "    print(f'Val Epoch: {epoch}, Ave Loss: {total_loss / len(val_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(val_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')\n",
    "    return top2n / len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, training_set, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    # training_set.extend(validation_set)\n",
    "    for mol, target in training_set:\n",
    "        mol, target = mol.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        # squeeze\n",
    "        output = torch.squeeze(output)\n",
    "        # output.squeeze_(1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        total_loss += loss.item()\n",
    "        all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "        all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    train_writer.add_scalar('Ave Loss', total_loss / len(training_set), epoch)\n",
    "    train_writer.add_scalar('ACC', accuracy_score(all_labels, all_pred), epoch)\n",
    "    train_writer.add_scalar('Top2', top2n / len(training_set), epoch)\n",
    "    train_writer.add_scalar('AUC', roc_auc_score(all_labels, all_pred_raw), epoch)\n",
    "    train_writer.add_scalar('MCC', mcc, epoch)\n",
    "    print(f'Train Epoch: {epoch}, Ave Loss: {total_loss / len(training_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(training_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(args['seed'])\n",
    "    model = Model().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'], weight_decay=args['weight_decay'])\n",
    "    criterion = nn.BCEWithLogitsLoss(torch.tensor(args['pos_weight']))\n",
    "    max_top2 = 0\n",
    "    for epoch in range(1, args['epoch'] + 1):\n",
    "        train(args, model, device, training_set, optimizer, criterion, epoch)\n",
    "        top2acc = val(args, model, device, validation_set, optimizer, criterion, epoch)\n",
    "        random.shuffle(training_set)\n",
    "        if top2acc > max_top2:\n",
    "            max_top2 = top2acc\n",
    "            print('Saving model (epoch = {:4d}, top2acc = {:.4f})'\n",
    "                .format(epoch, max_top2))\n",
    "            torch.save(model.state_dict(), args['save_path'])\n",
    "    model.load_state_dict(torch.load(args['save_path']))\n",
    "    test(model, device, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 0.01,\n",
    "    'epoch':500,\n",
    "    'seed': 100,\n",
    "    'save_path': './model/model',\n",
    "    'momentum':0.9,\n",
    "    'weight_decay': 1e-7,\n",
    "    'pos_weight': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 8404, FP: 173, FN: 283, TP: 763\n",
      "Train Epoch: 442, Ave Loss: 0.33422599586909035 ACC: 0.9526135300841734 Top2: 0.960919540229885 AUC: 0.9815875019032403 MCC: 0.7450330497099723\n",
      "TN: 2169, FP: 71, FN: 158, TP: 119\n",
      "Val Epoch: 442, Ave Loss: 1.3193886621968411 ACC: 0.9090186730234406 Top2: 0.8440366972477065 AUC: 0.8653252320783908 MCC: 0.47137863126357243\n",
      "TN: 8408, FP: 169, FN: 300, TP: 746\n",
      "Train Epoch: 443, Ave Loss: 0.34123600212617294 ACC: 0.9512626000207836 Top2: 0.9540229885057471 AUC: 0.9803901046219257 MCC: 0.7358741176706742\n",
      "TN: 2161, FP: 79, FN: 147, TP: 130\n",
      "Val Epoch: 443, Ave Loss: 1.3041883744203289 ACC: 0.9102105681366707 Top2: 0.7798165137614679 AUC: 0.8747985430634349 MCC: 0.49227618818992824\n",
      "TN: 8421, FP: 156, FN: 282, TP: 764\n",
      "Train Epoch: 444, Ave Loss: 0.31747935995721827 ACC: 0.9544840486334822 Top2: 0.9586206896551724 AUC: 0.9831219092548416 MCC: 0.7539026208002072\n",
      "TN: 2167, FP: 73, FN: 151, TP: 126\n",
      "Val Epoch: 444, Ave Loss: 1.4023154385541694 ACC: 0.911005164878824 Top2: 0.8073394495412844 AUC: 0.8708064724084579 MCC: 0.48976262176701\n",
      "TN: 8403, FP: 174, FN: 269, TP: 777\n",
      "Train Epoch: 445, Ave Loss: 0.3286510375314563 ACC: 0.9539644601475631 Top2: 0.9701149425287356 AUC: 0.9814216998593999 MCC: 0.75361122877843\n",
      "TN: 2157, FP: 83, FN: 152, TP: 125\n",
      "Val Epoch: 445, Ave Loss: 1.3720493275499177 ACC: 0.9066348827969806 Top2: 0.7889908256880734 AUC: 0.8771418901495616 MCC: 0.47080471568322185\n",
      "TN: 8439, FP: 138, FN: 262, TP: 784\n",
      "Train Epoch: 446, Ave Loss: 0.2962922826867853 ACC: 0.9584329211264678 Top2: 0.9632183908045977 AUC: 0.9846404887810813 MCC: 0.7756103958671184\n",
      "TN: 2156, FP: 84, FN: 147, TP: 130\n",
      "Val Epoch: 446, Ave Loss: 1.3323237747274372 ACC: 0.9082240762812872 Top2: 0.7798165137614679 AUC: 0.8793587222795255 MCC: 0.4845145646117334\n",
      "TN: 8415, FP: 162, FN: 270, TP: 776\n",
      "Train Epoch: 447, Ave Loss: 0.2932503963044241 ACC: 0.9551075548165853 Top2: 0.9655172413793104 AUC: 0.9857210722526852 MCC: 0.758712350400071\n",
      "TN: 2158, FP: 82, FN: 142, TP: 135\n",
      "Val Epoch: 447, Ave Loss: 1.4493061343315827 ACC: 0.911005164878824 Top2: 0.7981651376146789 AUC: 0.8731796351211965 MCC: 0.5025890811004464\n",
      "TN: 8411, FP: 166, FN: 246, TP: 800\n",
      "Train Epoch: 448, Ave Loss: 0.2971272305801674 ACC: 0.9571859087602619 Top2: 0.9839080459770115 AUC: 0.9851365016181165 MCC: 0.7721258766722657\n",
      "TN: 2151, FP: 89, FN: 143, TP: 134\n",
      "Val Epoch: 448, Ave Loss: 1.3686875375906227 ACC: 0.9078267779102106 Top2: 0.8165137614678899 AUC: 0.8761595861268695 MCC: 0.4890121004621025\n",
      "TN: 8436, FP: 141, FN: 251, TP: 795\n",
      "Train Epoch: 449, Ave Loss: 0.28600399368602436 ACC: 0.9592642627039385 Top2: 0.9632183908045977 AUC: 0.9859067705417865 MCC: 0.7810871017857592\n",
      "TN: 2142, FP: 98, FN: 161, TP: 116\n",
      "Val Epoch: 449, Ave Loss: 1.4727728100207318 ACC: 0.8970997218911403 Top2: 0.7522935779816514 AUC: 0.8650020951521402 MCC: 0.42079196250452616\n",
      "TN: 8414, FP: 163, FN: 293, TP: 753\n",
      "Train Epoch: 450, Ave Loss: 0.3115144752257237 ACC: 0.9526135300841734 Top2: 0.9517241379310345 AUC: 0.9835201685507352 MCC: 0.7433542050602898\n",
      "TN: 2146, FP: 94, FN: 144, TP: 133\n",
      "Val Epoch: 450, Ave Loss: 1.33893257861 ACC: 0.9054429876837505 Top2: 0.8256880733944955 AUC: 0.8753722924187726 MCC: 0.47872476267641084\n",
      "TN: 8434, FP: 143, FN: 266, TP: 780\n",
      "Train Epoch: 451, Ave Loss: 0.2947981981013427 ACC: 0.9574976618518134 Top2: 0.9793103448275862 AUC: 0.9845167084989404 MCC: 0.7705764473047101\n",
      "TN: 2126, FP: 114, FN: 129, TP: 148\n",
      "Val Epoch: 451, Ave Loss: 1.5166463337180742 ACC: 0.9034564958283671 Top2: 0.8073394495412844 AUC: 0.8678813176895306 MCC: 0.49539254932594823\n",
      "TN: 8410, FP: 167, FN: 249, TP: 797\n",
      "Train Epoch: 452, Ave Loss: 0.2855387508894044 ACC: 0.9567702379715266 Top2: 0.967816091954023 AUC: 0.9863919156818303 MCC: 0.7697429000163473\n",
      "TN: 2139, FP: 101, FN: 139, TP: 138\n",
      "Val Epoch: 452, Ave Loss: 1.4349638266967388 ACC: 0.9046483909415971 Top2: 0.8165137614678899 AUC: 0.8782740781330584 MCC: 0.48371278984265637\n",
      "TN: 8382, FP: 195, FN: 276, TP: 770\n",
      "Train Epoch: 453, Ave Loss: 0.412651489454134 ACC: 0.9510547646264159 Top2: 0.9471264367816092 AUC: 0.9754265208812488 MCC: 0.739257421997399\n",
      "TN: 2164, FP: 76, FN: 166, TP: 111\n",
      "Val Epoch: 453, Ave Loss: 1.6470703383600083 ACC: 0.9038537941994438 Top2: 0.7981651376146789 AUC: 0.8622348826714801 MCC: 0.43771023500186107\n",
      "TN: 8376, FP: 201, FN: 399, TP: 647\n",
      "Train Epoch: 454, Ave Loss: 0.507564282982563 ACC: 0.9376493816897018 Top2: 0.9172413793103448 AUC: 0.9531571049881948 MCC: 0.6534476838046935\n",
      "TN: 2137, FP: 103, FN: 154, TP: 123\n",
      "Val Epoch: 454, Ave Loss: 1.3609000997438412 ACC: 0.8978943186332936 Top2: 0.8073394495412844 AUC: 0.8652027462609593 MCC: 0.43575959026861144\n",
      "TN: 8380, FP: 197, FN: 355, TP: 691\n",
      "Train Epoch: 455, Ave Loss: 0.4565326343215741 ACC: 0.9426374311545256 Top2: 0.9310344827586207 AUC: 0.9632088887283814 MCC: 0.6857621535268323\n",
      "TN: 2162, FP: 78, FN: 148, TP: 129\n",
      "Val Epoch: 455, Ave Loss: 1.161798748059557 ACC: 0.9102105681366707 Top2: 0.7981651376146789 AUC: 0.8716598439917482 MCC: 0.49083059946867735\n",
      "TN: 8395, FP: 182, FN: 317, TP: 729\n",
      "Train Epoch: 456, Ave Loss: 0.40272854280479786 ACC: 0.9481450691052686 Top2: 0.9563218390804598 AUC: 0.9724574103314682 MCC: 0.7184275217274848\n",
      "TN: 2131, FP: 109, FN: 147, TP: 130\n",
      "Val Epoch: 456, Ave Loss: 1.1159540557114562 ACC: 0.8982916170043703 Top2: 0.7981651376146789 AUC: 0.8725180505415162 MCC: 0.4490683536081717\n",
      "TN: 8419, FP: 158, FN: 308, TP: 738\n",
      "Train Epoch: 457, Ave Loss: 0.34407763993677826 ACC: 0.9515743531123351 Top2: 0.960919540229885 AUC: 0.979481342226342 MCC: 0.7360069289825478\n",
      "TN: 2161, FP: 79, FN: 141, TP: 136\n",
      "Val Epoch: 457, Ave Loss: 1.229871544101032 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.8639440433212997 MCC: 0.5102436186459821\n",
      "TN: 8433, FP: 144, FN: 280, TP: 766\n",
      "Train Epoch: 458, Ave Loss: 0.30649200002194227 ACC: 0.9559388963940559 Top2: 0.9563218390804598 AUC: 0.9831262006018586 MCC: 0.7611206342303262\n",
      "TN: 2108, FP: 132, FN: 124, TP: 153\n",
      "Val Epoch: 458, Ave Loss: 1.230012337224336 ACC: 0.8982916170043703 Top2: 0.8073394495412844 AUC: 0.866322846828262 MCC: 0.4873145250115146\n",
      "TN: 8391, FP: 186, FN: 245, TP: 801\n",
      "Train Epoch: 459, Ave Loss: 0.2948223480308583 ACC: 0.9552114725137691 Top2: 0.967816091954023 AUC: 0.9850202451262001 MCC: 0.7633843137876988\n",
      "TN: 2145, FP: 95, FN: 157, TP: 120\n",
      "Val Epoch: 459, Ave Loss: 1.3223075451363528 ACC: 0.899880810488677 Top2: 0.7889908256880734 AUC: 0.8743714543579164 MCC: 0.4375715680509093\n",
      "TN: 8442, FP: 135, FN: 284, TP: 762\n",
      "Train Epoch: 460, Ave Loss: 0.2967970181118867 ACC: 0.9564584848799751 Top2: 0.9770114942528736 AUC: 0.9847950330054744 MCC: 0.763074206136804\n",
      "TN: 2134, FP: 106, FN: 133, TP: 144\n",
      "Val Epoch: 460, Ave Loss: 1.3127594160518412 ACC: 0.9050456893126738 Top2: 0.8256880733944955 AUC: 0.8738702294997421 MCC: 0.49442605102996356\n",
      "TN: 8420, FP: 157, FN: 267, TP: 779\n",
      "Train Epoch: 461, Ave Loss: 0.2906402446007648 ACC: 0.9559388963940559 Top2: 0.967816091954023 AUC: 0.9850734689755674 MCC: 0.7630600757478657\n",
      "TN: 2127, FP: 113, FN: 143, TP: 134\n",
      "Val Epoch: 461, Ave Loss: 1.3267942703480686 ACC: 0.8982916170043703 Top2: 0.8348623853211009 AUC: 0.8646854048478597 MCC: 0.45582619589510476\n",
      "TN: 8400, FP: 177, FN: 269, TP: 777\n",
      "Train Epoch: 462, Ave Loss: 0.2994992284160235 ACC: 0.9536527070560117 Top2: 0.960919540229885 AUC: 0.9851107535360143 MCC: 0.7521912523215224\n",
      "TN: 2154, FP: 86, FN: 148, TP: 129\n",
      "Val Epoch: 462, Ave Loss: 1.3403317637516212 ACC: 0.9070321811680572 Top2: 0.8073394495412844 AUC: 0.8749427862300154 MCC: 0.47844959651063773\n",
      "TN: 8413, FP: 164, FN: 294, TP: 752\n",
      "Train Epoch: 463, Ave Loss: 0.32609128884903565 ACC: 0.9524056946898056 Top2: 0.9701149425287356 AUC: 0.9818084784087284 MCC: 0.7422165910566746\n",
      "TN: 2148, FP: 92, FN: 137, TP: 140\n",
      "Val Epoch: 463, Ave Loss: 1.2379907202018305 ACC: 0.9090186730234406 Top2: 0.8073394495412844 AUC: 0.8781411165549251 MCC: 0.5023615460752395\n",
      "TN: 8417, FP: 160, FN: 290, TP: 756\n",
      "Train Epoch: 464, Ave Loss: 0.3164616427239505 ACC: 0.9532370362672763 Top2: 0.960919540229885 AUC: 0.9816275173208797 MCC: 0.7467670470711358\n",
      "TN: 2170, FP: 70, FN: 149, TP: 128\n",
      "Val Epoch: 464, Ave Loss: 1.3401529563402912 ACC: 0.9129916567342073 Top2: 0.8165137614678899 AUC: 0.8699313434760186 MCC: 0.5008421085223173\n",
      "TN: 8406, FP: 171, FN: 246, TP: 800\n",
      "Train Epoch: 465, Ave Loss: 0.28276477572386016 ACC: 0.9566663202743427 Top2: 0.9701149425287356 AUC: 0.9858316441030984 MCC: 0.769755422834647\n",
      "TN: 2150, FP: 90, FN: 156, TP: 121\n",
      "Val Epoch: 465, Ave Loss: 1.4703640510512883 ACC: 0.902264600715137 Top2: 0.7981651376146789 AUC: 0.8692391374419803 MCC: 0.4479138459408558\n",
      "TN: 8416, FP: 161, FN: 246, TP: 800\n",
      "Train Epoch: 466, Ave Loss: 0.303467167662549 ACC: 0.957705497246181 Top2: 0.967816091954023 AUC: 0.9830091638650301 MCC: 0.774513660840613\n",
      "TN: 2154, FP: 86, FN: 139, TP: 138\n",
      "Val Epoch: 466, Ave Loss: 1.336600697935781 ACC: 0.9106078665077473 Top2: 0.8073394495412844 AUC: 0.87260991490459 MCC: 0.5053692312575527\n",
      "TN: 8425, FP: 152, FN: 274, TP: 772\n",
      "Train Epoch: 467, Ave Loss: 0.2992538383641448 ACC: 0.9557310609996883 Top2: 0.9770114942528736 AUC: 0.9848252396299321 MCC: 0.7610148510928462\n",
      "TN: 2156, FP: 84, FN: 138, TP: 139\n",
      "Val Epoch: 467, Ave Loss: 1.356883577101035 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.8744560662712738 MCC: 0.511349889644648\n",
      "TN: 8408, FP: 169, FN: 266, TP: 780\n",
      "Train Epoch: 468, Ave Loss: 0.3159902925818355 ACC: 0.9547958017250338 Top2: 0.9724137931034482 AUC: 0.9833422727107558 MCC: 0.7579207308144855\n",
      "TN: 2136, FP: 104, FN: 145, TP: 132\n",
      "Val Epoch: 468, Ave Loss: 1.4293633716649785 ACC: 0.901072705601907 Top2: 0.8073394495412844 AUC: 0.86517454228984 MCC: 0.4617644264098464\n",
      "TN: 8400, FP: 177, FN: 258, TP: 788\n",
      "Train Epoch: 469, Ave Loss: 0.3074207747269308 ACC: 0.9547958017250338 Top2: 0.9655172413793104 AUC: 0.9831156115637646 MCC: 0.7592641927672757\n",
      "TN: 2148, FP: 92, FN: 149, TP: 128\n",
      "Val Epoch: 469, Ave Loss: 1.3849015902565502 ACC: 0.9042510925705205 Top2: 0.8165137614678899 AUC: 0.8788083419288293 MCC: 0.4665275480882049\n",
      "TN: 8421, FP: 156, FN: 281, TP: 765\n",
      "Train Epoch: 470, Ave Loss: 0.31513885591821716 ACC: 0.9545879663306661 Top2: 0.9701149425287356 AUC: 0.9832410080675095 MCC: 0.7545480099737092\n",
      "TN: 2132, FP: 108, FN: 127, TP: 150\n",
      "Val Epoch: 470, Ave Loss: 1.3578123958853987 ACC: 0.9066348827969806 Top2: 0.8073394495412844 AUC: 0.8753166903042805 MCC: 0.50898943290632\n",
      "TN: 8407, FP: 170, FN: 235, TP: 811\n",
      "Train Epoch: 471, Ave Loss: 0.2861334939504841 ACC: 0.9579133326405487 Top2: 0.9701149425287356 AUC: 0.9857783645219518 MCC: 0.7772030922570534\n",
      "TN: 2160, FP: 80, FN: 144, TP: 133\n",
      "Val Epoch: 471, Ave Loss: 1.416831890531226 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.8733125966993296 MCC: 0.49973105359305664\n",
      "TN: 8428, FP: 149, FN: 252, TP: 794\n",
      "Train Epoch: 472, Ave Loss: 0.27992350740182226 ACC: 0.958329003429284 Top2: 0.9747126436781609 AUC: 0.9866684010396429 MCC: 0.7765189811421452\n",
      "TN: 2151, FP: 89, FN: 160, TP: 117\n",
      "Val Epoch: 472, Ave Loss: 1.394158941205514 ACC: 0.901072705601907 Top2: 0.7798165137614679 AUC: 0.870654976792161 MCC: 0.43685029005812803\n",
      "TN: 8418, FP: 159, FN: 269, TP: 777\n",
      "Train Epoch: 473, Ave Loss: 0.3007893777124381 ACC: 0.9555232256053205 Top2: 0.967816091954023 AUC: 0.9846492944022331 MCC: 0.7608066974931291\n",
      "TN: 2160, FP: 80, FN: 157, TP: 120\n",
      "Val Epoch: 473, Ave Loss: 1.5679713278088223 ACC: 0.9058402860548271 Top2: 0.7889908256880734 AUC: 0.8653639118102114 MCC: 0.4599616288837419\n",
      "TN: 8406, FP: 171, FN: 264, TP: 782\n",
      "Train Epoch: 474, Ave Loss: 0.3129209509472585 ACC: 0.9547958017250338 Top2: 0.9724137931034482 AUC: 0.9823024291699242 MCC: 0.7582523247375186\n",
      "TN: 2159, FP: 81, FN: 154, TP: 123\n",
      "Val Epoch: 474, Ave Loss: 1.3213914685622028 ACC: 0.9066348827969806 Top2: 0.8073394495412844 AUC: 0.8788534682826199 MCC: 0.4677310272387872\n",
      "TN: 8426, FP: 151, FN: 281, TP: 765\n",
      "Train Epoch: 475, Ave Loss: 0.33366077701127855 ACC: 0.9551075548165853 Top2: 0.9724137931034482 AUC: 0.9812168298381705 MCC: 0.7570055731036739\n",
      "TN: 2165, FP: 75, FN: 159, TP: 118\n",
      "Val Epoch: 475, Ave Loss: 1.4732346809992498 ACC: 0.9070321811680572 Top2: 0.8348623853211009 AUC: 0.864099568076328 MCC: 0.46165613129078226\n",
      "TN: 8391, FP: 186, FN: 309, TP: 737\n",
      "Train Epoch: 476, Ave Loss: 0.3662818958239925 ACC: 0.9485607398940039 Top2: 0.9494252873563218 AUC: 0.9775793280575402 MCC: 0.721825291418375\n",
      "TN: 2141, FP: 99, FN: 156, TP: 121\n",
      "Val Epoch: 476, Ave Loss: 1.5454602773907944 ACC: 0.8986889153754469 Top2: 0.7981651376146789 AUC: 0.8555457065497679 MCC: 0.43506270836563193\n",
      "TN: 8390, FP: 187, FN: 289, TP: 757\n",
      "Train Epoch: 477, Ave Loss: 0.3412284595482791 ACC: 0.9505351761404968 Top2: 0.9517241379310345 AUC: 0.9800640737121891 MCC: 0.7345006789549182\n",
      "TN: 2156, FP: 84, FN: 154, TP: 123\n",
      "Val Epoch: 477, Ave Loss: 1.4901491913158413 ACC: 0.9054429876837505 Top2: 0.7798165137614679 AUC: 0.8675718798349665 MCC: 0.4631050980242405\n",
      "TN: 8399, FP: 178, FN: 289, TP: 757\n",
      "Train Epoch: 478, Ave Loss: 0.3301001902546144 ACC: 0.9514704354151512 Top2: 0.960919540229885 AUC: 0.9810304627677159 MCC: 0.7387476229475369\n",
      "TN: 2141, FP: 99, FN: 144, TP: 133\n",
      "Val Epoch: 478, Ave Loss: 1.4740277048854002 ACC: 0.9034564958283671 Top2: 0.7889908256880734 AUC: 0.8628755157297576 MCC: 0.47164090549887566\n",
      "TN: 8422, FP: 155, FN: 275, TP: 771\n",
      "Train Epoch: 479, Ave Loss: 0.3063826956127182 ACC: 0.9553153902109529 Top2: 0.9724137931034482 AUC: 0.9834044136448339 MCC: 0.7589017602378946\n",
      "TN: 2166, FP: 74, FN: 141, TP: 136\n",
      "Val Epoch: 479, Ave Loss: 1.431223976897896 ACC: 0.9145808502185141 Top2: 0.8165137614678899 AUC: 0.8695107013924704 MCC: 0.5182485488436867\n",
      "TN: 8403, FP: 174, FN: 260, TP: 786\n",
      "Train Epoch: 480, Ave Loss: 0.31759465059380737 ACC: 0.9548997194222176 Top2: 0.9632183908045977 AUC: 0.9830465041572564 MCC: 0.7593965278882412\n",
      "TN: 2177, FP: 63, FN: 149, TP: 128\n",
      "Val Epoch: 480, Ave Loss: 1.5496452388251114 ACC: 0.9157727453317441 Top2: 0.8256880733944955 AUC: 0.8646370551830841 MCC: 0.5128624762870121\n",
      "TN: 8420, FP: 157, FN: 280, TP: 766\n",
      "Train Epoch: 481, Ave Loss: 0.31015209745699934 ACC: 0.9545879663306661 Top2: 0.967816091954023 AUC: 0.9835114186613627 MCC: 0.7547039779463685\n",
      "TN: 2166, FP: 74, FN: 150, TP: 127\n",
      "Val Epoch: 481, Ave Loss: 1.5255032966229551 ACC: 0.911005164878824 Top2: 0.7981651376146789 AUC: 0.8710466090768437 MCC: 0.49118275979203624\n",
      "TN: 8419, FP: 158, FN: 261, TP: 785\n",
      "Train Epoch: 482, Ave Loss: 0.2973597638625106 ACC: 0.9564584848799751 Top2: 0.9655172413793104 AUC: 0.984583475170712 MCC: 0.7664124122911948\n",
      "TN: 2150, FP: 90, FN: 139, TP: 138\n",
      "Val Epoch: 482, Ave Loss: 1.3696146022502778 ACC: 0.9090186730234406 Top2: 0.8256880733944955 AUC: 0.8799687338834451 MCC: 0.49940694605430286\n",
      "TN: 8428, FP: 149, FN: 267, TP: 779\n",
      "Train Epoch: 483, Ave Loss: 0.29113484924317584 ACC: 0.9567702379715266 Top2: 0.9701149425287356 AUC: 0.9849773873878093 MCC: 0.7669729560678125\n",
      "TN: 2158, FP: 82, FN: 135, TP: 142\n",
      "Val Epoch: 483, Ave Loss: 1.5900135800784982 ACC: 0.9137862534763608 Top2: 0.7981651376146789 AUC: 0.8644444623517278 MCC: 0.5232034166367038\n",
      "TN: 8420, FP: 157, FN: 253, TP: 793\n",
      "Train Epoch: 484, Ave Loss: 0.2885608376448345 ACC: 0.9573937441546295 Top2: 0.9632183908045977 AUC: 0.9855244505348133 MCC: 0.7719940916192822\n",
      "TN: 2153, FP: 87, FN: 141, TP: 136\n",
      "Val Epoch: 484, Ave Loss: 1.45236347249598 ACC: 0.9094159713945172 Top2: 0.8165137614678899 AUC: 0.8719040097988653 MCC: 0.49794721613512066\n",
      "TN: 8418, FP: 159, FN: 239, TP: 807\n",
      "Train Epoch: 485, Ave Loss: 0.28644946076144157 ACC: 0.9586407565208355 Top2: 0.9793103448275862 AUC: 0.9861582880624089 MCC: 0.7799027084821439\n",
      "TN: 2168, FP: 72, FN: 150, TP: 127\n",
      "Val Epoch: 485, Ave Loss: 1.474282132982866 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.8810582129963899 MCC: 0.4944673665335635\n",
      "TN: 8436, FP: 141, FN: 262, TP: 784\n",
      "Train Epoch: 486, Ave Loss: 0.2759569027906032 ACC: 0.9581211680349163 Top2: 0.9770114942528736 AUC: 0.9864949080102395 MCC: 0.7741158053022228\n",
      "TN: 2139, FP: 101, FN: 138, TP: 139\n",
      "Val Epoch: 486, Ave Loss: 1.4392808513938014 ACC: 0.9050456893126738 Top2: 0.7889908256880734 AUC: 0.8735237235688499 MCC: 0.4866567637239842\n",
      "TN: 8396, FP: 181, FN: 225, TP: 821\n",
      "Train Epoch: 487, Ave Loss: 0.2892742579373087 ACC: 0.9578094149433648 Top2: 0.9747126436781609 AUC: 0.9862321884019493 MCC: 0.7783875938074631\n",
      "TN: 2172, FP: 68, FN: 173, TP: 104\n",
      "Val Epoch: 487, Ave Loss: 1.4888108015303936 ACC: 0.9042510925705205 Top2: 0.7981651376146789 AUC: 0.8686113976276432 MCC: 0.4280217191891136\n",
      "TN: 8437, FP: 140, FN: 248, TP: 798\n",
      "Train Epoch: 488, Ave Loss: 0.27717139108775274 ACC: 0.9596799334926738 Top2: 0.9770114942528736 AUC: 0.9872323509158181 MCC: 0.7834759223677255\n",
      "TN: 2174, FP: 66, FN: 177, TP: 100\n",
      "Val Epoch: 488, Ave Loss: 1.6990238794621182 ACC: 0.9034564958283671 Top2: 0.7981651376146789 AUC: 0.8640745874161939 MCC: 0.4180498203677067\n",
      "TN: 8404, FP: 173, FN: 297, TP: 749\n",
      "Train Epoch: 489, Ave Loss: 0.3808458297105632 ACC: 0.9511586823235997 Top2: 0.9471264367816092 AUC: 0.9760718948871888 MCC: 0.7359099907953975\n",
      "TN: 2168, FP: 72, FN: 149, TP: 128\n",
      "Val Epoch: 489, Ave Loss: 1.3706611332594665 ACC: 0.912197059992054 Top2: 0.8165137614678899 AUC: 0.8723971763795771 MCC: 0.4975134751341596\n",
      "TN: 8397, FP: 180, FN: 342, TP: 704\n",
      "Train Epoch: 490, Ave Loss: 0.4481920730265563 ACC: 0.9457549620700405 Top2: 0.960919540229885 AUC: 0.9693401089801508 MCC: 0.7026838488254982\n",
      "TN: 2170, FP: 70, FN: 159, TP: 118\n",
      "Val Epoch: 490, Ave Loss: 1.4622027167982055 ACC: 0.9090186730234406 Top2: 0.8165137614678899 AUC: 0.8565884476534296 MCC: 0.46990971077887356\n",
      "TN: 8372, FP: 205, FN: 381, TP: 665\n",
      "Train Epoch: 491, Ave Loss: 0.5162312294304203 ACC: 0.9391042294502754 Top2: 0.9195402298850575 AUC: 0.9562736260946 MCC: 0.6641150934476487\n",
      "TN: 2195, FP: 45, FN: 205, TP: 72\n",
      "Val Epoch: 491, Ave Loss: 1.3991855246809508 ACC: 0.9006754072308304 Top2: 0.7339449541284404 AUC: 0.8550501224858175 MCC: 0.35652029997128554\n",
      "TN: 8390, FP: 187, FN: 345, TP: 701\n",
      "Train Epoch: 492, Ave Loss: 0.4018785708490809 ACC: 0.9447157850982022 Top2: 0.9402298850574713 AUC: 0.9720224795247016 MCC: 0.6972977219870125\n",
      "TN: 2163, FP: 77, FN: 152, TP: 125\n",
      "Val Epoch: 492, Ave Loss: 1.2754703888139867 ACC: 0.9090186730234406 Top2: 0.7981651376146789 AUC: 0.8746414066529138 MCC: 0.4802116212928687\n",
      "TN: 8392, FP: 185, FN: 344, TP: 702\n",
      "Train Epoch: 493, Ave Loss: 0.41842382171757614 ACC: 0.9450275381897537 Top2: 0.9310344827586207 AUC: 0.970294682898436 MCC: 0.6989303369827591\n",
      "TN: 2169, FP: 71, FN: 158, TP: 119\n",
      "Val Epoch: 493, Ave Loss: 1.2423552870237773 ACC: 0.9090186730234406 Top2: 0.7889908256880734 AUC: 0.8686686113976276 MCC: 0.47137863126357243\n",
      "TN: 8415, FP: 162, FN: 287, TP: 759\n",
      "Train Epoch: 494, Ave Loss: 0.32636190612991767 ACC: 0.9533409539644602 Top2: 0.9632183908045977 AUC: 0.9826150844525946 MCC: 0.7477389237406497\n",
      "TN: 2165, FP: 75, FN: 159, TP: 118\n",
      "Val Epoch: 494, Ave Loss: 1.3556964904317683 ACC: 0.9070321811680572 Top2: 0.8348623853211009 AUC: 0.8707911616812789 MCC: 0.46165613129078226\n",
      "TN: 8400, FP: 177, FN: 279, TP: 767\n",
      "Train Epoch: 495, Ave Loss: 0.34030611705451125 ACC: 0.9526135300841734 Top2: 0.9632183908045977 AUC: 0.9821938079317915 MCC: 0.7457248970598962\n",
      "TN: 2179, FP: 61, FN: 160, TP: 117\n",
      "Val Epoch: 495, Ave Loss: 1.2987586532275133 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.8781153300670449 MCC: 0.4823933907288284\n",
      "TN: 8421, FP: 156, FN: 278, TP: 768\n",
      "Train Epoch: 496, Ave Loss: 0.3111859473148417 ACC: 0.9548997194222176 Top2: 0.9747126436781609 AUC: 0.9832319795192399 MCC: 0.7564820539984514\n",
      "TN: 2122, FP: 118, FN: 128, TP: 149\n",
      "Val Epoch: 496, Ave Loss: 1.2859605210833251 ACC: 0.902264600715137 Top2: 0.7981651376146789 AUC: 0.8675936371841155 MCC: 0.49313118666476335\n",
      "TN: 8388, FP: 189, FN: 241, TP: 805\n",
      "Train Epoch: 497, Ave Loss: 0.302367085615262 ACC: 0.9553153902109529 Top2: 0.9724137931034482 AUC: 0.9848892197127317 MCC: 0.7645533558169759\n",
      "TN: 2172, FP: 68, FN: 155, TP: 122\n",
      "Val Epoch: 497, Ave Loss: 1.3164983032818098 ACC: 0.9114024632499007 Top2: 0.8623853211009175 AUC: 0.8760250128932439 MCC: 0.4857953220756701\n",
      "TN: 8403, FP: 174, FN: 255, TP: 791\n",
      "Train Epoch: 498, Ave Loss: 0.30313537584609956 ACC: 0.9554193079081368 Top2: 0.960919540229885 AUC: 0.9849482953989402 MCC: 0.7625986545622552\n",
      "TN: 2160, FP: 80, FN: 144, TP: 133\n",
      "Val Epoch: 498, Ave Loss: 1.3348358924759107 ACC: 0.911005164878824 Top2: 0.8256880733944955 AUC: 0.8773981433728726 MCC: 0.49973105359305664\n",
      "TN: 8420, FP: 157, FN: 244, TP: 802\n",
      "Train Epoch: 499, Ave Loss: 0.288816041865081 ACC: 0.958329003429284 Top2: 0.9747126436781609 AUC: 0.9864397335485917 MCC: 0.7777028226585773\n",
      "TN: 2137, FP: 103, FN: 138, TP: 139\n",
      "Val Epoch: 499, Ave Loss: 1.3438583795694685 ACC: 0.9042510925705205 Top2: 0.8073394495412844 AUC: 0.8767019082001031 MCC: 0.48390672848743277\n",
      "TN: 8402, FP: 175, FN: 251, TP: 795\n",
      "Train Epoch: 500, Ave Loss: 0.31176606386604894 ACC: 0.9557310609996883 Top2: 0.9655172413793104 AUC: 0.9832541607674579 MCC: 0.7646834420324589\n",
      "TN: 2144, FP: 96, FN: 145, TP: 132\n",
      "Val Epoch: 500, Ave Loss: 1.3019779694924525 ACC: 0.9042510925705205 Top2: 0.8165137614678899 AUC: 0.8796496260959257 MCC: 0.4728682089442283\n",
      "TN: 2688, FP: 102, FN: 203, TP: 136\n",
      "ACC: 0.9025247682965803 Top2: 0.7867647058823529 AUC: 0.8269768769626036 MCC: 0.4274948305540407\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_set):\n",
    "    model.eval()\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    with torch.no_grad():\n",
    "        for mol, target in test_set:\n",
    "            mol, target = mol.to(device), target.to(device)\n",
    "            output = model(mol)\n",
    "            # squeeze\n",
    "            output = torch.squeeze(output)\n",
    "            # tracking\n",
    "            top2n += top2(output, target)\n",
    "            all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "            all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "            all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    print(f'ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(test_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(\"cuda\")\n",
    "model.load_state_dict(torch.load(args['save_path']))\n",
    "# model.load_state_dict(torch.load('./model/model80'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 2688, FP: 102, FN: 203, TP: 136\n",
      "ACC: 0.9025247682965803 Top2: 0.7941176470588235 AUC: 0.8269546737716877 MCC: 0.4274948305540407\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 819, FP: 57, FN: 69, TP: 48\n",
      "ACC: 0.8731117824773413 Top2: 0.5714285714285714 AUC: 0.7910275923974555 MCC: 0.36190904461960005\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试在新数据集上的效果\n",
    "# 1. 位点在2个以上的\n",
    "# 2. 相似度大的\n",
    "# 3. 全部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def som2vec(mol, som):\n",
    "    y = np.zeros(len(mol.GetAtoms()))\n",
    "    # try:\n",
    "    for i in som:\n",
    "        y[int(i) - 1] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "# load test set 全部\n",
    "testSet = []\n",
    "filepath = '../../raw_database/new2.txt'\n",
    "with open(filepath) as f:\n",
    "    for line in f.readlines():\n",
    "        raw = eval(line)\n",
    "        smiles = raw[1]\n",
    "        som = raw[-1]\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        g = mol2graph(mol)\n",
    "        y = som2vec(mol, som)\n",
    "        mol = from_networkx(g)\n",
    "        mol.feature = mol.feature.float()\n",
    "        label = torch.tensor(y).float()\n",
    "        testSet.append((mol,label))\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# load som大于2的\n",
    "testSet = []\n",
    "filepath = '../../raw_database/new2.txt'\n",
    "biggerpath = '../../bigger.txt'\n",
    "with open(biggerpath, 'r') as f:\n",
    "    bigger = eval(f.readline())\n",
    "\n",
    "bigger = [int(i) for i in bigger]\n",
    "\n",
    "with open(filepath, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        raw = eval(line)\n",
    "        idx = raw[0]\n",
    "        smiles = raw[1]\n",
    "        som = raw[-1]\n",
    "        if idx in bigger:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            g = mol2graph(mol)\n",
    "            y = som2vec(mol, som)\n",
    "            mol = from_networkx(g)\n",
    "            mol.feature = mol.feature.float()\n",
    "            label = torch.tensor(y).float()\n",
    "            testSet.append((mol,label))\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# load testset 相似度大的\n",
    "import pickle\n",
    "bigger = pickle.load(open('../../bigger9.pkl', 'rb'))\n",
    "testSet = []\n",
    "filepath = '../../raw_database/new2.txt'\n",
    "with open(filepath, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        raw = eval(line)\n",
    "        idx = raw[0]\n",
    "        smiles = raw[1]\n",
    "        som = raw[-1]\n",
    "        if idx in bigger:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            g = mol2graph(mol)\n",
    "            y = som2vec(mol, som)\n",
    "            mol = from_networkx(g)\n",
    "            mol.feature = mol.feature.float()\n",
    "            label = torch.tensor(y).float()\n",
    "            testSet.append((mol,label))\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae2bdcb5ffd42edc58b1d6fb8428ae1d2700e79a4fc0c0139ea5d98047639f54"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
