{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [20:08:15] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "[20:08:15] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# sklearn, condusion matrix, mcc and auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "# build dataset\n",
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch_geometric.utils import from_networkx\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import ChebConv\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# random\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = SummaryWriter(\"./runs/3g1lCheb/train\")\n",
    "val_writer = SummaryWriter(\"./runs/3g1lCheb/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = {\n",
    "    'C':[1,0,0,0,0,0,0,0,0,0],\n",
    "    'N':[0,1,0,0,0,0,0,0,0,0],\n",
    "    'O':[0,0,1,0,0,0,0,0,0,0],\n",
    "    'F':[0,0,0,1,0,0,0,0,0,0],\n",
    "    'P':[0,0,0,0,1,0,0,0,0,0],\n",
    "    'S':[0,0,0,0,0,1,0,0,0,0],\n",
    "    'Cl':[0,0,0,0,0,0,1,0,0,0],\n",
    "    'Br':[0,0,0,0,0,0,0,1,0,0],\n",
    "    'I':[0,0,0,0,0,0,0,0,1,0],\n",
    "    'other':[0,0,0,0,0,0,0,0,0,1],\n",
    "}\n",
    "\n",
    "zero_five = {\n",
    "    0:[1,0,0,0,0,0],\n",
    "    1:[0,1,0,0,0,0],\n",
    "    2:[0,0,1,0,0,0],\n",
    "    3:[0,0,0,1,0,0],\n",
    "    4:[0,0,0,0,1,0],\n",
    "    5:[0,0,0,0,0,1]\n",
    "}\n",
    "\n",
    "num_H = {\n",
    "    0:[1,0,0,0,0],\n",
    "    1:[0,1,0,0,0],\n",
    "    2:[0,0,1,0,0],\n",
    "    3:[0,0,0,1,0],\n",
    "    4:[0,0,0,0,1]\n",
    "}\n",
    "\n",
    "def mol2graph(mol):\n",
    "    # mol = Chem.MolFromSmiles(smiles)\n",
    "    # mol = add_atom_index(mol)\n",
    "    # graph\n",
    "    g = nx.Graph()\n",
    "    for atom in mol.GetAtoms():\n",
    "        # atom number\n",
    "        idx = atom.GetIdx()\n",
    "        # print(idx)\n",
    "        feature = []\n",
    "        # identity one-hot 10\n",
    "        feature.extend(identity.get(atom.GetSymbol(),[0,0,0,0,0,0,0,0,0,1]))\n",
    "        # degree of atom one-hot 6\n",
    "        feature.extend(zero_five[atom.GetDegree()])\n",
    "        # number of hydrogen atoms attached one-hot 5\n",
    "        feature.extend(num_H[atom.GetNumImplicitHs()])\n",
    "        # implicit valence electrons one-hot 6\n",
    "        feature.extend(zero_five[atom.GetImplicitValence()])\n",
    "        # aromatic 0 or 1\n",
    "        if atom.GetIsAromatic():\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "        # total feature 28d\n",
    "        g.add_node(idx, feature=feature)\n",
    "    # add edge\n",
    "    bonds_info = [(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()) for bond in mol.GetBonds()]\n",
    "    # add self_loop\n",
    "    for atom in mol.GetAtoms():\n",
    "        bonds_info.append((atom.GetIdx(), atom.GetIdx()))\n",
    "    g.add_edges_from(bonds_info)\n",
    "    # print(g.nodes.data)\n",
    "    return g\n",
    "\n",
    "\n",
    "def mol2y(mol):\n",
    "    _y = []\n",
    "    som = ['PRIMARY_SOM_1A2', 'PRIMARY_SOM_2A6','PRIMARY_SOM_2B6','PRIMARY_SOM_2C8','PRIMARY_SOM_2C9','PRIMARY_SOM_2C19','PRIMARY_SOM_2D6','PRIMARY_SOM_2E1','PRIMARY_SOM_3A4',\n",
    "           'SECONDARY_SOM_1A2', 'SECONDARY_SOM_2A6','SECONDARY_SOM_2B6','SECONDARY_SOM_2C8','SECONDARY_SOM_2C9','SECONDARY_SOM_2C19','SECONDARY_SOM_2D6','SECONDARY_SOM_2E1','SECONDARY_SOM_3A4',\n",
    "           'TERTIARY_SOM_1A2', 'TERTIARY_SOM_2A6','TERTIARY_SOM_2B6','TERTIARY_SOM_2C8','TERTIARY_SOM_2C9','TERTIARY_SOM_2C19','TERTIARY_SOM_2D6','TERTIARY_SOM_2E1','TERTIARY_SOM_3A4'\n",
    "          ]\n",
    "    result = []\n",
    "    for k in som:\n",
    "        try:\n",
    "            _res = mol.GetProp(k)\n",
    "            if ' ' in _res:\n",
    "                res = _res.split(' ')\n",
    "                for s in res:\n",
    "                    result.append(int(s))\n",
    "                # res = [int(temp) for temp in res]\n",
    "            else:\n",
    "                # res = [int(_res)]\n",
    "                result.append(int(_res))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for data in result:\n",
    "        _y.append(data)\n",
    "    _y = list(set(_y))\n",
    "\n",
    "    y = np.zeros(len(mol.GetAtoms()))\n",
    "    for i in _y:\n",
    "        y[i-1] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = Chem.SDMolSupplier('../../raw_database/merged.sdf')\n",
    "dataset = []\n",
    "for mol in mols:\n",
    "    g = mol2graph(mol)\n",
    "    y = mol2y(mol)\n",
    "    graph = from_networkx(g)\n",
    "    graph.feature = graph.feature.float()\n",
    "    label = torch.tensor(y, dtype=torch.float)\n",
    "    dataset.append((graph, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed('42')\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(dataset)\n",
    "ratio = 0.8\n",
    "training_set = dataset[:int(total * 0.8)]\n",
    "test_set = dataset[int(total * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = training_set[int(len(training_set) * 0.8):]\n",
    "training_set = training_set[:int(len(training_set) * 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 136, 109)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set), len(test_set), len(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def top2(output, label):\n",
    "    preds = torch.sigmoid(output)\n",
    "    _, indices = torch.topk(preds, 2)\n",
    "    pos_index = []\n",
    "    for i in range(label.shape[0]):\n",
    "        if label[i] == 1:\n",
    "            pos_index.append(i)\n",
    "    # print(pos_index)      \n",
    "    for li in pos_index:\n",
    "        if li in indices:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def MCC(output, label):\n",
    "    tn,fp,fn,tp=confusion_matrix(label, output).ravel()\n",
    "    # print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "    up = (tp * tn) - (fp * fn)\n",
    "    down = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    return up / down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = ChebConv(28, 1024, 2)\n",
    "        self.conv2 = ChebConv(1024, 1024, 2)\n",
    "        self.conv3 = ChebConv(1024, 1024, 2)\n",
    "        self.linear1 = nn.Linear(1024, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, mol):\n",
    "        res = self.conv1(mol.feature, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv2(res, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv3(res, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        res = self.linear1(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, training_set, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    # training_set.extend(validation_set)\n",
    "    for mol, target in training_set:\n",
    "        mol, target = mol.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        # squeeze\n",
    "        output = torch.squeeze(output)\n",
    "        # output.squeeze_(1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        total_loss += loss.item()\n",
    "        all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "        all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    train_writer.add_scalar('Ave Loss', total_loss / len(training_set), epoch)\n",
    "    train_writer.add_scalar('ACC', accuracy_score(all_labels, all_pred), epoch)\n",
    "    train_writer.add_scalar('Top2', top2n / len(training_set), epoch)\n",
    "    train_writer.add_scalar('AUC', roc_auc_score(all_labels, all_pred_raw), epoch)\n",
    "    train_writer.add_scalar('MCC', mcc, epoch)\n",
    "    print(f'Train Epoch: {epoch}, Ave Loss: {total_loss / len(training_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(training_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, model, device, val_set, optimizer, criterion, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    for mol, target in val_set:\n",
    "        mol, target = mol.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        # squeeze\n",
    "        output = torch.squeeze(output)\n",
    "        # output.squeeze_(1)\n",
    "        loss = criterion(output, target)\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        total_loss += loss.item()\n",
    "        all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "        all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    val_writer.add_scalar('Ave Loss', total_loss / len(val_set), epoch)\n",
    "    val_writer.add_scalar('ACC', accuracy_score(all_labels, all_pred), epoch)\n",
    "    val_writer.add_scalar('Top2', top2n / len(val_set), epoch)\n",
    "    val_writer.add_scalar('AUC', roc_auc_score(all_labels, all_pred_raw), epoch)\n",
    "    val_writer.add_scalar('MCC', mcc, epoch)\n",
    "    print(f'Val Epoch: {epoch}, Ave Loss: {total_loss / len(val_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(val_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')\n",
    "    return top2n / len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(args['seed'])\n",
    "    model = Model().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'], weight_decay=args['weight_decay'])\n",
    "    criterion = nn.BCEWithLogitsLoss(torch.tensor(args['pos_weight']))\n",
    "    max_top2 = 0\n",
    "    for epoch in range(1, args['epoch'] + 1):\n",
    "        train(args, model, device, training_set, optimizer, criterion, epoch)\n",
    "        top2acc = val(args, model, device, validation_set, optimizer, criterion, epoch)\n",
    "        random.shuffle(training_set)\n",
    "        if top2acc > max_top2:\n",
    "            max_top2 = top2acc\n",
    "            print('Saving model (epoch = {:4d}, top2acc = {:.4f})'\n",
    "                .format(epoch, max_top2))\n",
    "            torch.save(model.state_dict(), args['save_path'])\n",
    "    model.load_state_dict(torch.load(args['save_path']))\n",
    "    # test(model, device, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 0.01,\n",
    "    'epoch':500,\n",
    "    'seed': 42,\n",
    "    'save_path': './retrain_model',\n",
    "    'momentum':0.9,\n",
    "    'weight_decay': 1e-7,\n",
    "    'pos_weight': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Ave Loss: 1.053682095874315 ACC: 0.8943157019640445 Top2: 0.6367816091954023 AUC: 0.750053446776485 MCC: 0.18780554007420672\n",
      "Val Epoch: 1, Ave Loss: 0.931671139843967 ACC: 0.8963051251489869 Top2: 0.7706422018348624 AUC: 0.8171754125838061 MCC: 0.2436982066685073\n",
      "Saving model (epoch =    1, top2acc = 0.7706)\n",
      "Train Epoch: 2, Ave Loss: 0.9373206773023496 ACC: 0.8960823028161696 Top2: 0.6873563218390805 AUC: 0.8069832922813046 MCC: 0.2821868615108739\n",
      "Val Epoch: 2, Ave Loss: 0.893361100636491 ACC: 0.9006754072308304 Top2: 0.7706422018348624 AUC: 0.8486607142857143 MCC: 0.3637290927845773\n",
      "Train Epoch: 3, Ave Loss: 0.886433260536742 ACC: 0.8983684921542139 Top2: 0.7241379310344828 AUC: 0.8352636035143123 MCC: 0.3031971375707193\n",
      "Val Epoch: 3, Ave Loss: 0.900269382043716 ACC: 0.8955105284068335 Top2: 0.7522935779816514 AUC: 0.8428563370293966 MCC: 0.3391392654791964\n",
      "Train Epoch: 4, Ave Loss: 0.8620172794694188 ACC: 0.8989919983373168 Top2: 0.7471264367816092 AUC: 0.8388824908806088 MCC: 0.3263668975083488\n",
      "Val Epoch: 4, Ave Loss: 0.8629095547516411 ACC: 0.8974970202622169 Top2: 0.7981651376146789 AUC: 0.86526076585869 MCC: 0.36936142617647577\n",
      "Saving model (epoch =    4, top2acc = 0.7982)\n",
      "Train Epoch: 5, Ave Loss: 0.8344739007367485 ACC: 0.9040839654993246 Top2: 0.7701149425287356 AUC: 0.854824900780713 MCC: 0.3679642638907074\n",
      "Val Epoch: 5, Ave Loss: 0.8683581133501246 ACC: 0.8959078267779103 Top2: 0.8256880733944955 AUC: 0.8663897305312016 MCC: 0.3985399975679856\n",
      "Saving model (epoch =    5, top2acc = 0.8257)\n",
      "Train Epoch: 6, Ave Loss: 0.8100161720795193 ACC: 0.9054348955627143 Top2: 0.8045977011494253 AUC: 0.8664137112661346 MCC: 0.3902070419522658\n",
      "Val Epoch: 6, Ave Loss: 0.8219651484161342 ACC: 0.9042510925705205 Top2: 0.7889908256880734 AUC: 0.8761676444043321 MCC: 0.41181764239224267\n",
      "Train Epoch: 7, Ave Loss: 0.7808828369639386 ACC: 0.9106307804219058 Top2: 0.8 AUC: 0.8724363660115507 MCC: 0.42516838869871754\n",
      "Val Epoch: 7, Ave Loss: 0.8133618331283604 ACC: 0.8994835121176004 Top2: 0.7889908256880734 AUC: 0.8759541000515729 MCC: 0.3996124258427764\n",
      "Train Epoch: 8, Ave Loss: 0.7723077255932764 ACC: 0.9110464512106412 Top2: 0.8091954022988506 AUC: 0.873705322897669 MCC: 0.43651471513722906\n",
      "Val Epoch: 8, Ave Loss: 0.8117939721143573 ACC: 0.9074294795391339 Top2: 0.7614678899082569 AUC: 0.8808180763280041 MCC: 0.38496785235711106\n",
      "Train Epoch: 9, Ave Loss: 0.7636796405517506 ACC: 0.9102151096331705 Top2: 0.7770114942528735 AUC: 0.8787843828853502 MCC: 0.42583362034668504\n",
      "Val Epoch: 9, Ave Loss: 0.8160873997375506 ACC: 0.9062375844259039 Top2: 0.7522935779816514 AUC: 0.8826924316658071 MCC: 0.45998485892518287\n",
      "Train Epoch: 10, Ave Loss: 0.738578076292386 ACC: 0.9109425335134573 Top2: 0.8206896551724138 AUC: 0.8853114102347177 MCC: 0.43453545803543897\n",
      "Val Epoch: 10, Ave Loss: 0.9456304371903795 ACC: 0.8895510528406834 Top2: 0.8073394495412844 AUC: 0.8825594700876741 MCC: 0.4397009680393787\n",
      "Train Epoch: 11, Ave Loss: 0.7150697298604867 ACC: 0.9138522290346046 Top2: 0.8137931034482758 AUC: 0.8915201533916912 MCC: 0.46213551592110014\n",
      "Val Epoch: 11, Ave Loss: 0.8000775782066748 ACC: 0.9102105681366707 Top2: 0.7798165137614679 AUC: 0.8839060082516761 MCC: 0.4351264545671026\n",
      "Train Epoch: 12, Ave Loss: 0.707834538945864 ACC: 0.9156188298867297 Top2: 0.8275862068965517 AUC: 0.8937484213973473 MCC: 0.47353639785986595\n",
      "Val Epoch: 12, Ave Loss: 0.8309713890519711 ACC: 0.909813269765594 Top2: 0.8073394495412844 AUC: 0.8811686113976277 MCC: 0.4290085087640946\n",
      "Train Epoch: 13, Ave Loss: 0.7014616676145929 ACC: 0.9161384183726489 Top2: 0.8183908045977012 AUC: 0.8963101326394057 MCC: 0.47883951892210813\n",
      "Val Epoch: 13, Ave Loss: 0.8681053705171707 ACC: 0.9046483909415971 Top2: 0.7889908256880734 AUC: 0.8806963963383186 MCC: 0.4425510731018474\n",
      "Train Epoch: 14, Ave Loss: 0.6843536753630287 ACC: 0.9187363608022446 Top2: 0.8413793103448276 AUC: 0.9011614725762862 MCC: 0.49801584648120517\n",
      "Val Epoch: 14, Ave Loss: 0.7632463958558686 ACC: 0.9117997616209773 Top2: 0.7981651376146789 AUC: 0.8989000451263537 MCC: 0.4491365018527432\n",
      "Train Epoch: 15, Ave Loss: 0.6679676010476789 ACC: 0.9187363608022446 Top2: 0.8344827586206897 AUC: 0.9070794630399099 MCC: 0.49642678090379944\n",
      "Val Epoch: 15, Ave Loss: 0.8009088179523792 ACC: 0.9062375844259039 Top2: 0.8440366972477065 AUC: 0.8974237686952038 MCC: 0.42437189544631965\n",
      "Saving model (epoch =   15, top2acc = 0.8440)\n",
      "Train Epoch: 16, Ave Loss: 0.6711542838606341 ACC: 0.9191520315909799 Top2: 0.8344827586206897 AUC: 0.9051019880417437 MCC: 0.5027810020771103\n",
      "Val Epoch: 16, Ave Loss: 0.8245215523953832 ACC: 0.9137862534763608 Top2: 0.7889908256880734 AUC: 0.8807697266632284 MCC: 0.4517635325212126\n",
      "Train Epoch: 17, Ave Loss: 0.6626833440768051 ACC: 0.9149953237036267 Top2: 0.8505747126436781 AUC: 0.9093250078971931 MCC: 0.476035418434004\n",
      "Val Epoch: 17, Ave Loss: 0.7973401807590362 ACC: 0.911005164878824 Top2: 0.7981651376146789 AUC: 0.8998074071686435 MCC: 0.44595826595387383\n",
      "Train Epoch: 18, Ave Loss: 0.6353153796779559 ACC: 0.9210225501402889 Top2: 0.8459770114942529 AUC: 0.914795304976558 MCC: 0.5169325083299572\n",
      "Val Epoch: 18, Ave Loss: 0.8044531173799017 ACC: 0.9018673023440604 Top2: 0.7889908256880734 AUC: 0.8939683793192368 MCC: 0.418015378639744\n",
      "Train Epoch: 19, Ave Loss: 0.6258597927764807 ACC: 0.9184246077106931 Top2: 0.8413793103448276 AUC: 0.9175037579938878 MCC: 0.5018847431019747\n",
      "Val Epoch: 19, Ave Loss: 0.7955789421129664 ACC: 0.9141835518474374 Top2: 0.8256880733944955 AUC: 0.9027857465188242 MCC: 0.46412582136930447\n",
      "Train Epoch: 20, Ave Loss: 0.6252847850761957 ACC: 0.9226852332952302 Top2: 0.825287356321839 AUC: 0.918994917484642 MCC: 0.5290099519714947\n",
      "Val Epoch: 20, Ave Loss: 0.7637747682015831 ACC: 0.9102105681366707 Top2: 0.8348623853211009 AUC: 0.8989322782362043 MCC: 0.44565802910153723\n",
      "Train Epoch: 21, Ave Loss: 0.6232690584877956 ACC: 0.9214382209290242 Top2: 0.864367816091954 AUC: 0.920574801968268 MCC: 0.51903541794205\n",
      "Val Epoch: 21, Ave Loss: 0.8679429266704332 ACC: 0.9074294795391339 Top2: 0.7981651376146789 AUC: 0.886789259927798 MCC: 0.45268046207232787\n",
      "Train Epoch: 22, Ave Loss: 0.6128463769058601 ACC: 0.9219578094149433 Top2: 0.8482758620689655 AUC: 0.9227925923993892 MCC: 0.5338849256754159\n",
      "Val Epoch: 22, Ave Loss: 0.7691528575682859 ACC: 0.9165673420738975 Top2: 0.8165137614678899 AUC: 0.9023691335740073 MCC: 0.5006908474067336\n",
      "Train Epoch: 23, Ave Loss: 0.6005017278951476 ACC: 0.9213343032318404 Top2: 0.8597701149425288 AUC: 0.9250643869247895 MCC: 0.524105227907411\n",
      "Val Epoch: 23, Ave Loss: 0.8349485276694145 ACC: 0.9102105681366707 Top2: 0.7889908256880734 AUC: 0.8898054731820526 MCC: 0.45524190717924323\n",
      "Train Epoch: 24, Ave Loss: 0.6061507643837396 ACC: 0.9226852332952302 Top2: 0.8827586206896552 AUC: 0.9230161325667317 MCC: 0.5378853142865553\n",
      "Val Epoch: 24, Ave Loss: 0.8157493536352017 ACC: 0.9038537941994438 Top2: 0.7706422018348624 AUC: 0.8957403945332646 MCC: 0.4132463370925137\n",
      "Train Epoch: 25, Ave Loss: 0.5885734088785797 ACC: 0.9224773979008625 Top2: 0.8551724137931035 AUC: 0.9283473231246089 MCC: 0.5323228742248561\n",
      "Val Epoch: 25, Ave Loss: 0.8402081057429314 ACC: 0.9106078665077473 Top2: 0.8256880733944955 AUC: 0.9004867199587415 MCC: 0.47248908117666155\n",
      "Train Epoch: 26, Ave Loss: 0.5715165916193362 ACC: 0.9248675049360906 Top2: 0.8666666666666667 AUC: 0.9359822982492865 MCC: 0.5496404086364403\n",
      "Val Epoch: 26, Ave Loss: 0.758183721965606 ACC: 0.9030591974572905 Top2: 0.7706422018348624 AUC: 0.9004625451263538 MCC: 0.37295498711963565\n",
      "Train Epoch: 27, Ave Loss: 0.5745447142235667 ACC: 0.9248675049360906 Top2: 0.8551724137931035 AUC: 0.9311516348025791 MCC: 0.5515838708786415\n",
      "Val Epoch: 27, Ave Loss: 0.8815876633065556 ACC: 0.9078267779102106 Top2: 0.7889908256880734 AUC: 0.8928805118617844 MCC: 0.42055938183991787\n",
      "Train Epoch: 28, Ave Loss: 0.5553678120874489 ACC: 0.9251792580276421 Top2: 0.8896551724137931 AUC: 0.9379727030202835 MCC: 0.5544553689795767\n",
      "Val Epoch: 28, Ave Loss: 0.8145801256829446 ACC: 0.9133889551052841 Top2: 0.7889908256880734 AUC: 0.900512506446622 MCC: 0.47248528874251083\n",
      "Train Epoch: 29, Ave Loss: 0.5583142369805939 ACC: 0.9260105996051128 Top2: 0.8896551724137931 AUC: 0.9363259961331062 MCC: 0.5600386488820338\n",
      "Val Epoch: 29, Ave Loss: 0.8527731323324212 ACC: 0.8974970202622169 Top2: 0.7614678899082569 AUC: 0.8976107207323363 MCC: 0.4186642489109938\n",
      "Train Epoch: 30, Ave Loss: 0.553218057144272 ACC: 0.9266341057882157 Top2: 0.8873563218390804 AUC: 0.9388570548964714 MCC: 0.5661798186679309\n",
      "Val Epoch: 30, Ave Loss: 0.7873056086310006 ACC: 0.909813269765594 Top2: 0.7798165137614679 AUC: 0.9010878674574523 MCC: 0.45191626453648454\n",
      "Train Epoch: 31, Ave Loss: 0.5418451497044029 ACC: 0.9260105996051128 Top2: 0.871264367816092 AUC: 0.93999013770431 MCC: 0.5617358904580105\n",
      "Val Epoch: 31, Ave Loss: 0.7966650729879327 ACC: 0.9094159713945172 Top2: 0.7706422018348624 AUC: 0.8983875386797318 MCC: 0.4600048536767224\n",
      "Train Epoch: 32, Ave Loss: 0.5290818289870526 ACC: 0.9280889535487894 Top2: 0.8919540229885058 AUC: 0.9429475445804077 MCC: 0.5772558284934164\n",
      "Val Epoch: 32, Ave Loss: 0.76491068665861 ACC: 0.9153754469606674 Top2: 0.7798165137614679 AUC: 0.9092106111397628 MCC: 0.4875964632624634\n",
      "Train Epoch: 33, Ave Loss: 0.5320636529682313 ACC: 0.9291281305206277 Top2: 0.8988505747126436 AUC: 0.9451326204569961 MCC: 0.5822064366109955\n",
      "Val Epoch: 33, Ave Loss: 0.8457540555626427 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.905959902011346 MCC: 0.4515931055667607\n",
      "Train Epoch: 34, Ave Loss: 0.5170854542427495 ACC: 0.9298555544009145 Top2: 0.8942528735632184 AUC: 0.9473783882413972 MCC: 0.5918428822071364\n",
      "Val Epoch: 34, Ave Loss: 0.8302270328164647 ACC: 0.9117997616209773 Top2: 0.7981651376146789 AUC: 0.9018082774626095 MCC: 0.45409116017401036\n",
      "Train Epoch: 35, Ave Loss: 0.517472578957794 ACC: 0.9287124597318923 Top2: 0.8988505747126436 AUC: 0.9468857750429078 MCC: 0.5820904844135567\n",
      "Val Epoch: 35, Ave Loss: 0.8381951038956369 ACC: 0.9133889551052841 Top2: 0.7981651376146789 AUC: 0.9026423091799898 MCC: 0.4700221300901362\n",
      "Train Epoch: 36, Ave Loss: 0.5053564016974654 ACC: 0.9287124597318923 Top2: 0.903448275862069 AUC: 0.9495212751609479 MCC: 0.5881525556097805\n",
      "Val Epoch: 36, Ave Loss: 0.8655990088751556 ACC: 0.912197059992054 Top2: 0.7889908256880734 AUC: 0.9015141503352244 MCC: 0.45999864472386\n",
      "Train Epoch: 37, Ave Loss: 0.508342681527014 ACC: 0.9307908136755689 Top2: 0.9172413793103448 AUC: 0.9494424704248166 MCC: 0.5986085399240423\n",
      "Val Epoch: 37, Ave Loss: 0.8284914375479342 ACC: 0.9157727453317441 Top2: 0.7889908256880734 AUC: 0.9077552862300156 MCC: 0.5323651344462746\n",
      "Train Epoch: 38, Ave Loss: 0.4961987344587068 ACC: 0.9307908136755689 Top2: 0.9126436781609195 AUC: 0.9514727791498941 MCC: 0.5948330555721757\n",
      "Val Epoch: 38, Ave Loss: 0.8405061875119668 ACC: 0.9193484306714342 Top2: 0.8256880733944955 AUC: 0.9113653945332645 MCC: 0.5494650841620677\n",
      "Train Epoch: 39, Ave Loss: 0.49116866452277047 ACC: 0.9297516367037306 Top2: 0.9195402298850575 AUC: 0.9530167723675596 MCC: 0.5925011994127956\n",
      "Val Epoch: 39, Ave Loss: 0.795409415105204 ACC: 0.9169646404449742 Top2: 0.8348623853211009 AUC: 0.9059325038679731 MCC: 0.509710883282973\n",
      "Train Epoch: 40, Ave Loss: 0.49339576423878717 ACC: 0.9305829782812013 Top2: 0.9149425287356322 AUC: 0.9525012534077197 MCC: 0.5996817896484203\n",
      "Val Epoch: 40, Ave Loss: 0.875391669161276 ACC: 0.9125943583631307 Top2: 0.7889908256880734 AUC: 0.9049074909747292 MCC: 0.5200105123722258\n",
      "Train Epoch: 41, Ave Loss: 0.48241801663273787 ACC: 0.9333887561051647 Top2: 0.9218390804597701 AUC: 0.9562726786543496 MCC: 0.6221316740393658\n",
      "Val Epoch: 41, Ave Loss: 0.8170912219296902 ACC: 0.911005164878824 Top2: 0.8165137614678899 AUC: 0.9028824458483754 MCC: 0.46049135344008446\n",
      "Train Epoch: 42, Ave Loss: 0.46815686713678 ACC: 0.9339083445910838 Top2: 0.9379310344827586 AUC: 0.9577074375843081 MCC: 0.6230121387953623\n",
      "Val Epoch: 42, Ave Loss: 0.8690353939907813 ACC: 0.9145808502185141 Top2: 0.8623853211009175 AUC: 0.907099342444559 MCC: 0.4959711071282083\n",
      "Saving model (epoch =   42, top2acc = 0.8624)\n",
      "Train Epoch: 43, Ave Loss: 0.4810383076822991 ACC: 0.9317260729502235 Top2: 0.9195402298850575 AUC: 0.9552398573177274 MCC: 0.604875327352582\n",
      "Val Epoch: 43, Ave Loss: 0.8994066554096041 ACC: 0.911005164878824 Top2: 0.8256880733944955 AUC: 0.9063547576070139 MCC: 0.44983376615423626\n",
      "Train Epoch: 44, Ave Loss: 0.4635990235587882 ACC: 0.9347396861685545 Top2: 0.9241379310344827 AUC: 0.9589631860386989 MCC: 0.6285927714010934\n",
      "Val Epoch: 44, Ave Loss: 0.8737613293983111 ACC: 0.9090186730234406 Top2: 0.7706422018348624 AUC: 0.9050195010314596 MCC: 0.48168627902053496\n",
      "Train Epoch: 45, Ave Loss: 0.45609617002374264 ACC: 0.9361945339291281 Top2: 0.9356321839080459 AUC: 0.9609408282322035 MCC: 0.6347059176772862\n",
      "Val Epoch: 45, Ave Loss: 0.8473728427646356 ACC: 0.9129916567342073 Top2: 0.7889908256880734 AUC: 0.9049929087158329 MCC: 0.49001850807654646\n",
      "Train Epoch: 46, Ave Loss: 0.43489317198584376 ACC: 0.9394159825418269 Top2: 0.9471264367816092 AUC: 0.9635054375267931 MCC: 0.6562756231878767\n",
      "Val Epoch: 46, Ave Loss: 0.89002130022956 ACC: 0.9102105681366707 Top2: 0.8165137614678899 AUC: 0.9103532748839608 MCC: 0.47643693980175156\n",
      "Train Epoch: 47, Ave Loss: 0.4392965661615803 ACC: 0.9370258755065988 Top2: 0.9356321839080459 AUC: 0.9634588457591795 MCC: 0.6478748121707192\n",
      "Val Epoch: 47, Ave Loss: 0.9595642716136821 ACC: 0.9114024632499007 Top2: 0.8165137614678899 AUC: 0.9055698813821558 MCC: 0.45323815626010605\n",
      "Train Epoch: 48, Ave Loss: 0.4468895147243712 ACC: 0.9353631923516574 Top2: 0.9195402298850575 AUC: 0.9628712098767414 MCC: 0.6331422076414086\n",
      "Val Epoch: 48, Ave Loss: 0.9164408062405269 ACC: 0.9042510925705205 Top2: 0.7981651376146789 AUC: 0.8981820526044353 MCC: 0.5026608250889018\n",
      "Train Epoch: 49, Ave Loss: 0.4349329548376166 ACC: 0.9394159825418269 Top2: 0.9356321839080459 AUC: 0.9634742834620849 MCC: 0.6576161166786362\n",
      "Val Epoch: 49, Ave Loss: 0.9121683417998869 ACC: 0.9094159713945172 Top2: 0.7798165137614679 AUC: 0.9048212674058793 MCC: 0.4803531322339032\n",
      "Train Epoch: 50, Ave Loss: 0.43978659239872225 ACC: 0.9390003117530915 Top2: 0.9379310344827586 AUC: 0.9641076751354449 MCC: 0.6586460260331901\n",
      "Val Epoch: 50, Ave Loss: 0.8727546837351737 ACC: 0.9145808502185141 Top2: 0.8073394495412844 AUC: 0.909998710675606 MCC: 0.4946974572684007\n",
      "Train Epoch: 51, Ave Loss: 0.4233902565753543 ACC: 0.9366102047178635 Top2: 0.9448275862068966 AUC: 0.9675286589529426 MCC: 0.645044780065271\n",
      "Val Epoch: 51, Ave Loss: 0.8926321617199466 ACC: 0.9153754469606674 Top2: 0.7981651376146789 AUC: 0.9132542547705004 MCC: 0.5009491918215807\n",
      "Train Epoch: 52, Ave Loss: 0.42883933971519395 ACC: 0.939312064844643 Top2: 0.9356321839080459 AUC: 0.9657818577898872 MCC: 0.661166331963263\n",
      "Val Epoch: 52, Ave Loss: 0.8828534385479918 ACC: 0.9157727453317441 Top2: 0.8256880733944955 AUC: 0.9065924767921609 MCC: 0.5115904435894999\n",
      "Train Epoch: 53, Ave Loss: 0.40496477359363187 ACC: 0.9406629949080328 Top2: 0.9563218390804598 AUC: 0.9691653341198203 MCC: 0.6702147929501273\n",
      "Val Epoch: 53, Ave Loss: 0.9317292399072182 ACC: 0.9078267779102106 Top2: 0.8256880733944955 AUC: 0.9088504061371843 MCC: 0.46943791451859784\n",
      "Train Epoch: 54, Ave Loss: 0.4124444714677406 ACC: 0.9433648550348124 Top2: 0.9425287356321839 AUC: 0.9689769049735262 MCC: 0.685550159041179\n",
      "Val Epoch: 54, Ave Loss: 1.052390058601805 ACC: 0.9090186730234406 Top2: 0.7981651376146789 AUC: 0.9015923156266117 MCC: 0.4611245142746012\n",
      "Train Epoch: 55, Ave Loss: 0.4345487984992935 ACC: 0.9360906162319443 Top2: 0.9287356321839081 AUC: 0.9639613234826299 MCC: 0.6430992762732409\n",
      "Val Epoch: 55, Ave Loss: 0.9745342557493666 ACC: 0.9129916567342073 Top2: 0.7706422018348624 AUC: 0.8925541516245488 MCC: 0.49947920882224733\n",
      "Train Epoch: 56, Ave Loss: 0.41717114300887237 ACC: 0.9403512418164813 Top2: 0.9448275862068966 AUC: 0.968323059737111 MCC: 0.6700262461238528\n",
      "Val Epoch: 56, Ave Loss: 0.8747655295792522 ACC: 0.9106078665077473 Top2: 0.7706422018348624 AUC: 0.906623904074265 MCC: 0.48669532828271816\n",
      "Train Epoch: 57, Ave Loss: 0.40812522841741383 ACC: 0.9412865010911359 Top2: 0.9586206896551724 AUC: 0.9693994633252566 MCC: 0.6737537235865239\n",
      "Val Epoch: 57, Ave Loss: 0.930311205395318 ACC: 0.9058402860548271 Top2: 0.8440366972477065 AUC: 0.9070542160907683 MCC: 0.45840176780352004\n",
      "Train Epoch: 58, Ave Loss: 0.398917325766114 ACC: 0.944196196612283 Top2: 0.9494252873563218 AUC: 0.9707420418920182 MCC: 0.6941141607487044\n",
      "Val Epoch: 58, Ave Loss: 0.9143452582658704 ACC: 0.9141835518474374 Top2: 0.8165137614678899 AUC: 0.9090027075812275 MCC: 0.49412732659732866\n",
      "Train Epoch: 59, Ave Loss: 0.39381407683548963 ACC: 0.9444040320066507 Top2: 0.9540229885057471 AUC: 0.972208345009141 MCC: 0.6901817931142947\n",
      "Val Epoch: 59, Ave Loss: 0.9578397057847966 ACC: 0.9114024632499007 Top2: 0.8073394495412844 AUC: 0.904934889118102 MCC: 0.49705480909341887\n",
      "Train Epoch: 60, Ave Loss: 0.3831691655826598 ACC: 0.9436766081263639 Top2: 0.9563218390804598 AUC: 0.9731689936913854 MCC: 0.6913270688927253\n",
      "Val Epoch: 60, Ave Loss: 0.9256747402250767 ACC: 0.9141835518474374 Top2: 0.8532110091743119 AUC: 0.9113976276431149 MCC: 0.5179762387090086\n",
      "Train Epoch: 61, Ave Loss: 0.3845307568325311 ACC: 0.9462745505559597 Top2: 0.960919540229885 AUC: 0.9735788451974031 MCC: 0.7069972836993189\n",
      "Val Epoch: 61, Ave Loss: 1.0663061503731055 ACC: 0.9094159713945172 Top2: 0.8165137614678899 AUC: 0.9067359141309953 MCC: 0.47305264356960447\n",
      "Train Epoch: 62, Ave Loss: 0.37223572408733285 ACC: 0.9444040320066507 Top2: 0.9747126436781609 AUC: 0.9751275756163211 MCC: 0.6948746166730531\n",
      "Val Epoch: 62, Ave Loss: 0.9531592220247407 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.908874580969572 MCC: 0.5411211685696271\n",
      "Train Epoch: 63, Ave Loss: 0.36401673915001886 ACC: 0.9483529044996363 Top2: 0.967816091954023 AUC: 0.9758740470701692 MCC: 0.7181087900364622\n",
      "Val Epoch: 63, Ave Loss: 1.0305308939530216 ACC: 0.9173619388160509 Top2: 0.8256880733944955 AUC: 0.902189433986591 MCC: 0.5261562480790877\n",
      "Train Epoch: 64, Ave Loss: 0.3670211581731334 ACC: 0.9459627974644081 Top2: 0.9770114942528736 AUC: 0.9766963137440587 MCC: 0.7042190630631205\n",
      "Val Epoch: 64, Ave Loss: 1.053391545481228 ACC: 0.9070321811680572 Top2: 0.8073394495412844 AUC: 0.9072371389891698 MCC: 0.48910957807423316\n",
      "Train Epoch: 65, Ave Loss: 0.5076521008023843 ACC: 0.9368180401122311 Top2: 0.9379310344827586 AUC: 0.9649846704167466 MCC: 0.6537045264332685\n",
      "Val Epoch: 65, Ave Loss: 0.8869319438524202 ACC: 0.9125943583631307 Top2: 0.7614678899082569 AUC: 0.8662519339865912 MCC: 0.4351444389098434\n",
      "Train Epoch: 66, Ave Loss: 0.5760490866013274 ACC: 0.9296477190065469 Top2: 0.9126436781609195 AUC: 0.9427860338835844 MCC: 0.5958971427000885\n",
      "Val Epoch: 66, Ave Loss: 0.8125494384471703 ACC: 0.9141835518474374 Top2: 0.8348623853211009 AUC: 0.9096602630221764 MCC: 0.47307888144312094\n",
      "Train Epoch: 67, Ave Loss: 0.4515293295294078 ACC: 0.936298451626312 Top2: 0.9448275862068966 AUC: 0.96405322518693 MCC: 0.6393834402124418\n",
      "Val Epoch: 67, Ave Loss: 0.9234382332464971 ACC: 0.909813269765594 Top2: 0.8256880733944955 AUC: 0.9033893115007736 MCC: 0.48488432387952957\n",
      "Train Epoch: 68, Ave Loss: 0.4070217749871643 ACC: 0.9413904187883196 Top2: 0.9448275862068966 AUC: 0.9697247139900811 MCC: 0.6758778013925881\n",
      "Val Epoch: 68, Ave Loss: 0.8568432731639355 ACC: 0.9137862534763608 Top2: 0.7889908256880734 AUC: 0.907723053120165 MCC: 0.5177378526995531\n",
      "Train Epoch: 69, Ave Loss: 0.38431389182056674 ACC: 0.9451314558869376 Top2: 0.9540229885057471 AUC: 0.9734956376506959 MCC: 0.6984471130705594\n",
      "Val Epoch: 69, Ave Loss: 0.9545507251259384 ACC: 0.9018673023440604 Top2: 0.8165137614678899 AUC: 0.9026592315626611 MCC: 0.46936687584371944\n",
      "Train Epoch: 70, Ave Loss: 0.3645324012708526 ACC: 0.9485607398940039 Top2: 0.9540229885057471 AUC: 0.9759330670245985 MCC: 0.7199775852576947\n",
      "Val Epoch: 70, Ave Loss: 0.9292339139079692 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9078672962867458 MCC: 0.5271911980287856\n",
      "Train Epoch: 71, Ave Loss: 0.36032959102049483 ACC: 0.9494959991686585 Top2: 0.9563218390804598 AUC: 0.977066038368878 MCC: 0.7257507140982583\n",
      "Val Epoch: 71, Ave Loss: 0.9563781732368634 ACC: 0.909813269765594 Top2: 0.8256880733944955 AUC: 0.9115708806085612 MCC: 0.49507554451204266\n",
      "Train Epoch: 72, Ave Loss: 0.3455231614058704 ACC: 0.9485607398940039 Top2: 0.9701149425287356 AUC: 0.979446175473514 MCC: 0.72201382838619\n",
      "Val Epoch: 72, Ave Loss: 0.9990532819455097 ACC: 0.9137862534763608 Top2: 0.8532110091743119 AUC: 0.9152825232078391 MCC: 0.5095967825840306\n",
      "Train Epoch: 73, Ave Loss: 0.35434453566271384 ACC: 0.9482489868024525 Top2: 0.9655172413793104 AUC: 0.9787503084753992 MCC: 0.7181688091566149\n",
      "Val Epoch: 73, Ave Loss: 0.9093654208726873 ACC: 0.9082240762812872 Top2: 0.7706422018348624 AUC: 0.9067915162454874 MCC: 0.47703452842661903\n",
      "Train Epoch: 74, Ave Loss: 0.345419874797241 ACC: 0.9520939415982542 Top2: 0.9632183908045977 AUC: 0.9791695229203632 MCC: 0.7383270590702172\n",
      "Val Epoch: 74, Ave Loss: 0.948393984484153 ACC: 0.909813269765594 Top2: 0.7981651376146789 AUC: 0.9054820461578134 MCC: 0.4936177934841916\n",
      "Train Epoch: 75, Ave Loss: 0.34869370325579685 ACC: 0.9486646575911878 Top2: 0.9632183908045977 AUC: 0.9783886649585991 MCC: 0.7219251507260848\n",
      "Val Epoch: 75, Ave Loss: 0.8460757843295642 ACC: 0.9149781485895908 Top2: 0.8073394495412844 AUC: 0.9172390729757608 MCC: 0.5278793619568658\n",
      "Train Epoch: 76, Ave Loss: 0.3640882275386973 ACC: 0.9490803283799231 Top2: 0.960919540229885 AUC: 0.9769014066924058 MCC: 0.7206677557877563\n",
      "Val Epoch: 76, Ave Loss: 0.9993521968210373 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9113686178442495 MCC: 0.5049450319901146\n",
      "Train Epoch: 77, Ave Loss: 0.32882797545076 ACC: 0.9519900239010703 Top2: 0.967816091954023 AUC: 0.9806114712498699 MCC: 0.7407880801699723\n",
      "Val Epoch: 77, Ave Loss: 1.0188626475491211 ACC: 0.9086213746523639 Top2: 0.8073394495412844 AUC: 0.9053184631253224 MCC: 0.4697165007928792\n",
      "Train Epoch: 78, Ave Loss: 0.3189994518348069 ACC: 0.9542762132391146 Top2: 0.9724137931034482 AUC: 0.9821091513588187 MCC: 0.7509429115475246\n",
      "Val Epoch: 78, Ave Loss: 1.0890233569154772 ACC: 0.9161700437028208 Top2: 0.8256880733944955 AUC: 0.9103806730273337 MCC: 0.5339522046365757\n",
      "Train Epoch: 79, Ave Loss: 0.3304859461807264 ACC: 0.9505351761404968 Top2: 0.9632183908045977 AUC: 0.9809298111740434 MCC: 0.7318031608086675\n",
      "Val Epoch: 79, Ave Loss: 1.0613324206766732 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9081944623517277 MCC: 0.5312418628326014\n",
      "Train Epoch: 80, Ave Loss: 0.3291404377079409 ACC: 0.9517821885067027 Top2: 0.9701149425287356 AUC: 0.9803974054850325 MCC: 0.7391424329277714\n",
      "Val Epoch: 80, Ave Loss: 1.0343182060457425 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9092073878287776 MCC: 0.5125822496943438\n",
      "Train Epoch: 81, Ave Loss: 0.33464994412360305 ACC: 0.952197859295438 Top2: 0.9632183908045977 AUC: 0.9801008566866208 MCC: 0.7424337189797453\n",
      "Val Epoch: 81, Ave Loss: 1.0158985399538565 ACC: 0.9145808502185141 Top2: 0.8623853211009175 AUC: 0.9141712867457452 MCC: 0.5102732953487166\n",
      "Train Epoch: 82, Ave Loss: 0.3282534751489336 ACC: 0.9550036371194014 Top2: 0.9747126436781609 AUC: 0.9820159678235915 MCC: 0.7565125536794051\n",
      "Val Epoch: 82, Ave Loss: 0.9986181542738726 ACC: 0.9165673420738975 Top2: 0.8348623853211009 AUC: 0.9163405750386797 MCC: 0.5407711768825421\n",
      "Train Epoch: 83, Ave Loss: 0.34956887225512034 ACC: 0.9492881637742908 Top2: 0.9724137931034482 AUC: 0.9794421070536146 MCC: 0.7253346646225841\n",
      "Val Epoch: 83, Ave Loss: 1.0600064291699935 ACC: 0.9125943583631307 Top2: 0.8165137614678899 AUC: 0.9018614620938628 MCC: 0.5033100825082278\n",
      "Train Epoch: 84, Ave Loss: 0.43690838701497825 ACC: 0.9463784682531435 Top2: 0.9586206896551724 AUC: 0.9708503287394743 MCC: 0.7078594175710079\n",
      "Val Epoch: 84, Ave Loss: 0.8215000444869383 ACC: 0.9090186730234406 Top2: 0.8440366972477065 AUC: 0.9086948813821557 MCC: 0.48463709668203514\n",
      "Train Epoch: 85, Ave Loss: 0.3652962700361111 ACC: 0.9494959991686585 Top2: 0.9655172413793104 AUC: 0.9770670415409078 MCC: 0.7233214509676501\n",
      "Val Epoch: 85, Ave Loss: 0.9985165717507448 ACC: 0.9102105681366707 Top2: 0.8256880733944955 AUC: 0.9108053442496131 MCC: 0.5125744367575039\n",
      "Train Epoch: 86, Ave Loss: 0.3319160055534808 ACC: 0.9513665177179673 Top2: 0.967816091954023 AUC: 0.9815604162584314 MCC: 0.7377421068836316\n",
      "Val Epoch: 86, Ave Loss: 1.0115415367272196 ACC: 0.9046483909415971 Top2: 0.7981651376146789 AUC: 0.9034344378545642 MCC: 0.4930827402954365\n",
      "Train Epoch: 87, Ave Loss: 0.3207254340399022 ACC: 0.9529252831757248 Top2: 0.9747126436781609 AUC: 0.9820373130951179 MCC: 0.745799513500031\n",
      "Val Epoch: 87, Ave Loss: 1.1204283161669832 ACC: 0.901072705601907 Top2: 0.8073394495412844 AUC: 0.8983456356369263 MCC: 0.5068906668210135\n",
      "Train Epoch: 88, Ave Loss: 0.31374842142645426 ACC: 0.9525096123869895 Top2: 0.9701149425287356 AUC: 0.9828677723405854 MCC: 0.7459577467936895\n",
      "Val Epoch: 88, Ave Loss: 1.0750064804375443 ACC: 0.912197059992054 Top2: 0.7889908256880734 AUC: 0.8977525464156781 MCC: 0.5002932813961988\n",
      "Train Epoch: 89, Ave Loss: 0.3085369585728429 ACC: 0.954068377844747 Top2: 0.9701149425287356 AUC: 0.9833772165364661 MCC: 0.7513175030855972\n",
      "Val Epoch: 89, Ave Loss: 1.0626492245174055 ACC: 0.9114024632499007 Top2: 0.8256880733944955 AUC: 0.9070614685404849 MCC: 0.4984694776808484\n",
      "Train Epoch: 90, Ave Loss: 0.3211700213398634 ACC: 0.9542762132391146 Top2: 0.9747126436781609 AUC: 0.9825494881481912 MCC: 0.7535610935532274\n",
      "Val Epoch: 90, Ave Loss: 1.0084655617893494 ACC: 0.9114024632499007 Top2: 0.8256880733944955 AUC: 0.9114137441980402 MCC: 0.5013026608214491\n",
      "Train Epoch: 91, Ave Loss: 0.3101897852474404 ACC: 0.9553153902109529 Top2: 0.9793103448275862 AUC: 0.9836035990245601 MCC: 0.7596800083657608\n",
      "Val Epoch: 91, Ave Loss: 0.972033442784135 ACC: 0.909813269765594 Top2: 0.8256880733944955 AUC: 0.9097891954615781 MCC: 0.5096708072741162\n",
      "Train Epoch: 92, Ave Loss: 0.308149621163545 ACC: 0.9543801309362985 Top2: 0.9770114942528736 AUC: 0.983310282669356 MCC: 0.7550182585761371\n",
      "Val Epoch: 92, Ave Loss: 1.0150760971607828 ACC: 0.9102105681366707 Top2: 0.8440366972477065 AUC: 0.9126095925734915 MCC: 0.5125744367575039\n",
      "Train Epoch: 93, Ave Loss: 0.2904976312050807 ACC: 0.9564584848799751 Top2: 0.9770114942528736 AUC: 0.9852246135614146 MCC: 0.7656501766145526\n",
      "Val Epoch: 93, Ave Loss: 1.0792319134108448 ACC: 0.9050456893126738 Top2: 0.8165137614678899 AUC: 0.9053063757091284 MCC: 0.49907220018482584\n",
      "Train Epoch: 94, Ave Loss: 0.2923011451886513 ACC: 0.9558349786968721 Top2: 0.9724137931034482 AUC: 0.9853790463222487 MCC: 0.764327949897628\n",
      "Val Epoch: 94, Ave Loss: 1.1440232233005498 ACC: 0.9114024632499007 Top2: 0.8165137614678899 AUC: 0.9127860688499226 MCC: 0.5226649149549655\n",
      "Train Epoch: 95, Ave Loss: 0.28355930839314275 ACC: 0.9579133326405487 Top2: 0.9747126436781609 AUC: 0.9861561702547901 MCC: 0.7770422405715386\n",
      "Val Epoch: 95, Ave Loss: 1.1908058628286107 ACC: 0.9145808502185141 Top2: 0.8165137614678899 AUC: 0.9113452488396081 MCC: 0.5142485331779465\n",
      "Train Epoch: 96, Ave Loss: 0.2816382345547047 ACC: 0.9574976618518134 Top2: 0.9816091954022989 AUC: 0.9861986936025045 MCC: 0.7729356076087706\n",
      "Val Epoch: 96, Ave Loss: 1.1427133701070973 ACC: 0.9046483909415971 Top2: 0.8073394495412844 AUC: 0.9045432568334193 MCC: 0.49930176709751933\n",
      "Train Epoch: 97, Ave Loss: 0.2868590949923786 ACC: 0.9565624025771589 Top2: 0.9816091954022989 AUC: 0.9856491782572049 MCC: 0.769613896164513\n",
      "Val Epoch: 97, Ave Loss: 1.0013200640592963 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.9110366168127901 MCC: 0.5241078994085301\n",
      "Train Epoch: 98, Ave Loss: 0.28106404171652066 ACC: 0.9569780733658942 Top2: 0.9770114942528736 AUC: 0.9862744888225459 MCC: 0.7702222383595517\n",
      "Val Epoch: 98, Ave Loss: 1.1088759378826998 ACC: 0.9149781485895908 Top2: 0.8256880733944955 AUC: 0.9119939401753481 MCC: 0.5265406346742858\n",
      "Train Epoch: 99, Ave Loss: 0.2767907424748192 ACC: 0.95957601579549 Top2: 0.9793103448275862 AUC: 0.9870162788069208 MCC: 0.7860303394972301\n",
      "Val Epoch: 99, Ave Loss: 1.116260364496415 ACC: 0.9117997616209773 Top2: 0.8073394495412844 AUC: 0.9065820010314596 MCC: 0.5198491937890899\n",
      "Train Epoch: 100, Ave Loss: 0.2742342389364552 ACC: 0.957705497246181 Top2: 0.9816091954022989 AUC: 0.9867287028249994 MCC: 0.7770854928410403\n",
      "Val Epoch: 100, Ave Loss: 1.1154864195140437 ACC: 0.9082240762812872 Top2: 0.8165137614678899 AUC: 0.9130592444559051 MCC: 0.5069310011830305\n",
      "Train Epoch: 101, Ave Loss: 0.2689409136471927 ACC: 0.9581211680349163 Top2: 0.9701149425287356 AUC: 0.9874356047154436 MCC: 0.7775144470659818\n",
      "Val Epoch: 101, Ave Loss: 1.159693770350914 ACC: 0.911005164878824 Top2: 0.8073394495412844 AUC: 0.9061549123259411 MCC: 0.5140506170697363\n",
      "Train Epoch: 102, Ave Loss: 0.2736495114129333 ACC: 0.9601995219785929 Top2: 0.9793103448275862 AUC: 0.9870563499563397 MCC: 0.7867612435596855\n",
      "Val Epoch: 102, Ave Loss: 1.1136505678819424 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.9096119133574008 MCC: 0.5283710878130083\n",
      "Train Epoch: 103, Ave Loss: 0.2665419972462032 ACC: 0.9588485919152031 Top2: 0.9793103448275862 AUC: 0.9878433941456217 MCC: 0.7838531486883866\n",
      "Val Epoch: 103, Ave Loss: 1.1269502000722482 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9122123194945848 MCC: 0.5039280114346477\n",
      "Train Epoch: 104, Ave Loss: 0.2612579923316384 ACC: 0.9604073573729606 Top2: 0.9816091954022989 AUC: 0.9883972565697178 MCC: 0.7915067637327698\n",
      "Val Epoch: 104, Ave Loss: 1.1593888713860747 ACC: 0.9058402860548271 Top2: 0.8073394495412844 AUC: 0.9091542031975246 MCC: 0.48943774033515436\n",
      "Train Epoch: 105, Ave Loss: 0.2649455971471723 ACC: 0.9606151927673283 Top2: 0.9793103448275862 AUC: 0.9876654983056423 MCC: 0.7902441388058395\n",
      "Val Epoch: 105, Ave Loss: 1.1427282009027053 ACC: 0.9102105681366707 Top2: 0.8348623853211009 AUC: 0.9126216799896854 MCC: 0.5212888792420262\n",
      "Train Epoch: 106, Ave Loss: 0.2592763655882695 ACC: 0.9600956042814092 Top2: 0.9839080459770115 AUC: 0.9883185075653662 MCC: 0.7893209630722495\n",
      "Val Epoch: 106, Ave Loss: 1.1919482608847511 ACC: 0.9082240762812872 Top2: 0.7889908256880734 AUC: 0.9057802024239299 MCC: 0.48750659074441627\n",
      "Train Epoch: 107, Ave Loss: 0.2594539641635686 ACC: 0.9612386989504312 Top2: 0.9793103448275862 AUC: 0.9883824319163863 MCC: 0.7942827102073496\n",
      "Val Epoch: 107, Ave Loss: 1.1466626601492336 ACC: 0.911005164878824 Top2: 0.8073394495412844 AUC: 0.9084225116039195 MCC: 0.5197934570444092\n",
      "Train Epoch: 108, Ave Loss: 0.25557676118583944 ACC: 0.9611347812532475 Top2: 0.9770114942528736 AUC: 0.9885671270334576 MCC: 0.7933747459293905\n",
      "Val Epoch: 108, Ave Loss: 1.1774404535113852 ACC: 0.9086213746523639 Top2: 0.8165137614678899 AUC: 0.9100333612686952 MCC: 0.5023954915745683\n",
      "Train Epoch: 109, Ave Loss: 0.2559405233438343 ACC: 0.961966122830718 Top2: 0.9770114942528736 AUC: 0.9888113436909731 MCC: 0.8001005322183028\n",
      "Val Epoch: 109, Ave Loss: 1.1683571178555796 ACC: 0.9094159713945172 Top2: 0.8256880733944955 AUC: 0.9084853661681278 MCC: 0.5008841963214578\n",
      "Train Epoch: 110, Ave Loss: 0.2553299461432335 ACC: 0.9610308635560636 Top2: 0.9839080459770115 AUC: 0.9887124755142427 MCC: 0.7934632014589772\n",
      "Val Epoch: 110, Ave Loss: 1.1618480863436254 ACC: 0.9106078665077473 Top2: 0.8256880733944955 AUC: 0.9105611784424962 MCC: 0.5024873649140027\n",
      "Train Epoch: 111, Ave Loss: 0.2537060861219653 ACC: 0.9616543697391666 Top2: 0.9747126436781609 AUC: 0.9887042272108852 MCC: 0.7952685423580497\n",
      "Val Epoch: 111, Ave Loss: 1.1624960308497208 ACC: 0.9066348827969806 Top2: 0.8165137614678899 AUC: 0.9068487300154718 MCC: 0.5059540401871436\n",
      "Train Epoch: 112, Ave Loss: 0.25555236295455663 ACC: 0.9601995219785929 Top2: 0.9793103448275862 AUC: 0.9886575797114923 MCC: 0.7924890501179502\n",
      "Val Epoch: 112, Ave Loss: 1.1080371311725834 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9126869520371326 MCC: 0.5226877806064516\n",
      "Train Epoch: 113, Ave Loss: 0.24798287177266778 ACC: 0.9624857113166372 Top2: 0.9839080459770115 AUC: 0.9894790104086901 MCC: 0.8006566110049449\n",
      "Val Epoch: 113, Ave Loss: 1.2232559100826754 ACC: 0.9078267779102106 Top2: 0.8256880733944955 AUC: 0.9058261346054668 MCC: 0.5040358159486049\n",
      "Train Epoch: 114, Ave Loss: 0.2497193863883616 ACC: 0.9601995219785929 Top2: 0.9862068965517241 AUC: 0.9894127453229333 MCC: 0.7902552619927499\n",
      "Val Epoch: 114, Ave Loss: 1.2121871718114194 ACC: 0.911005164878824 Top2: 0.8073394495412844 AUC: 0.9072032942238267 MCC: 0.5226667472577232\n",
      "Train Epoch: 115, Ave Loss: 0.24304880845184174 ACC: 0.9617582874363504 Top2: 0.9793103448275862 AUC: 0.9896535066101235 MCC: 0.7982662538710728\n",
      "Val Epoch: 115, Ave Loss: 1.1107925131350678 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9105837416193914 MCC: 0.5270913094065779\n",
      "Train Epoch: 116, Ave Loss: 0.23929322671497325 ACC: 0.9635248882884755 Top2: 0.9839080459770115 AUC: 0.9901701402055523 MCC: 0.8063336799252252\n",
      "Val Epoch: 116, Ave Loss: 1.2558530093875069 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9085208225889634 MCC: 0.5169140675144476\n",
      "Train Epoch: 117, Ave Loss: 0.24573064759437657 ACC: 0.9612386989504312 Top2: 0.9816091954022989 AUC: 0.989649549653783 MCC: 0.7952967561351285\n",
      "Val Epoch: 117, Ave Loss: 1.2290287736721588 ACC: 0.9082240762812872 Top2: 0.8073394495412844 AUC: 0.9063974664775658 MCC: 0.5262781666057321\n",
      "Train Epoch: 118, Ave Loss: 0.2500984542847955 ACC: 0.9610308635560636 Top2: 0.9793103448275862 AUC: 0.9890376147155082 MCC: 0.793900810204909\n",
      "Val Epoch: 118, Ave Loss: 1.2386501916053272 ACC: 0.9090186730234406 Top2: 0.8256880733944955 AUC: 0.9037607980917998 MCC: 0.533321817639856\n",
      "Train Epoch: 119, Ave Loss: 0.2421947785227015 ACC: 0.9611347812532475 Top2: 0.9816091954022989 AUC: 0.9898161319425356 MCC: 0.7945253648876635\n",
      "Val Epoch: 119, Ave Loss: 1.2724311156876582 ACC: 0.9102105681366707 Top2: 0.8348623853211009 AUC: 0.911450006446622 MCC: 0.5154790146802881\n",
      "Train Epoch: 120, Ave Loss: 0.2487575400556909 ACC: 0.9612386989504312 Top2: 0.9770114942528736 AUC: 0.9894845278548549 MCC: 0.7958927808396122\n",
      "Val Epoch: 120, Ave Loss: 1.2386787332965001 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.9092557374935535 MCC: 0.5144220444886393\n",
      "Train Epoch: 121, Ave Loss: 0.24371760241226162 ACC: 0.9621739582250858 Top2: 0.9816091954022989 AUC: 0.989513675575503 MCC: 0.800320320004232\n",
      "Val Epoch: 121, Ave Loss: 1.1662299016257214 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9115507349149046 MCC: 0.5328246463186501\n",
      "Train Epoch: 122, Ave Loss: 0.2378927398887913 ACC: 0.9631092174997402 Top2: 0.9839080459770115 AUC: 0.9901452838319209 MCC: 0.8064792760168115\n",
      "Val Epoch: 122, Ave Loss: 1.2942152889394694 ACC: 0.9082240762812872 Top2: 0.8165137614678899 AUC: 0.9055932503867974 MCC: 0.5054390543027287\n",
      "Train Epoch: 123, Ave Loss: 0.2380373134343255 ACC: 0.9621739582250858 Top2: 0.9816091954022989 AUC: 0.9900367183255677 MCC: 0.8013429243073418\n",
      "Val Epoch: 123, Ave Loss: 1.2177499505308609 ACC: 0.9094159713945172 Top2: 0.8256880733944955 AUC: 0.9073854112944817 MCC: 0.5008841963214578\n",
      "Train Epoch: 124, Ave Loss: 0.2335586333091602 ACC: 0.9632131351969241 Top2: 0.9793103448275862 AUC: 0.9906321566571277 MCC: 0.804474304921423\n",
      "Val Epoch: 124, Ave Loss: 1.3080441704833743 ACC: 0.9082240762812872 Top2: 0.7981651376146789 AUC: 0.9090381640020629 MCC: 0.5099134573833556\n",
      "Train Epoch: 125, Ave Loss: 0.23973941703109064 ACC: 0.9605112750701444 Top2: 0.9816091954022989 AUC: 0.9900062887739922 MCC: 0.7929122373293982\n",
      "Val Epoch: 125, Ave Loss: 1.2509896698149632 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9116990072202166 MCC: 0.5136577752254269\n",
      "Train Epoch: 126, Ave Loss: 0.24196280360996056 ACC: 0.9615504520419828 Top2: 0.9839080459770115 AUC: 0.9897620721164768 MCC: 0.7965846703869385\n",
      "Val Epoch: 126, Ave Loss: 1.3251067010211597 ACC: 0.9062375844259039 Top2: 0.8348623853211009 AUC: 0.9044675090252708 MCC: 0.47699391148040965\n",
      "Train Epoch: 127, Ave Loss: 0.23340920670669674 ACC: 0.9627974644081887 Top2: 0.9839080459770115 AUC: 0.9903887202445243 MCC: 0.8044768387790598\n",
      "Val Epoch: 127, Ave Loss: 1.3277977331573654 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9057665033522434 MCC: 0.5013026608214491\n",
      "Train Epoch: 128, Ave Loss: 0.23418060195514637 ACC: 0.9629013821053726 Top2: 0.9839080459770115 AUC: 0.9905746414607433 MCC: 0.8038225039573383\n",
      "Val Epoch: 128, Ave Loss: 1.230196691289733 ACC: 0.9141835518474374 Top2: 0.8256880733944955 AUC: 0.9093467960288808 MCC: 0.523393589467457\n",
      "Train Epoch: 129, Ave Loss: 0.2325346476363302 ACC: 0.9634209705912917 Top2: 0.9816091954022989 AUC: 0.9906164960271043 MCC: 0.80790159016445\n",
      "Val Epoch: 129, Ave Loss: 1.2546844931680217 ACC: 0.9086213746523639 Top2: 0.8348623853211009 AUC: 0.9093604951005674 MCC: 0.5023954915745683\n",
      "Train Epoch: 130, Ave Loss: 0.23195792872906443 ACC: 0.9620700405279019 Top2: 0.9816091954022989 AUC: 0.9905505653320243 MCC: 0.8005734466510979\n",
      "Val Epoch: 130, Ave Loss: 1.249056167454793 ACC: 0.9090186730234406 Top2: 0.8440366972477065 AUC: 0.9108416064981949 MCC: 0.5053156817410007\n",
      "Train Epoch: 131, Ave Loss: 0.23612343430307592 ACC: 0.9639405590772109 Top2: 0.9839080459770115 AUC: 0.9902770894903017 MCC: 0.8104244922622683\n",
      "Val Epoch: 131, Ave Loss: 1.3118391361470316 ACC: 0.9090186730234406 Top2: 0.8256880733944955 AUC: 0.9044102952552863 MCC: 0.5038386869066614\n",
      "Train Epoch: 132, Ave Loss: 0.23652576894780256 ACC: 0.9624857113166372 Top2: 0.9862068965517241 AUC: 0.9902779254669933 MCC: 0.8014780221675393\n",
      "Val Epoch: 132, Ave Loss: 1.3461329011241756 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9080663357400722 MCC: 0.524147265121951\n",
      "Train Epoch: 133, Ave Loss: 0.23877610207962047 ACC: 0.9631092174997402 Top2: 0.9862068965517241 AUC: 0.9902586422713063 MCC: 0.807215730602131\n",
      "Val Epoch: 133, Ave Loss: 1.3150912913815023 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9066045642083548 MCC: 0.5156923969794759\n",
      "Train Epoch: 134, Ave Loss: 0.22853176236352282 ACC: 0.9643562298659462 Top2: 0.9816091954022989 AUC: 0.9909136021433105 MCC: 0.8123403449902714\n",
      "Val Epoch: 134, Ave Loss: 1.3923075567537635 ACC: 0.9153754469606674 Top2: 0.8532110091743119 AUC: 0.909421738009283 MCC: 0.5347919288677355\n",
      "Train Epoch: 135, Ave Loss: 0.2323823894049649 ACC: 0.963836641380027 Top2: 0.9747126436781609 AUC: 0.9907235567754127 MCC: 0.8108020305259471\n",
      "Val Epoch: 135, Ave Loss: 1.2646054933653983 ACC: 0.9129916567342073 Top2: 0.8532110091743119 AUC: 0.9130318463125322 MCC: 0.5173579103682175\n",
      "Train Epoch: 136, Ave Loss: 0.2313442977689696 ACC: 0.9634209705912917 Top2: 0.9816091954022989 AUC: 0.9905603183934267 MCC: 0.8080448621608163\n",
      "Val Epoch: 136, Ave Loss: 1.2566403516252107 ACC: 0.9070321811680572 Top2: 0.8256880733944955 AUC: 0.9034553893759669 MCC: 0.4951869137898573\n",
      "Train Epoch: 137, Ave Loss: 0.23957319850219727 ACC: 0.9629013821053726 Top2: 0.9862068965517241 AUC: 0.9901243844146302 MCC: 0.8048078755202754\n",
      "Val Epoch: 137, Ave Loss: 1.2862171296640663 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.9089060082516762 MCC: 0.5255285150805782\n",
      "Train Epoch: 138, Ave Loss: 0.23006346367916736 ACC: 0.9645640652603138 Top2: 0.9816091954022989 AUC: 0.9907812391671353 MCC: 0.8135721227377338\n",
      "Val Epoch: 138, Ave Loss: 1.3233842377739407 ACC: 0.9094159713945172 Top2: 0.8256880733944955 AUC: 0.9102227307890665 MCC: 0.49501090173272827\n",
      "Train Epoch: 139, Ave Loss: 0.22597003535772467 ACC: 0.9639405590772109 Top2: 0.9770114942528736 AUC: 0.9909999864014458 MCC: 0.8098732721552503\n",
      "Val Epoch: 139, Ave Loss: 1.363532628175504 ACC: 0.9106078665077473 Top2: 0.8165137614678899 AUC: 0.909587738525013 MCC: 0.5053692312575527\n",
      "Train Epoch: 140, Ave Loss: 0.22200335146577102 ACC: 0.9641483944715785 Top2: 0.9839080459770115 AUC: 0.9913199982790026 MCC: 0.8127950218850145\n",
      "Val Epoch: 140, Ave Loss: 1.4106592064829642 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9049268308406395 MCC: 0.4981694079204207\n",
      "Train Epoch: 141, Ave Loss: 0.2231506760000343 ACC: 0.963836641380027 Top2: 0.9839080459770115 AUC: 0.991109164957373 MCC: 0.8100878121168194\n",
      "Val Epoch: 141, Ave Loss: 1.324441538234085 ACC: 0.9102105681366707 Top2: 0.8440366972477065 AUC: 0.9120285907684373 MCC: 0.5067672548159433\n",
      "Train Epoch: 142, Ave Loss: 0.2270559684318759 ACC: 0.9641483944715785 Top2: 0.9770114942528736 AUC: 0.9909798672290672 MCC: 0.8130867131629586\n",
      "Val Epoch: 142, Ave Loss: 1.3129731998031258 ACC: 0.9070321811680572 Top2: 0.8348623853211009 AUC: 0.9102428764827231 MCC: 0.46929341505309746\n",
      "Train Epoch: 143, Ave Loss: 0.22255401205090294 ACC: 0.9634209705912917 Top2: 0.9839080459770115 AUC: 0.9913215587688271 MCC: 0.8077590686996277\n",
      "Val Epoch: 143, Ave Loss: 1.3957634714696798 ACC: 0.9094159713945172 Top2: 0.8348623853211009 AUC: 0.9108714221248067 MCC: 0.5008841963214578\n",
      "Train Epoch: 144, Ave Loss: 0.22187460243888865 ACC: 0.9640444767743946 Top2: 0.9839080459770115 AUC: 0.9913742253003999 MCC: 0.8116043382158477\n",
      "Val Epoch: 144, Ave Loss: 1.301999177410985 ACC: 0.909813269765594 Top2: 0.8165137614678899 AUC: 0.9098939530685921 MCC: 0.515512332825879\n",
      "Train Epoch: 145, Ave Loss: 0.21909966615658205 ACC: 0.9652914891406006 Top2: 0.9793103448275862 AUC: 0.9915955919283441 MCC: 0.8181457318191727\n",
      "Val Epoch: 145, Ave Loss: 1.4195738652685135 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9072742070654977 MCC: 0.5084046444125043\n",
      "Train Epoch: 146, Ave Loss: 0.22374334627724363 ACC: 0.9631092174997402 Top2: 0.9816091954022989 AUC: 0.9911761545562625 MCC: 0.8061898452210698\n",
      "Val Epoch: 146, Ave Loss: 1.337586727189039 ACC: 0.9074294795391339 Top2: 0.8256880733944955 AUC: 0.9074410134089737 MCC: 0.49660543067476987\n",
      "Train Epoch: 147, Ave Loss: 0.21993055476294915 ACC: 0.9656032422321521 Top2: 0.9839080459770115 AUC: 0.9914389856281116 MCC: 0.8199845983827873\n",
      "Val Epoch: 147, Ave Loss: 1.3756475222854123 ACC: 0.9094159713945172 Top2: 0.8073394495412844 AUC: 0.9067238267148015 MCC: 0.49501090173272827\n",
      "Train Epoch: 148, Ave Loss: 0.21910273048900442 ACC: 0.9645640652603138 Top2: 0.9839080459770115 AUC: 0.991457321383548 MCC: 0.8145405828650601\n",
      "Val Epoch: 148, Ave Loss: 1.4415400881974274 ACC: 0.911005164878824 Top2: 0.8073394495412844 AUC: 0.9067560598246519 MCC: 0.5126159047343926\n",
      "Train Epoch: 149, Ave Loss: 0.21961390305948467 ACC: 0.9636288059856594 Top2: 0.9862068965517241 AUC: 0.9915240323235404 MCC: 0.8082945412595293\n",
      "Val Epoch: 149, Ave Loss: 1.336403352225791 ACC: 0.911005164878824 Top2: 0.8073394495412844 AUC: 0.9094547769468799 MCC: 0.5197934570444092\n",
      "Train Epoch: 150, Ave Loss: 0.2187135408281076 ACC: 0.96446014756313 Top2: 0.9816091954022989 AUC: 0.9915542946797774 MCC: 0.8125526215485512\n",
      "Val Epoch: 150, Ave Loss: 1.4192400255504571 ACC: 0.9102105681366707 Top2: 0.8073394495412844 AUC: 0.9112042289840123 MCC: 0.5067672548159433\n",
      "Train Epoch: 151, Ave Loss: 0.22524787481899725 ACC: 0.9646679829574977 Top2: 0.9862068965517241 AUC: 0.9911593235588709 MCC: 0.8167496921569118\n",
      "Val Epoch: 151, Ave Loss: 1.3006543510261317 ACC: 0.9114024632499007 Top2: 0.8165137614678899 AUC: 0.9105990523465703 MCC: 0.5183810583403471\n",
      "Train Epoch: 152, Ave Loss: 0.2223599160530308 ACC: 0.9643562298659462 Top2: 0.9839080459770115 AUC: 0.9912524513623189 MCC: 0.8130292852639593\n",
      "Val Epoch: 152, Ave Loss: 1.4113070895060829 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9101639053635895 MCC: 0.5212365394845138\n",
      "Train Epoch: 153, Ave Loss: 0.2183210361375072 ACC: 0.9641483944715785 Top2: 0.9839080459770115 AUC: 0.9917003119419159 MCC: 0.812506254237418\n",
      "Val Epoch: 153, Ave Loss: 1.3695610577891817 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.9106933341928829 MCC: 0.511251685495539\n",
      "Train Epoch: 154, Ave Loss: 0.21836447334905695 ACC: 0.9645640652603138 Top2: 0.9816091954022989 AUC: 0.9915590876128095 MCC: 0.8149669516986106\n",
      "Val Epoch: 154, Ave Loss: 1.4348205676222345 ACC: 0.9102105681366707 Top2: 0.8256880733944955 AUC: 0.9058833483754513 MCC: 0.4980660920443985\n",
      "Train Epoch: 155, Ave Loss: 0.22261816042032392 ACC: 0.9639405590772109 Top2: 0.9862068965517241 AUC: 0.9914981170460998 MCC: 0.8100099157203142\n",
      "Val Epoch: 155, Ave Loss: 1.3258639380415642 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9085385507993813 MCC: 0.5382679425754832\n",
      "Train Epoch: 156, Ave Loss: 0.22138421068722025 ACC: 0.9648758183518653 Top2: 0.9816091954022989 AUC: 0.9914062710735791 MCC: 0.8170967865142321\n",
      "Val Epoch: 156, Ave Loss: 1.3028260842988315 ACC: 0.909813269765594 Top2: 0.8348623853211009 AUC: 0.9086634541000517 MCC: 0.5213528118541859\n",
      "Train Epoch: 157, Ave Loss: 0.2202489149820059 ACC: 0.9643562298659462 Top2: 0.9839080459770115 AUC: 0.9912965351998575 MCC: 0.8138809867918125\n",
      "Val Epoch: 157, Ave Loss: 1.4306147005706715 ACC: 0.9117997616209773 Top2: 0.8165137614678899 AUC: 0.90551508509541 MCC: 0.5170130946051832\n",
      "Train Epoch: 158, Ave Loss: 0.2232223918915326 ACC: 0.9645640652603138 Top2: 0.9839080459770115 AUC: 0.9913136448561463 MCC: 0.8141209534718173\n",
      "Val Epoch: 158, Ave Loss: 1.324383471772608 ACC: 0.912197059992054 Top2: 0.8623853211009175 AUC: 0.9111212287261475 MCC: 0.5283843302901519\n",
      "Train Epoch: 159, Ave Loss: 0.2167311805572488 ACC: 0.9653954068377845 Top2: 0.9839080459770115 AUC: 0.9917180346477785 MCC: 0.8194506525445509\n",
      "Val Epoch: 159, Ave Loss: 1.3427575842508488 ACC: 0.9114024632499007 Top2: 0.8165137614678899 AUC: 0.9099100696235173 MCC: 0.5069823420172274\n",
      "Train Epoch: 160, Ave Loss: 0.2146505152664939 ACC: 0.965707159929336 Top2: 0.9839080459770115 AUC: 0.991929871141438 MCC: 0.8200584942251069\n",
      "Val Epoch: 160, Ave Loss: 1.3784914309859615 ACC: 0.9094159713945172 Top2: 0.8073394495412844 AUC: 0.9049292483238782 MCC: 0.5038215249928114\n",
      "Train Epoch: 161, Ave Loss: 0.21690474159362344 ACC: 0.9651875714434168 Top2: 0.9816091954022989 AUC: 0.9917885910805523 MCC: 0.8171257794717623\n",
      "Val Epoch: 161, Ave Loss: 1.3021008230990372 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9109423349664776 MCC: 0.5140259328072316\n",
      "Train Epoch: 162, Ave Loss: 0.2168817949627259 ACC: 0.965083653746233 Top2: 0.9839080459770115 AUC: 0.9916347713692919 MCC: 0.8159803325005457\n",
      "Val Epoch: 162, Ave Loss: 1.335868718807074 ACC: 0.9086213746523639 Top2: 0.8440366972477065 AUC: 0.9104233818978855 MCC: 0.5157496831366416\n",
      "Train Epoch: 163, Ave Loss: 0.22515803500735662 ACC: 0.9648758183518653 Top2: 0.9885057471264368 AUC: 0.9910636878253483 MCC: 0.8158272022315165\n",
      "Val Epoch: 163, Ave Loss: 1.4028248533235488 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9075046738009283 MCC: 0.5186117432374951\n",
      "Train Epoch: 164, Ave Loss: 0.22536772425423782 ACC: 0.9636288059856594 Top2: 0.9885057471264368 AUC: 0.9911859076176648 MCC: 0.8069528671799671\n",
      "Val Epoch: 164, Ave Loss: 1.3383571948944428 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9086924638989171 MCC: 0.5198085745236014\n",
      "Train Epoch: 165, Ave Loss: 0.21801236594466328 ACC: 0.9652914891406006 Top2: 0.9839080459770115 AUC: 0.9916329322205704 MCC: 0.8189785747299041\n",
      "Val Epoch: 165, Ave Loss: 1.3666107278442574 ACC: 0.9106078665077473 Top2: 0.8256880733944955 AUC: 0.9089285714285714 MCC: 0.5096958388750882\n",
      "Train Epoch: 166, Ave Loss: 0.21430527225319873 ACC: 0.96446014756313 Top2: 0.9862068965517241 AUC: 0.9917712027653662 MCC: 0.8136448246217909\n",
      "Val Epoch: 166, Ave Loss: 1.3978504060685886 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9095506704486849 MCC: 0.5169140675144476\n",
      "Train Epoch: 167, Ave Loss: 0.21140246332957632 ACC: 0.965083653746233 Top2: 0.9862068965517241 AUC: 0.992017704425839 MCC: 0.818036379184445\n",
      "Val Epoch: 167, Ave Loss: 1.3314568505978834 ACC: 0.9094159713945172 Top2: 0.8440366972477065 AUC: 0.9096973310985044 MCC: 0.5199717086171126\n",
      "Train Epoch: 168, Ave Loss: 0.2126330245415098 ACC: 0.9667463369011743 Top2: 0.9885057471264368 AUC: 0.9921302268885327 MCC: 0.8257773576897187\n",
      "Val Epoch: 168, Ave Loss: 1.3914378935789518 ACC: 0.9106078665077473 Top2: 0.7889908256880734 AUC: 0.9029332129963898 MCC: 0.521247683568366\n",
      "Train Epoch: 169, Ave Loss: 0.21444444797809042 ACC: 0.9654993245349683 Top2: 0.9862068965517241 AUC: 0.9918165684338321 MCC: 0.8191007686461048\n",
      "Val Epoch: 169, Ave Loss: 1.3496699773155392 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9116305118617845 MCC: 0.5242126193427202\n",
      "Train Epoch: 170, Ave Loss: 0.21017214319441302 ACC: 0.9651875714434168 Top2: 0.9839080459770115 AUC: 0.9920581099659345 MCC: 0.8171257794717623\n",
      "Val Epoch: 170, Ave Loss: 1.4628022939020302 ACC: 0.9106078665077473 Top2: 0.8165137614678899 AUC: 0.9056552991232595 MCC: 0.5255822721102814\n",
      "Train Epoch: 171, Ave Loss: 0.21386746779840643 ACC: 0.9656032422321521 Top2: 0.9862068965517241 AUC: 0.9918836694962806 MCC: 0.8199845983827873\n",
      "Val Epoch: 171, Ave Loss: 1.3835381514918064 ACC: 0.9102105681366707 Top2: 0.8348623853211009 AUC: 0.9056810856111397 MCC: 0.5285503959122441\n",
      "Train Epoch: 172, Ave Loss: 0.21169418595185224 ACC: 0.96446014756313 Top2: 0.9816091954022989 AUC: 0.9919740664425358 MCC: 0.815373406013656\n",
      "Val Epoch: 172, Ave Loss: 1.4637154176918157 ACC: 0.911005164878824 Top2: 0.8165137614678899 AUC: 0.9083024432697266 MCC: 0.4983034711188838\n",
      "Train Epoch: 173, Ave Loss: 0.20974579906439494 ACC: 0.9643562298659462 Top2: 0.9839080459770115 AUC: 0.9920776718205186 MCC: 0.8137371786454954\n",
      "Val Epoch: 173, Ave Loss: 1.474268842801694 ACC: 0.9137862534763608 Top2: 0.8440366972477065 AUC: 0.9093838641052089 MCC: 0.5232034166367038\n",
      "Train Epoch: 174, Ave Loss: 0.2100437461795611 ACC: 0.9659149953237036 Top2: 0.9839080459770115 AUC: 0.9922590787626029 MCC: 0.8203722132096979\n",
      "Val Epoch: 174, Ave Loss: 1.3878882728445499 ACC: 0.9086213746523639 Top2: 0.8348623853211009 AUC: 0.9063241361526561 MCC: 0.5187127940281329\n",
      "Train Epoch: 175, Ave Loss: 0.21096226688399944 ACC: 0.9658110776265197 Top2: 0.9885057471264368 AUC: 0.9921058721009164 MCC: 0.8221800101618582\n",
      "Val Epoch: 175, Ave Loss: 1.4279546537552694 ACC: 0.9141835518474374 Top2: 0.8348623853211009 AUC: 0.9090156008251676 MCC: 0.5329394704499809\n",
      "Train Epoch: 176, Ave Loss: 0.2117956036146496 ACC: 0.9647719006546815 Top2: 0.9839080459770115 AUC: 0.9919068539165286 MCC: 0.8142671136579799\n",
      "Val Epoch: 176, Ave Loss: 1.4940660402795953 ACC: 0.9106078665077473 Top2: 0.8165137614678899 AUC: 0.9079938112429088 MCC: 0.5024873649140027\n",
      "Train Epoch: 177, Ave Loss: 0.21261102721970365 ACC: 0.9649797360490492 Top2: 0.9839080459770115 AUC: 0.9918684547204929 MCC: 0.816718189072151\n",
      "Val Epoch: 177, Ave Loss: 1.4770670270859019 ACC: 0.9102105681366707 Top2: 0.8440366972477065 AUC: 0.9081162970603404 MCC: 0.49661760915940595\n",
      "Train Epoch: 178, Ave Loss: 0.3048012054187449 ACC: 0.9578094149433648 Top2: 0.9701149425287356 AUC: 0.9844355630280726 MCC: 0.7760897863787296\n",
      "Val Epoch: 178, Ave Loss: 1.2900728577627254 ACC: 0.9137862534763608 Top2: 0.8073394495412844 AUC: 0.9083040549252193 MCC: 0.5259456831643774\n",
      "Train Epoch: 179, Ave Loss: 0.331064391399303 ACC: 0.9527174477813571 Top2: 0.9586206896551724 AUC: 0.9819889379105621 MCC: 0.7472527326470996\n",
      "Val Epoch: 179, Ave Loss: 1.1040195996505677 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9018437338834451 MCC: 0.5313620292392763\n",
      "Train Epoch: 180, Ave Loss: 0.2809394463623617 ACC: 0.9591603450067546 Top2: 0.9724137931034482 AUC: 0.9868622919003223 MCC: 0.7836686682724558\n",
      "Val Epoch: 180, Ave Loss: 1.2796442497775564 ACC: 0.9026618990862137 Top2: 0.8073394495412844 AUC: 0.9036318656523981 MCC: 0.4912354021219774\n",
      "Train Epoch: 181, Ave Loss: 0.2780619869204693 ACC: 0.9606151927673283 Top2: 0.9747126436781609 AUC: 0.9865441749032664 MCC: 0.7899631226894474\n",
      "Val Epoch: 181, Ave Loss: 1.1549682490647248 ACC: 0.9050456893126738 Top2: 0.7981651376146789 AUC: 0.9021580067044868 MCC: 0.45842752605095527\n",
      "Train Epoch: 182, Ave Loss: 0.26815242806864803 ACC: 0.9599916865842253 Top2: 0.9747126436781609 AUC: 0.9877893900513424 MCC: 0.786476989050407\n",
      "Val Epoch: 182, Ave Loss: 1.2080435817463135 ACC: 0.9094159713945172 Top2: 0.8073394495412844 AUC: 0.9074764698298092 MCC: 0.5302346920206455\n",
      "Train Epoch: 183, Ave Loss: 0.24076177167610188 ACC: 0.9611347812532475 Top2: 0.9839080459770115 AUC: 0.989718879987409 MCC: 0.7949693997916546\n",
      "Val Epoch: 183, Ave Loss: 1.2194191500509304 ACC: 0.9161700437028208 Top2: 0.8440366972477065 AUC: 0.9161560404847859 MCC: 0.5485037059333207\n",
      "Train Epoch: 184, Ave Loss: 0.2205359889292563 ACC: 0.9640444767743946 Top2: 0.9862068965517241 AUC: 0.9915117156002836 MCC: 0.8118903587389006\n",
      "Val Epoch: 184, Ave Loss: 1.2981074533589214 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9103121776689015 MCC: 0.5283843302901519\n",
      "Train Epoch: 185, Ave Loss: 0.21780442200295105 ACC: 0.9649797360490492 Top2: 0.9816091954022989 AUC: 0.9916855430203637 MCC: 0.8174230632776972\n",
      "Val Epoch: 185, Ave Loss: 1.302401727753723 ACC: 0.9153754469606674 Top2: 0.8440366972477065 AUC: 0.9150834837545126 MCC: 0.5307878093017875\n",
      "Train Epoch: 186, Ave Loss: 0.21696163755872586 ACC: 0.965083653746233 Top2: 0.9862068965517241 AUC: 0.9917534243277243 MCC: 0.8169183910363792\n",
      "Val Epoch: 186, Ave Loss: 1.3067088806414293 ACC: 0.9125943583631307 Top2: 0.7981651376146789 AUC: 0.9083314530685921 MCC: 0.5242126193427202\n",
      "Train Epoch: 187, Ave Loss: 0.21419098503207523 ACC: 0.9654993245349683 Top2: 0.9839080459770115 AUC: 0.991859649099341 MCC: 0.8196461446674417\n",
      "Val Epoch: 187, Ave Loss: 1.4223374711553567 ACC: 0.9117997616209773 Top2: 0.8165137614678899 AUC: 0.9097061952037131 MCC: 0.5099362719992304\n",
      "Train Epoch: 188, Ave Loss: 0.2135040648268927 ACC: 0.9660189130208875 Top2: 0.9839080459770115 AUC: 0.9918853971814432 MCC: 0.8209847233467389\n",
      "Val Epoch: 188, Ave Loss: 1.3585827364354999 ACC: 0.9145808502185141 Top2: 0.8165137614678899 AUC: 0.9068285843218153 MCC: 0.5494721732173631\n",
      "Train Epoch: 189, Ave Loss: 0.21152053629058692 ACC: 0.9654993245349683 Top2: 0.9816091954022989 AUC: 0.9920659124150564 MCC: 0.8195086562295746\n",
      "Val Epoch: 189, Ave Loss: 1.355999565020726 ACC: 0.9074294795391339 Top2: 0.8165137614678899 AUC: 0.9061927862300154 MCC: 0.514677004318554\n",
      "Train Epoch: 190, Ave Loss: 0.20901971132276637 ACC: 0.9658110776265197 Top2: 0.9839080459770115 AUC: 0.9924259954420321 MCC: 0.822039071406083\n",
      "Val Epoch: 190, Ave Loss: 1.4500370466319492 ACC: 0.9078267779102106 Top2: 0.8256880733944955 AUC: 0.9113105982465188 MCC: 0.48148586847710473\n",
      "Train Epoch: 191, Ave Loss: 0.21215063451216115 ACC: 0.9649797360490492 Top2: 0.9862068965517241 AUC: 0.9918719100908182 MCC: 0.8157631845420984\n",
      "Val Epoch: 191, Ave Loss: 1.3429449327719138 ACC: 0.9145808502185141 Top2: 0.8256880733944955 AUC: 0.9129053313563693 MCC: 0.53444115666355\n",
      "Train Epoch: 192, Ave Loss: 0.2103356825002963 ACC: 0.9645640652603138 Top2: 0.9862068965517241 AUC: 0.9920033256267429 MCC: 0.8155457404954229\n",
      "Val Epoch: 192, Ave Loss: 1.4171299822591117 ACC: 0.9106078665077473 Top2: 0.8256880733944955 AUC: 0.9120922511603919 MCC: 0.5154698839252518\n",
      "Train Epoch: 193, Ave Loss: 0.20601992168162037 ACC: 0.9645640652603138 Top2: 0.9862068965517241 AUC: 0.992434801063184 MCC: 0.8146819623732753\n",
      "Val Epoch: 193, Ave Loss: 1.470721767230491 ACC: 0.9106078665077473 Top2: 0.8165137614678899 AUC: 0.9090212416193915 MCC: 0.495294680752388\n",
      "Train Epoch: 194, Ave Loss: 0.2086347432765651 ACC: 0.9648758183518653 Top2: 0.9839080459770115 AUC: 0.9922927964891654 MCC: 0.8150149937864953\n",
      "Val Epoch: 194, Ave Loss: 1.4232228790604644 ACC: 0.911005164878824 Top2: 0.8256880733944955 AUC: 0.9113251031459515 MCC: 0.5040193968588875\n",
      "Train Epoch: 195, Ave Loss: 0.21069749640057794 ACC: 0.9642523121687624 Top2: 0.9862068965517241 AUC: 0.9920552119134035 MCC: 0.8131218606676545\n",
      "Val Epoch: 195, Ave Loss: 1.4876175379124457 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.910344410778752 MCC: 0.513205391062051\n",
      "Train Epoch: 196, Ave Loss: 0.20757026630775008 ACC: 0.9656032422321521 Top2: 0.9839080459770115 AUC: 0.9923309170263038 MCC: 0.8202586266888598\n",
      "Val Epoch: 196, Ave Loss: 1.4725675737055608 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9102009734399175 MCC: 0.5140506170697363\n",
      "Train Epoch: 197, Ave Loss: 0.209934526484214 ACC: 0.9652914891406006 Top2: 0.9862068965517241 AUC: 0.9921643904693307 MCC: 0.8189785747299041\n",
      "Val Epoch: 197, Ave Loss: 1.4703013817949937 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9091517857142858 MCC: 0.5175321395481265\n",
      "Train Epoch: 198, Ave Loss: 0.20724871388248445 ACC: 0.9658110776265197 Top2: 0.9816091954022989 AUC: 0.9923199378657538 MCC: 0.8200158749272457\n",
      "Val Epoch: 198, Ave Loss: 1.462297582320806 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9098069236719959 MCC: 0.5340411926224726\n",
      "Train Epoch: 199, Ave Loss: 0.20888249322643576 ACC: 0.9645640652603138 Top2: 0.9885057471264368 AUC: 0.992286944652324 MCC: 0.8138449991965546\n",
      "Val Epoch: 199, Ave Loss: 1.4080332916347047 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9101147498710676 MCC: 0.5411152306112247\n",
      "Train Epoch: 200, Ave Loss: 0.20508068078778283 ACC: 0.9670580899927258 Top2: 0.9839080459770115 AUC: 0.9925463203538478 MCC: 0.8263501777904865\n",
      "Val Epoch: 200, Ave Loss: 1.4126906634386898 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9105079938112429 MCC: 0.5269713801180247\n",
      "Train Epoch: 201, Ave Loss: 0.20629538222988214 ACC: 0.9658110776265197 Top2: 0.9862068965517241 AUC: 0.9922999858887135 MCC: 0.8206709627189652\n",
      "Val Epoch: 201, Ave Loss: 1.444419193047748 ACC: 0.9149781485895908 Top2: 0.8256880733944955 AUC: 0.9105531201650335 MCC: 0.5305628281835247\n",
      "Train Epoch: 202, Ave Loss: 0.20447899194576435 ACC: 0.9664345838096228 Top2: 0.9816091954022989 AUC: 0.9924456687601751 MCC: 0.8248757763050406\n",
      "Val Epoch: 202, Ave Loss: 1.5104867649279436 ACC: 0.9133889551052841 Top2: 0.8532110091743119 AUC: 0.9103162068076328 MCC: 0.5271911980287856\n",
      "Train Epoch: 203, Ave Loss: 0.20641369119687172 ACC: 0.9664345838096228 Top2: 0.9839080459770115 AUC: 0.9924098889577734 MCC: 0.8247406358597928\n",
      "Val Epoch: 203, Ave Loss: 1.4524633609880682 ACC: 0.9149781485895908 Top2: 0.8256880733944955 AUC: 0.9102146725116038 MCC: 0.5319074034245233\n",
      "Train Epoch: 204, Ave Loss: 0.20427974323919132 ACC: 0.9651875714434168 Top2: 0.9862068965517241 AUC: 0.9925117666505936 MCC: 0.8178069921756964\n",
      "Val Epoch: 204, Ave Loss: 1.4716480119575597 ACC: 0.9145808502185141 Top2: 0.8440366972477065 AUC: 0.912620874161939 MCC: 0.5290145314312866\n",
      "Train Epoch: 205, Ave Loss: 0.20327581367386774 ACC: 0.9669541722955419 Top2: 0.9885057471264368 AUC: 0.9925612007389588 MCC: 0.8268675899569397\n",
      "Val Epoch: 205, Ave Loss: 1.4764734118580236 ACC: 0.9145808502185141 Top2: 0.8348623853211009 AUC: 0.910557955131511 MCC: 0.5317247142446719\n",
      "Train Epoch: 206, Ave Loss: 0.20432906904781603 ACC: 0.9661228307180713 Top2: 0.9862068965517241 AUC: 0.9924671254952604 MCC: 0.8215969465651283\n",
      "Val Epoch: 206, Ave Loss: 1.4446268420639554 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9111518501805054 MCC: 0.5328246463186501\n",
      "Train Epoch: 207, Ave Loss: 0.20142227718420153 ACC: 0.9667463369011743 Top2: 0.9862068965517241 AUC: 0.9927263897332254 MCC: 0.8279528192514142\n",
      "Val Epoch: 207, Ave Loss: 1.532589037213882 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9097102243424445 MCC: 0.5173579103682175\n",
      "Train Epoch: 208, Ave Loss: 0.20463385915852816 ACC: 0.9659149953237036 Top2: 0.9862068965517241 AUC: 0.9924763212388684 MCC: 0.8203722132096979\n",
      "Val Epoch: 208, Ave Loss: 1.434322114634455 ACC: 0.9133889551052841 Top2: 0.8165137614678899 AUC: 0.9115410649819493 MCC: 0.5313488896650728\n",
      "Train Epoch: 209, Ave Loss: 0.20372198181024564 ACC: 0.9660189130208875 Top2: 0.9862068965517241 AUC: 0.9926547743966423 MCC: 0.8216309987479191\n",
      "Val Epoch: 209, Ave Loss: 1.4988835655130301 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9090317173800928 MCC: 0.534072892709803\n",
      "Train Epoch: 210, Ave Loss: 0.2033377105894123 ACC: 0.9654993245349683 Top2: 0.9862068965517241 AUC: 0.9925510575551004 MCC: 0.8209170856205319\n",
      "Val Epoch: 210, Ave Loss: 1.514104118421862 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.9115467057761732 MCC: 0.5141016139490049\n",
      "Train Epoch: 211, Ave Loss: 0.2059568340402402 ACC: 0.9665385015068066 Top2: 0.9839080459770115 AUC: 0.9923647462164252 MCC: 0.8228156474913971\n",
      "Val Epoch: 211, Ave Loss: 1.4672062900381624 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9102743037648272 MCC: 0.5342039553947123\n",
      "Train Epoch: 212, Ave Loss: 0.20181147485084405 ACC: 0.9662267484152551 Top2: 0.9885057471264368 AUC: 0.9926074023841163 MCC: 0.8224646591203656\n",
      "Val Epoch: 212, Ave Loss: 1.5176714095317565 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9096151366683858 MCC: 0.5327367841367721\n",
      "Train Epoch: 213, Ave Loss: 0.21344777907722645 ACC: 0.9653954068377845 Top2: 0.9839080459770115 AUC: 0.9920587787472879 MCC: 0.8198746277691963\n",
      "Val Epoch: 213, Ave Loss: 1.3577080067231666 ACC: 0.9074294795391339 Top2: 0.8532110091743119 AUC: 0.9022949974213511 MCC: 0.47541550934949295\n",
      "Train Epoch: 214, Ave Loss: 0.22153698598777857 ACC: 0.9640444767743946 Top2: 0.9839080459770115 AUC: 0.9914254428057072 MCC: 0.8087880837759774\n",
      "Val Epoch: 214, Ave Loss: 1.3702572681735163 ACC: 0.9161700437028208 Top2: 0.8348623853211009 AUC: 0.9133098568849922 MCC: 0.5418595125305645\n",
      "Train Epoch: 215, Ave Loss: 0.22498695476610914 ACC: 0.9630052998025563 Top2: 0.9862068965517241 AUC: 0.9913466380695759 MCC: 0.804580937253646\n",
      "Val Epoch: 215, Ave Loss: 1.3900464294781718 ACC: 0.9082240762812872 Top2: 0.8165137614678899 AUC: 0.9049010443527592 MCC: 0.5233080058704147\n",
      "Train Epoch: 216, Ave Loss: 0.22636677098478164 ACC: 0.9621739582250858 Top2: 0.9816091954022989 AUC: 0.9910977956743667 MCC: 0.803653332650806\n",
      "Val Epoch: 216, Ave Loss: 1.3939432515450347 ACC: 0.9106078665077473 Top2: 0.8165137614678899 AUC: 0.906364427539969 MCC: 0.5111388719542006\n",
      "Train Epoch: 217, Ave Loss: 0.2099978337155512 ACC: 0.9654993245349683 Top2: 0.9839080459770115 AUC: 0.9922127099221071 MCC: 0.8186998351015203\n",
      "Val Epoch: 217, Ave Loss: 1.4248809396369257 ACC: 0.912197059992054 Top2: 0.8165137614678899 AUC: 0.9103564981949457 MCC: 0.52555901145458\n",
      "Train Epoch: 218, Ave Loss: 0.20487707426144114 ACC: 0.9658110776265197 Top2: 0.9862068965517241 AUC: 0.9924922605277888 MCC: 0.8221800101618582\n",
      "Val Epoch: 218, Ave Loss: 1.4239736290461107 ACC: 0.9145808502185141 Top2: 0.8348623853211009 AUC: 0.9111308986591025 MCC: 0.53444115666355\n",
      "Train Epoch: 219, Ave Loss: 0.20524738126416622 ACC: 0.9663306661124389 Top2: 0.9862068965517241 AUC: 0.9924934308951572 MCC: 0.8233349393335498\n",
      "Val Epoch: 219, Ave Loss: 1.4255760256700667 ACC: 0.9114024632499007 Top2: 0.8165137614678899 AUC: 0.9085933470861268 MCC: 0.5183810583403471\n",
      "Train Epoch: 220, Ave Loss: 0.2026533483348511 ACC: 0.965707159929336 Top2: 0.9862068965517241 AUC: 0.9925769171007616 MCC: 0.8197939117417096\n",
      "Val Epoch: 220, Ave Loss: 1.4739824077137242 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9087045513151109 MCC: 0.5313488896650728\n",
      "Train Epoch: 221, Ave Loss: 0.20529595056313985 ACC: 0.9665385015068066 Top2: 0.9839080459770115 AUC: 0.9924138459141139 MCC: 0.8250830480005977\n",
      "Val Epoch: 221, Ave Loss: 1.422309161950371 ACC: 0.9129916567342073 Top2: 0.8165137614678899 AUC: 0.9090768437338834 MCC: 0.5298814834246461\n",
      "Train Epoch: 222, Ave Loss: 0.2038168865874715 ACC: 0.9656032422321521 Top2: 0.9862068965517241 AUC: 0.9925099832336515 MCC: 0.8203967762326007\n",
      "Val Epoch: 222, Ave Loss: 1.57127528401659 ACC: 0.9145808502185141 Top2: 0.8165137614678899 AUC: 0.9066279332129963 MCC: 0.5290145314312866\n",
      "Train Epoch: 223, Ave Loss: 0.20610096831532004 ACC: 0.9645640652603138 Top2: 0.9839080459770115 AUC: 0.9923361558135714 MCC: 0.8143999521631629\n",
      "Val Epoch: 223, Ave Loss: 1.4533360204204155 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.912243746776689 MCC: 0.5229131640968384\n",
      "Train Epoch: 224, Ave Loss: 0.20438861435610886 ACC: 0.9672659253870934 Top2: 0.9885057471264368 AUC: 0.9927035954354335 MCC: 0.8267404483284179\n",
      "Val Epoch: 224, Ave Loss: 1.4895867810299508 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9095804860752966 MCC: 0.539671137047754\n",
      "Train Epoch: 225, Ave Loss: 0.20098007672530907 ACC: 0.9661228307180713 Top2: 0.9862068965517241 AUC: 0.992623118745919 MCC: 0.8222427909640688\n",
      "Val Epoch: 225, Ave Loss: 1.5064223288725505 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9072298865394532 MCC: 0.5256975938758675\n",
      "Train Epoch: 226, Ave Loss: 0.2036469567449466 ACC: 0.9651875714434168 Top2: 0.9839080459770115 AUC: 0.9926306982679233 MCC: 0.8180848106744051\n",
      "Val Epoch: 226, Ave Loss: 1.4539370371851565 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9089906201650335 MCC: 0.5241037412296022\n",
      "Train Epoch: 227, Ave Loss: 0.20191797965310443 ACC: 0.9661228307180713 Top2: 0.9862068965517241 AUC: 0.9926792963795967 MCC: 0.8242961544739473\n",
      "Val Epoch: 227, Ave Loss: 1.5349550941924845 ACC: 0.9137862534763608 Top2: 0.8440366972477065 AUC: 0.9107698878287777 MCC: 0.5273189391707551\n",
      "Train Epoch: 228, Ave Loss: 0.20252948657577555 ACC: 0.9653954068377845 Top2: 0.9862068965517241 AUC: 0.9925379605869314 MCC: 0.8187589726784553\n",
      "Val Epoch: 228, Ave Loss: 1.411097938609467 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9111961707065498 MCC: 0.5312118210278248\n",
      "Train Epoch: 229, Ave Loss: 0.20149907193308308 ACC: 0.9654993245349683 Top2: 0.9839080459770115 AUC: 0.9927837934660508 MCC: 0.8183059701758119\n",
      "Val Epoch: 229, Ave Loss: 1.4738886291788313 ACC: 0.9149781485895908 Top2: 0.8348623853211009 AUC: 0.9121736397627642 MCC: 0.5319074034245233\n",
      "Train Epoch: 230, Ave Loss: 0.20042328297430953 ACC: 0.965083653746233 Top2: 0.9862068965517241 AUC: 0.9926124739760456 MCC: 0.8181794736726103\n",
      "Val Epoch: 230, Ave Loss: 1.497664069515424 ACC: 0.9157727453317441 Top2: 0.8256880733944955 AUC: 0.9112574136152657 MCC: 0.537668189113936\n",
      "Train Epoch: 231, Ave Loss: 0.19901841285772948 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9927658478330704 MCC: 0.8269968791877753\n",
      "Val Epoch: 231, Ave Loss: 1.5232054525174945 ACC: 0.9149781485895908 Top2: 0.8256880733944955 AUC: 0.9113621712222795 MCC: 0.5332537699285085\n",
      "Train Epoch: 232, Ave Loss: 0.20183205741753135 ACC: 0.9662267484152551 Top2: 0.9862068965517241 AUC: 0.992628914850981 MCC: 0.8231179931937479\n",
      "Val Epoch: 232, Ave Loss: 1.5447805841615363 ACC: 0.9153754469606674 Top2: 0.8256880733944955 AUC: 0.9098383509541 MCC: 0.5307878093017875\n",
      "Train Epoch: 233, Ave Loss: 0.2013183030839328 ACC: 0.9669541722955419 Top2: 0.9839080459770115 AUC: 0.9926873774876159 MCC: 0.8280591133179512\n",
      "Val Epoch: 233, Ave Loss: 1.479536723533458 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.9073950812274368 MCC: 0.5198491937890899\n",
      "Train Epoch: 234, Ave Loss: 0.19949700738274734 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9927377032844522 MCC: 0.8261303630516889\n",
      "Val Epoch: 234, Ave Loss: 1.5268335273511917 ACC: 0.9145808502185141 Top2: 0.8348623853211009 AUC: 0.9101260314595152 MCC: 0.5398907756072023\n",
      "Train Epoch: 235, Ave Loss: 0.20095993817547658 ACC: 0.9665385015068066 Top2: 0.9862068965517241 AUC: 0.9928065320320631 MCC: 0.8256217192665157\n",
      "Val Epoch: 235, Ave Loss: 1.5329020917979672 ACC: 0.9141835518474374 Top2: 0.8440366972477065 AUC: 0.9096489814337287 MCC: 0.527475516695514\n",
      "Train Epoch: 236, Ave Loss: 0.19902146631212445 ACC: 0.9665385015068066 Top2: 0.9885057471264368 AUC: 0.992757488066154 MCC: 0.8235421351466117\n",
      "Val Epoch: 236, Ave Loss: 1.492956974651569 ACC: 0.9169646404449742 Top2: 0.8256880733944955 AUC: 0.9095176315110881 MCC: 0.5423366658830974\n",
      "Train Epoch: 237, Ave Loss: 0.19870240926128574 ACC: 0.9673698430842772 Top2: 0.9839080459770115 AUC: 0.9929123666812238 MCC: 0.8285465248632237\n",
      "Val Epoch: 237, Ave Loss: 1.5531097935110347 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9093733883445074 MCC: 0.5312778452810839\n",
      "Train Epoch: 238, Ave Loss: 0.19888876289888963 ACC: 0.9667463369011743 Top2: 0.9885057471264368 AUC: 0.9927571536754775 MCC: 0.8261704286672289\n",
      "Val Epoch: 238, Ave Loss: 1.5372855692599683 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9090333290355854 MCC: 0.5213258124820263\n",
      "Train Epoch: 239, Ave Loss: 0.19850791228368678 ACC: 0.9647719006546815 Top2: 0.9862068965517241 AUC: 0.9928819928614279 MCC: 0.815075222998177\n",
      "Val Epoch: 239, Ave Loss: 1.5503003166387324 ACC: 0.9153754469606674 Top2: 0.8256880733944955 AUC: 0.9093669417225373 MCC: 0.5428488993348447\n",
      "Train Epoch: 240, Ave Loss: 0.1989151223745944 ACC: 0.9671620076899096 Top2: 0.9839080459770115 AUC: 0.9928523435547646 MCC: 0.8292761510689758\n",
      "Val Epoch: 240, Ave Loss: 1.582427721578165 ACC: 0.9149781485895908 Top2: 0.8440366972477065 AUC: 0.9096119133574005 MCC: 0.5346018540883455\n",
      "Train Epoch: 241, Ave Loss: 0.1983215622497593 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9929109733867377 MCC: 0.8263501777904865\n",
      "Val Epoch: 241, Ave Loss: 1.5357991654634693 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9084644146467251 MCC: 0.5298270549069729\n",
      "Train Epoch: 242, Ave Loss: 0.19905253810431683 ACC: 0.9664345838096228 Top2: 0.9862068965517241 AUC: 0.9928311654785766 MCC: 0.8254238900715266\n",
      "Val Epoch: 242, Ave Loss: 1.5398488988279446 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9104814014956162 MCC: 0.5258074743467643\n",
      "Train Epoch: 243, Ave Loss: 0.19655365258006807 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.992970996513197 MCC: 0.8258707122960701\n",
      "Val Epoch: 243, Ave Loss: 1.5247372803029449 ACC: 0.9145808502185141 Top2: 0.8256880733944955 AUC: 0.908877804280557 MCC: 0.5398907756072023\n",
      "Train Epoch: 244, Ave Loss: 0.19789180599922743 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9928712366279955 MCC: 0.8273894418397205\n",
      "Val Epoch: 244, Ave Loss: 1.6238829862716624 ACC: 0.9157727453317441 Top2: 0.8165137614678899 AUC: 0.9095281072717896 MCC: 0.5443405532530046\n",
      "Train Epoch: 245, Ave Loss: 0.1973232689606935 ACC: 0.9654993245349683 Top2: 0.9862068965517241 AUC: 0.9929709407814175 MCC: 0.82062951007358\n",
      "Val Epoch: 245, Ave Loss: 1.5333553316904227 ACC: 0.9145808502185141 Top2: 0.8348623853211009 AUC: 0.9107126740587932 MCC: 0.5317247142446719\n",
      "Train Epoch: 246, Ave Loss: 0.19766805249825742 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9929376689090905 MCC: 0.8289538907466791\n",
      "Val Epoch: 246, Ave Loss: 1.5846181223814626 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9091018243940174 MCC: 0.5185073890229217\n",
      "Train Epoch: 247, Ave Loss: 0.19729969892524277 ACC: 0.9667463369011743 Top2: 0.9885057471264368 AUC: 0.9928316670645916 MCC: 0.8260386255072951\n",
      "Val Epoch: 247, Ave Loss: 1.588869232347577 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9095466413099536 MCC: 0.5039280114346477\n",
      "Train Epoch: 248, Ave Loss: 0.19880167418675013 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9929232901099945 MCC: 0.8292794037892849\n",
      "Val Epoch: 248, Ave Loss: 1.6019781271810594 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9092799123259412 MCC: 0.5125822496943438\n",
      "Train Epoch: 249, Ave Loss: 0.1972467367629489 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.992898266541025 MCC: 0.8291113818645677\n",
      "Val Epoch: 249, Ave Loss: 1.6271247502301125 ACC: 0.9129916567342073 Top2: 0.8165137614678899 AUC: 0.9091018243940175 MCC: 0.5298814834246461\n",
      "Train Epoch: 250, Ave Loss: 0.1956034254955375 ACC: 0.9663306661124389 Top2: 0.9862068965517241 AUC: 0.9931613762717714 MCC: 0.8248138417160172\n",
      "Val Epoch: 250, Ave Loss: 1.569968593810082 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9095426121712223 MCC: 0.5271911980287856\n",
      "Train Epoch: 251, Ave Loss: 0.1974327886076928 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9928764754152631 MCC: 0.8289214678898698\n",
      "Val Epoch: 251, Ave Loss: 1.5266182573330864 ACC: 0.9086213746523639 Top2: 0.8348623853211009 AUC: 0.907932568334193 MCC: 0.509818219388469\n",
      "Train Epoch: 252, Ave Loss: 0.19813525593219294 ACC: 0.9672659253870934 Top2: 0.9885057471264368 AUC: 0.9929342692705445 MCC: 0.8281867744976612\n",
      "Val Epoch: 252, Ave Loss: 1.5274171232498652 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9071404396596183 MCC: 0.5326749961115761\n",
      "Train Epoch: 253, Ave Loss: 0.19600300188224995 ACC: 0.9664345838096228 Top2: 0.9862068965517241 AUC: 0.9929143730252837 MCC: 0.8242077483225709\n",
      "Val Epoch: 253, Ave Loss: 1.580136784035833 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9073233625580195 MCC: 0.5256975938758675\n",
      "Train Epoch: 254, Ave Loss: 0.19673879819557105 ACC: 0.9670580899927258 Top2: 0.9885057471264368 AUC: 0.9929244047455833 MCC: 0.8270941452060375\n",
      "Val Epoch: 254, Ave Loss: 1.5857216389329452 ACC: 0.9137862534763608 Top2: 0.7981651376146789 AUC: 0.9099906523981434 MCC: 0.5328246463186501\n",
      "Train Epoch: 255, Ave Loss: 0.19640439438876647 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9929728913936979 MCC: 0.8297838263146962\n",
      "Val Epoch: 255, Ave Loss: 1.592549388348835 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9101058857658586 MCC: 0.5299619597251506\n",
      "Train Epoch: 256, Ave Loss: 0.1963540095618272 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9928320571870477 MCC: 0.8295623140155006\n",
      "Val Epoch: 256, Ave Loss: 1.5763051854560342 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9068003803506962 MCC: 0.5283843302901519\n",
      "Train Epoch: 257, Ave Loss: 0.19651455840933663 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9929869915338969 MCC: 0.8309809176943518\n",
      "Val Epoch: 257, Ave Loss: 1.5906158743144578 ACC: 0.9149781485895908 Top2: 0.8165137614678899 AUC: 0.9090857078390923 MCC: 0.5373028960036114\n",
      "Train Epoch: 258, Ave Loss: 0.19353956827346558 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9932287002613375 MCC: 0.8350571488936331\n",
      "Val Epoch: 258, Ave Loss: 1.6703290123521983 ACC: 0.9094159713945172 Top2: 0.8256880733944955 AUC: 0.9079148401237751 MCC: 0.4920756059972139\n",
      "Train Epoch: 259, Ave Loss: 0.19582674876327322 ACC: 0.9671620076899096 Top2: 0.9885057471264368 AUC: 0.9930742675005032 MCC: 0.8256750957623125\n",
      "Val Epoch: 259, Ave Loss: 1.5763246761938334 ACC: 0.9153754469606674 Top2: 0.8256880733944955 AUC: 0.9108198491490459 MCC: 0.5415020275153493\n",
      "Train Epoch: 260, Ave Loss: 0.1959577410812317 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9930314097621122 MCC: 0.825862064096138\n",
      "Val Epoch: 260, Ave Loss: 1.6128846896193063 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9072105466735432 MCC: 0.5355843104441431\n",
      "Train Epoch: 261, Ave Loss: 0.19645153455164968 ACC: 0.9664345838096228 Top2: 0.9862068965517241 AUC: 0.993035032327776 MCC: 0.8264115551134349\n",
      "Val Epoch: 261, Ave Loss: 1.7213437553574054 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.907685985043837 MCC: 0.5147856218081741\n",
      "Train Epoch: 262, Ave Loss: 0.1974348009313658 ACC: 0.9663306661124389 Top2: 0.9862068965517241 AUC: 0.9928904640919031 MCC: 0.8232051438310357\n",
      "Val Epoch: 262, Ave Loss: 1.6708313004429003 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9086376676121712 MCC: 0.511349889644648\n",
      "Train Epoch: 263, Ave Loss: 0.1966698351236869 ACC: 0.965707159929336 Top2: 0.9862068965517241 AUC: 0.9929760123733468 MCC: 0.8199258109024816\n",
      "Val Epoch: 263, Ave Loss: 1.6721919551805138 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9080405492521919 MCC: 0.516954031088185\n",
      "Train Epoch: 264, Ave Loss: 0.19428886224738542 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9931669494497156 MCC: 0.8313827643807531\n",
      "Val Epoch: 264, Ave Loss: 1.5890370149242925 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9087746583290355 MCC: 0.5382706528785682\n",
      "Train Epoch: 265, Ave Loss: 0.19544532252487837 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.9930396023336903 MCC: 0.8354174261935209\n",
      "Val Epoch: 265, Ave Loss: 1.6414641228534617 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9089656395048994 MCC: 0.5230436299665046\n",
      "Train Epoch: 266, Ave Loss: 0.19544830232679192 ACC: 0.9677855138730126 Top2: 0.9839080459770115 AUC: 0.9930012588694341 MCC: 0.8301358433054479\n",
      "Val Epoch: 266, Ave Loss: 1.6856415601124362 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9068608174316658 MCC: 0.5200105123722258\n",
      "Train Epoch: 267, Ave Loss: 0.19527404536594287 ACC: 0.9659149953237036 Top2: 0.9862068965517241 AUC: 0.9930961143580446 MCC: 0.8212831462558303\n",
      "Val Epoch: 267, Ave Loss: 1.649774141862258 ACC: 0.9133889551052841 Top2: 0.8256880733944955 AUC: 0.9093435727178958 MCC: 0.5258074743467643\n",
      "Train Epoch: 268, Ave Loss: 0.19422935697034274 ACC: 0.9664345838096228 Top2: 0.9862068965517241 AUC: 0.9931389720964356 MCC: 0.8239459673425392\n",
      "Val Epoch: 268, Ave Loss: 1.6678476929009107 ACC: 0.9141835518474374 Top2: 0.8348623853211009 AUC: 0.9090994069107786 MCC: 0.5343088404670622\n",
      "Train Epoch: 269, Ave Loss: 0.19390546676326964 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9931648316420968 MCC: 0.8288038894980814\n",
      "Val Epoch: 269, Ave Loss: 1.696224636504041 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9082677926766375 MCC: 0.5170994374610192\n",
      "Train Epoch: 270, Ave Loss: 0.19534119104560918 ACC: 0.9662267484152551 Top2: 0.9839080459770115 AUC: 0.9929358854921484 MCC: 0.8224646591203656\n",
      "Val Epoch: 270, Ave Loss: 1.6617999717276555 ACC: 0.9145808502185141 Top2: 0.8256880733944955 AUC: 0.9090236591026303 MCC: 0.53444115666355\n",
      "Train Epoch: 271, Ave Loss: 0.19507358690504661 ACC: 0.9666424192039904 Top2: 0.9885057471264368 AUC: 0.9931510658925746 MCC: 0.8258266007665909\n",
      "Val Epoch: 271, Ave Loss: 1.7051418361833293 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9074925863847343 MCC: 0.5285760448963506\n",
      "Train Epoch: 272, Ave Loss: 0.1971325284969973 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.992936219882825 MCC: 0.8264844767595898\n",
      "Val Epoch: 272, Ave Loss: 1.604231058362521 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9098270693656524 MCC: 0.5396891011591498\n",
      "Train Epoch: 273, Ave Loss: 0.19412057979357705 ACC: 0.9677855138730126 Top2: 0.9885057471264368 AUC: 0.993122642685059 MCC: 0.8312285190258231\n",
      "Val Epoch: 273, Ave Loss: 1.631736903583335 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9068205260443527 MCC: 0.5297929745508286\n",
      "Train Epoch: 274, Ave Loss: 0.19395481361191685 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.993213374021991 MCC: 0.8320449924855882\n",
      "Val Epoch: 274, Ave Loss: 1.7349846988339728 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9060719120680764 MCC: 0.5244249315576534\n",
      "Train Epoch: 275, Ave Loss: 0.1954221410433554 ACC: 0.9669541722955419 Top2: 0.9885057471264368 AUC: 0.9929679312653276 MCC: 0.8263583715828452\n",
      "Val Epoch: 275, Ave Loss: 1.6393517754406792 ACC: 0.9114024632499007 Top2: 0.8073394495412844 AUC: 0.9082041322846829 MCC: 0.5098277776023405\n",
      "Train Epoch: 276, Ave Loss: 0.1960747425993889 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9929719996852269 MCC: 0.8292794037892849\n",
      "Val Epoch: 276, Ave Loss: 1.6633087906710722 ACC: 0.9145808502185141 Top2: 0.8440366972477065 AUC: 0.9080961513666839 MCC: 0.53444115666355\n",
      "Train Epoch: 277, Ave Loss: 0.1941441432634206 ACC: 0.9669541722955419 Top2: 0.9885057471264368 AUC: 0.9929096358240311 MCC: 0.825862064096138\n",
      "Val Epoch: 277, Ave Loss: 1.6849997504196093 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9072363331614235 MCC: 0.5354521592624486\n",
      "Train Epoch: 278, Ave Loss: 0.19424350960663028 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.993117348166012 MCC: 0.8298878295504356\n",
      "Val Epoch: 278, Ave Loss: 1.7017096724584 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9073016052088705 MCC: 0.5369656619330004\n",
      "Train Epoch: 279, Ave Loss: 0.1947032873154984 ACC: 0.966850254598358 Top2: 0.9862068965517241 AUC: 0.9929884405601623 MCC: 0.8270458258092366\n",
      "Val Epoch: 279, Ave Loss: 1.6912288299932414 ACC: 0.9157727453317441 Top2: 0.8440366972477065 AUC: 0.9089366297060341 MCC: 0.5403317789878349\n",
      "Train Epoch: 280, Ave Loss: 0.1948481781914393 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9930320228116861 MCC: 0.8295182094252326\n",
      "Val Epoch: 280, Ave Loss: 1.6445658982872515 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9069510701392469 MCC: 0.5342039553947123\n",
      "Train Epoch: 281, Ave Loss: 0.19415263828099663 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9931108275478173 MCC: 0.8306487501033906\n",
      "Val Epoch: 281, Ave Loss: 1.6276573731044972 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.9076102372356885 MCC: 0.526949589880808\n",
      "Train Epoch: 282, Ave Loss: 0.1932012754702143 ACC: 0.9666424192039904 Top2: 0.9862068965517241 AUC: 0.9931532394319729 MCC: 0.8260957635071744\n",
      "Val Epoch: 282, Ave Loss: 1.738982684548431 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9071468862815885 MCC: 0.5273189391707551\n",
      "Train Epoch: 283, Ave Loss: 0.19576339011400254 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9929361084192663 MCC: 0.8301704603937831\n",
      "Val Epoch: 283, Ave Loss: 1.6854078228539413 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9080437725631769 MCC: 0.5230436299665046\n",
      "Train Epoch: 284, Ave Loss: 0.1932054382388401 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9931656676187883 MCC: 0.8293329694807813\n",
      "Val Epoch: 284, Ave Loss: 1.6553200926503768 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9081686758638472 MCC: 0.5298814834246461\n",
      "Train Epoch: 285, Ave Loss: 0.19299699978608936 ACC: 0.9665385015068066 Top2: 0.9862068965517241 AUC: 0.9931657790823473 MCC: 0.8256217192665157\n",
      "Val Epoch: 285, Ave Loss: 1.634459770835555 ACC: 0.9133889551052841 Top2: 0.8165137614678899 AUC: 0.9089696686436306 MCC: 0.5285760448963506\n",
      "Train Epoch: 286, Ave Loss: 0.19339002082494036 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9931757550708674 MCC: 0.8270941452060375\n",
      "Val Epoch: 286, Ave Loss: 1.694596005067282 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9083217831356369 MCC: 0.5299619597251506\n",
      "Train Epoch: 287, Ave Loss: 0.19328138056781838 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9930538696692274 MCC: 0.829032022861146\n",
      "Val Epoch: 287, Ave Loss: 1.6945314263757443 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.907889859463641 MCC: 0.5228110043757691\n",
      "Train Epoch: 288, Ave Loss: 0.19248811134770283 ACC: 0.9684090200561155 Top2: 0.9885057471264368 AUC: 0.993209361333871 MCC: 0.8346218303786328\n",
      "Val Epoch: 288, Ave Loss: 1.7325343767116055 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9089608045384219 MCC: 0.5286935237121229\n",
      "Train Epoch: 289, Ave Loss: 0.1936346724665092 ACC: 0.9658110776265197 Top2: 0.9862068965517241 AUC: 0.9931393622188918 MCC: 0.8200158749272457\n",
      "Val Epoch: 289, Ave Loss: 1.7089257297821971 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9073298091799896 MCC: 0.5259456831643774\n",
      "Train Epoch: 290, Ave Loss: 0.1921412221013036 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9932546712705574 MCC: 0.8307346781436022\n",
      "Val Epoch: 290, Ave Loss: 1.6701718575424211 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9080397434244456 MCC: 0.5298814834246461\n",
      "Train Epoch: 291, Ave Loss: 0.19310374788157686 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.993250825777776 MCC: 0.8306968459536298\n",
      "Val Epoch: 291, Ave Loss: 1.70845011815154 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9081154912325942 MCC: 0.5083148392167022\n",
      "Train Epoch: 292, Ave Loss: 0.19361583317469383 ACC: 0.9664345838096228 Top2: 0.9862068965517241 AUC: 0.9931322842829026 MCC: 0.8230547966634392\n",
      "Val Epoch: 292, Ave Loss: 1.6682621637009682 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9083717444559051 MCC: 0.5283710878130083\n",
      "Train Epoch: 293, Ave Loss: 0.1924380637475035 ACC: 0.9680972669645641 Top2: 0.9839080459770115 AUC: 0.9932100301152244 MCC: 0.831726272681181\n",
      "Val Epoch: 293, Ave Loss: 1.705220014331252 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9079293450232078 MCC: 0.5284859557203005\n",
      "Train Epoch: 294, Ave Loss: 0.1920571980343216 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9932642014048421 MCC: 0.8301568931959195\n",
      "Val Epoch: 294, Ave Loss: 1.716371175091316 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9087303378029913 MCC: 0.5242126193427202\n",
      "Train Epoch: 295, Ave Loss: 0.1922864038691538 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9931532394319729 MCC: 0.825862064096138\n",
      "Val Epoch: 295, Ave Loss: 1.6985852276687459 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9076400528623002 MCC: 0.5326262888691773\n",
      "Train Epoch: 296, Ave Loss: 0.1937479016756748 ACC: 0.9686168554504833 Top2: 0.9862068965517241 AUC: 0.9931631039569341 MCC: 0.8367131709231085\n",
      "Val Epoch: 296, Ave Loss: 1.6605678839793045 ACC: 0.9149781485895908 Top2: 0.8440366972477065 AUC: 0.9081171028880867 MCC: 0.5346018540883455\n",
      "Train Epoch: 297, Ave Loss: 0.19312927760573229 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9931888520390364 MCC: 0.8268675899569397\n",
      "Val Epoch: 297, Ave Loss: 1.7085067304250299 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9077528687467767 MCC: 0.5215225603126983\n",
      "Train Epoch: 298, Ave Loss: 0.19298639890092215 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9932074107215906 MCC: 0.8278140244661035\n",
      "Val Epoch: 298, Ave Loss: 1.6992334942417258 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9084700554409489 MCC: 0.5155960931867983\n",
      "Train Epoch: 299, Ave Loss: 0.1919870142763242 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9932951882742119 MCC: 0.8252904774913775\n",
      "Val Epoch: 299, Ave Loss: 1.7431228243659804 ACC: 0.9117997616209773 Top2: 0.8165137614678899 AUC: 0.9063620100567303 MCC: 0.5170130946051832\n",
      "Train Epoch: 300, Ave Loss: 0.19479301615039063 ACC: 0.9656032422321521 Top2: 0.9862068965517241 AUC: 0.9929297549964097 MCC: 0.8193129663219009\n",
      "Val Epoch: 300, Ave Loss: 1.6256974484235631 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9033756124290871 MCC: 0.5326749961115761\n",
      "Train Epoch: 301, Ave Loss: 0.1948762896527329 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9929992525253741 MCC: 0.8283451900004913\n",
      "Val Epoch: 301, Ave Loss: 1.689156098994697 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9069083612686951 MCC: 0.5212681982413272\n",
      "Train Epoch: 302, Ave Loss: 0.19272403916370462 ACC: 0.9670580899927258 Top2: 0.9885057471264368 AUC: 0.9932273069668514 MCC: 0.8299194422640493\n",
      "Val Epoch: 302, Ave Loss: 1.7017619240625435 ACC: 0.9145808502185141 Top2: 0.8348623853211009 AUC: 0.909248485043837 MCC: 0.5222703143370552\n",
      "Train Epoch: 303, Ave Loss: 0.19336122633412853 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9931388049010972 MCC: 0.8250254578649249\n",
      "Val Epoch: 303, Ave Loss: 1.7374514994138353 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9073354499742134 MCC: 0.5271911980287856\n",
      "Train Epoch: 304, Ave Loss: 0.19141557834210163 ACC: 0.9691364439364024 Top2: 0.9862068965517241 AUC: 0.9933255063622285 MCC: 0.8387360628117257\n",
      "Val Epoch: 304, Ave Loss: 1.7309321715017485 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9073950812274367 MCC: 0.5128812773903609\n",
      "Train Epoch: 305, Ave Loss: 0.19030971330567062 ACC: 0.9682011846617479 Top2: 0.9862068965517241 AUC: 0.9934043110983597 MCC: 0.8330431426166979\n",
      "Val Epoch: 305, Ave Loss: 1.6938350716680262 ACC: 0.909813269765594 Top2: 0.8256880733944955 AUC: 0.9098641374419805 MCC: 0.4921605617746722\n",
      "Train Epoch: 306, Ave Loss: 0.19466986700824548 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9931396966095684 MCC: 0.8270819072139891\n",
      "Val Epoch: 306, Ave Loss: 1.7268806371994538 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.9024513280041258 MCC: 0.5140506170697363\n",
      "Train Epoch: 307, Ave Loss: 0.1925141162731951 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9932409612528148 MCC: 0.8263978977515214\n",
      "Val Epoch: 307, Ave Loss: 1.7121872051321263 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.905447395564724 MCC: 0.5326262888691773\n",
      "Train Epoch: 308, Ave Loss: 0.19425161130454835 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9931443780790414 MCC: 0.830243931545036\n",
      "Val Epoch: 308, Ave Loss: 1.7356942109413571 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.9071904009798865 MCC: 0.516954031088185\n",
      "Train Epoch: 309, Ave Loss: 0.1928663579808538 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9931502299158829 MCC: 0.8282270607156652\n",
      "Val Epoch: 309, Ave Loss: 1.6933520657172942 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9072983818978855 MCC: 0.5240936644687275\n",
      "Train Epoch: 310, Ave Loss: 0.1942622824703364 ACC: 0.9659149953237036 Top2: 0.9862068965517241 AUC: 0.9930897052034088 MCC: 0.8225102223061234\n",
      "Val Epoch: 310, Ave Loss: 1.6982632684758847 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9080953455389376 MCC: 0.5170994374610192\n",
      "Train Epoch: 311, Ave Loss: 0.19348694933245308 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.993183891910666 MCC: 0.8272091984825584\n",
      "Val Epoch: 311, Ave Loss: 1.7035363357715367 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9088133380608561 MCC: 0.5298814834246461\n",
      "Train Epoch: 312, Ave Loss: 0.19173055107632644 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9932994796212291 MCC: 0.8304321136219129\n",
      "Val Epoch: 312, Ave Loss: 1.7346050582300365 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9077399755028364 MCC: 0.5187448702919161\n",
      "Train Epoch: 313, Ave Loss: 0.19216898494773346 ACC: 0.9669541722955419 Top2: 0.9839080459770115 AUC: 0.9932659290900048 MCC: 0.8271269537202486\n",
      "Val Epoch: 313, Ave Loss: 1.7277011875282051 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9063136603919546 MCC: 0.5186117432374951\n",
      "Train Epoch: 314, Ave Loss: 0.19136999019883827 ACC: 0.9667463369011743 Top2: 0.9862068965517241 AUC: 0.9932326014858984 MCC: 0.8247640939188501\n",
      "Val Epoch: 314, Ave Loss: 1.6778083860217021 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9077141890149562 MCC: 0.5297978229959907\n",
      "Train Epoch: 315, Ave Loss: 0.19278454183821225 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9932162720745219 MCC: 0.8283322770022329\n",
      "Val Epoch: 315, Ave Loss: 1.6819841522882046 ACC: 0.9145808502185141 Top2: 0.8348623853211009 AUC: 0.907352372356885 MCC: 0.5317247142446719\n",
      "Train Epoch: 316, Ave Loss: 0.1917486057251916 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9930923245970424 MCC: 0.8287879091409306\n",
      "Val Epoch: 316, Ave Loss: 1.7093226872035394 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9089527462609592 MCC: 0.5229131640968384\n",
      "Train Epoch: 317, Ave Loss: 0.19179957165014466 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9932341619757228 MCC: 0.8292158753134324\n",
      "Val Epoch: 317, Ave Loss: 1.7458018758458567 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.9078076650335224 MCC: 0.5228110043757691\n",
      "Train Epoch: 318, Ave Loss: 0.19115355643656984 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9933439535812238 MCC: 0.8302489005725934\n",
      "Val Epoch: 318, Ave Loss: 1.6581928113999878 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.908881027591542 MCC: 0.5297929745508286\n",
      "Train Epoch: 319, Ave Loss: 0.19165924023329034 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9932706662912575 MCC: 0.829692836004533\n",
      "Val Epoch: 319, Ave Loss: 1.7452966354650847 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9070244004641569 MCC: 0.5170994374610192\n",
      "Train Epoch: 320, Ave Loss: 0.19256644068714635 ACC: 0.9671620076899096 Top2: 0.9839080459770115 AUC: 0.9932430790604336 MCC: 0.8270819072139891\n",
      "Val Epoch: 320, Ave Loss: 1.6808267611647076 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9082258896338318 MCC: 0.5283843302901519\n",
      "Train Epoch: 321, Ave Loss: 0.19054502446398755 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.9933243359948601 MCC: 0.8336499209557796\n",
      "Val Epoch: 321, Ave Loss: 1.7011675131742519 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9076779267663744 MCC: 0.5270183351357622\n",
      "Train Epoch: 322, Ave Loss: 0.19156085503692605 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9932779114225849 MCC: 0.8318638249288067\n",
      "Val Epoch: 322, Ave Loss: 1.7470228986551382 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9090373581743167 MCC: 0.5187448702919161\n",
      "Train Epoch: 323, Ave Loss: 0.19070841915477327 ACC: 0.968720773147667 Top2: 0.9862068965517241 AUC: 0.9932599657896045 MCC: 0.8374468307980993\n",
      "Val Epoch: 323, Ave Loss: 1.8293978969973026 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9083854435275915 MCC: 0.513205391062051\n",
      "Train Epoch: 324, Ave Loss: 0.1908568414973363 ACC: 0.9677855138730126 Top2: 0.9839080459770115 AUC: 0.9933063346301004 MCC: 0.8302539448052902\n",
      "Val Epoch: 324, Ave Loss: 1.7395498070280013 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9072323040226921 MCC: 0.5232034166367038\n",
      "Train Epoch: 325, Ave Loss: 0.19106089560496964 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9932561760286024 MCC: 0.828215281933202\n",
      "Val Epoch: 325, Ave Loss: 1.7840048468713685 ACC: 0.9141835518474374 Top2: 0.8348623853211009 AUC: 0.9078487622485817 MCC: 0.523393589467457\n",
      "Train Epoch: 326, Ave Loss: 0.19181873152524567 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.9932647587226365 MCC: 0.8329409400445809\n",
      "Val Epoch: 326, Ave Loss: 1.7221007269199702 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9080421609076844 MCC: 0.5283710878130083\n",
      "Train Epoch: 327, Ave Loss: 0.1912911056049725 ACC: 0.9673698430842772 Top2: 0.9839080459770115 AUC: 0.9932604116438402 MCC: 0.8303569811934002\n",
      "Val Epoch: 327, Ave Loss: 1.7653178078740088 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9070679151624549 MCC: 0.5216636325967957\n",
      "Train Epoch: 328, Ave Loss: 0.1903366458059382 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.99339728889415 MCC: 0.8327113437148739\n",
      "Val Epoch: 328, Ave Loss: 1.7328103587433277 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9076730917998969 MCC: 0.5184308107227527\n",
      "Train Epoch: 329, Ave Loss: 0.190504958085511 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9933347578376159 MCC: 0.8293036101461554\n",
      "Val Epoch: 329, Ave Loss: 1.7868807406058804 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.9082484528107272 MCC: 0.5214102844759737\n",
      "Train Epoch: 330, Ave Loss: 0.1897772831055564 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9935098670886231 MCC: 0.8309646248230859\n",
      "Val Epoch: 330, Ave Loss: 1.8160391699384406 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9073958870551829 MCC: 0.49385868401457733\n",
      "Train Epoch: 331, Ave Loss: 0.19030995309625717 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9933306336859371 MCC: 0.8251424386883568\n",
      "Val Epoch: 331, Ave Loss: 1.7778577205795358 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.9065118940175347 MCC: 0.5284223417726511\n",
      "Train Epoch: 332, Ave Loss: 0.1909835046894875 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9932792489852915 MCC: 0.8289214678898698\n",
      "Val Epoch: 332, Ave Loss: 1.7731171288922523 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9076787325941207 MCC: 0.5243048607726423\n",
      "Train Epoch: 333, Ave Loss: 0.1907583266841976 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9933325285664382 MCC: 0.8281308505613345\n",
      "Val Epoch: 333, Ave Loss: 1.74210372912469 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9067471957194428 MCC: 0.5245738199473498\n",
      "Train Epoch: 334, Ave Loss: 0.19118080091024167 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9932916214403277 MCC: 0.8268429016142191\n",
      "Val Epoch: 334, Ave Loss: 1.7343704016825239 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.906533651366684 MCC: 0.5213258124820263\n",
      "Train Epoch: 335, Ave Loss: 0.1907984420040766 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9933692000773111 MCC: 0.8303286512035393\n",
      "Val Epoch: 335, Ave Loss: 1.7486981863782536 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9063176895306859 MCC: 0.5298270549069729\n",
      "Train Epoch: 336, Ave Loss: 0.19113593733013629 ACC: 0.9666424192039904 Top2: 0.9862068965517241 AUC: 0.9932200061037445 MCC: 0.8259608010571386\n",
      "Val Epoch: 336, Ave Loss: 1.7473223444548565 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9082855208870552 MCC: 0.5342039553947123\n",
      "Train Epoch: 337, Ave Loss: 0.18995891442234888 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9933654103163091 MCC: 0.8314646623725322\n",
      "Val Epoch: 337, Ave Loss: 1.7745391721057895 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9084539388860238 MCC: 0.5298814834246461\n",
      "Train Epoch: 338, Ave Loss: 0.19068785917250955 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9932644800637394 MCC: 0.8306487501033906\n",
      "Val Epoch: 338, Ave Loss: 1.770656209320421 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9084112300154719 MCC: 0.5198085745236014\n",
      "Train Epoch: 339, Ave Loss: 0.1903655501082452 ACC: 0.9676815961758287 Top2: 0.9885057471264368 AUC: 0.9933016531606271 MCC: 0.8311277067611339\n",
      "Val Epoch: 339, Ave Loss: 1.7584842343482932 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9072460030943785 MCC: 0.5141798565084859\n",
      "Train Epoch: 340, Ave Loss: 0.18989712573776163 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9933445108990182 MCC: 0.8295623140155006\n",
      "Val Epoch: 340, Ave Loss: 1.7648520031423107 ACC: 0.912197059992054 Top2: 0.8256880733944955 AUC: 0.9086328326456936 MCC: 0.5227361838734625\n",
      "Train Epoch: 341, Ave Loss: 0.19100564379938187 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9933064460936594 MCC: 0.8289186480273867\n",
      "Val Epoch: 341, Ave Loss: 1.8091930033330563 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.907553023465704 MCC: 0.52555901145458\n",
      "Train Epoch: 342, Ave Loss: 0.19201060209072784 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9931960414385843 MCC: 0.8317083001788667\n",
      "Val Epoch: 342, Ave Loss: 1.7329457027969153 ACC: 0.912197059992054 Top2: 0.8532110091743119 AUC: 0.9093258445074781 MCC: 0.5227361838734625\n",
      "Train Epoch: 343, Ave Loss: 0.19100192043809802 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9932236286694083 MCC: 0.8273485962365623\n",
      "Val Epoch: 343, Ave Loss: 1.7072297739787996 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9083830260443527 MCC: 0.526949589880808\n",
      "Train Epoch: 344, Ave Loss: 0.1911194996750588 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9933087868283959 MCC: 0.8292761510689758\n",
      "Val Epoch: 344, Ave Loss: 1.7323081322265352 ACC: 0.9141835518474374 Top2: 0.8440366972477065 AUC: 0.9081775399690563 MCC: 0.5302046642175549\n",
      "Train Epoch: 345, Ave Loss: 0.1911882310277062 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.99338229704548 MCC: 0.8278140244661035\n",
      "Val Epoch: 345, Ave Loss: 1.6890011015430992 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9077560920577616 MCC: 0.5198029986473033\n",
      "Train Epoch: 346, Ave Loss: 0.19352679893709357 ACC: 0.9659149953237036 Top2: 0.9862068965517241 AUC: 0.9931357396532279 MCC: 0.82155048548002\n",
      "Val Epoch: 346, Ave Loss: 1.7202574515206692 ACC: 0.9137862534763608 Top2: 0.8256880733944955 AUC: 0.9095361655492521 MCC: 0.5300693752697087\n",
      "Train Epoch: 347, Ave Loss: 0.19080527735371186 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9933093998779697 MCC: 0.8295300165537822\n",
      "Val Epoch: 347, Ave Loss: 1.7314440690708763 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9082347537390407 MCC: 0.5215225603126983\n",
      "Train Epoch: 348, Ave Loss: 0.18954846884883422 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.9933926631564562 MCC: 0.8351555298005319\n",
      "Val Epoch: 348, Ave Loss: 1.7573091932964424 ACC: 0.909813269765594 Top2: 0.8440366972477065 AUC: 0.9085393566271274 MCC: 0.4979923878107779\n",
      "Train Epoch: 349, Ave Loss: 0.19058217724307477 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.9932615820112084 MCC: 0.8309021034965085\n",
      "Val Epoch: 349, Ave Loss: 1.7474432016227124 ACC: 0.9114024632499007 Top2: 0.8256880733944955 AUC: 0.9090518630737494 MCC: 0.5240936644687275\n",
      "Train Epoch: 350, Ave Loss: 0.19075653128232356 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9933618992142041 MCC: 0.830243931545036\n",
      "Val Epoch: 350, Ave Loss: 1.734909697417598 ACC: 0.9117997616209773 Top2: 0.8256880733944955 AUC: 0.9088085030943784 MCC: 0.5127644370100667\n",
      "Train Epoch: 351, Ave Loss: 0.1900380642212726 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9932235729376288 MCC: 0.8291559746280023\n",
      "Val Epoch: 351, Ave Loss: 1.7737640868363338 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9069268953068592 MCC: 0.5256150789832389\n",
      "Train Epoch: 352, Ave Loss: 0.19128632616005556 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9931931991178328 MCC: 0.8307462790955097\n",
      "Val Epoch: 352, Ave Loss: 1.7556543889997176 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.9072508380608562 MCC: 0.5270183351357622\n",
      "Train Epoch: 353, Ave Loss: 0.19077437021001253 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9933168122046355 MCC: 0.8288362116617118\n",
      "Val Epoch: 353, Ave Loss: 1.8517260305382295 ACC: 0.9133889551052841 Top2: 0.8348623853211009 AUC: 0.9059462029396597 MCC: 0.5299619597251506\n",
      "Train Epoch: 354, Ave Loss: 0.19073481681498294 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9933867555878355 MCC: 0.831607332000385\n",
      "Val Epoch: 354, Ave Loss: 1.751130802762168 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9077883251676122 MCC: 0.5243048607726423\n",
      "Train Epoch: 355, Ave Loss: 0.19056116548807148 ACC: 0.9663306661124389 Top2: 0.9839080459770115 AUC: 0.9932347192935171 MCC: 0.8232051438310357\n",
      "Val Epoch: 355, Ave Loss: 1.7371336850287495 ACC: 0.9129916567342073 Top2: 0.8532110091743119 AUC: 0.9071541387313048 MCC: 0.5270913094065779\n",
      "Train Epoch: 356, Ave Loss: 0.19011329245299619 ACC: 0.9694481970279538 Top2: 0.9862068965517241 AUC: 0.9933320269804232 MCC: 0.8405477806259013\n",
      "Val Epoch: 356, Ave Loss: 1.8053690959149784 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9080260443527592 MCC: 0.5215225603126983\n",
      "Train Epoch: 357, Ave Loss: 0.18960954961044643 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9932967487640364 MCC: 0.8272046035133426\n",
      "Val Epoch: 357, Ave Loss: 1.7808686530484887 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9071339930376483 MCC: 0.5355843104441431\n",
      "Train Epoch: 358, Ave Loss: 0.19305298478715843 ACC: 0.9684090200561155 Top2: 0.9862068965517241 AUC: 0.9932896708280472 MCC: 0.8341362535928826\n",
      "Val Epoch: 358, Ave Loss: 1.721748911580007 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9049203842186695 MCC: 0.5255285150805782\n",
      "Train Epoch: 359, Ave Loss: 0.19011999394165044 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9934138969644237 MCC: 0.8321961480748327\n",
      "Val Epoch: 359, Ave Loss: 1.7464224643295692 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9073854112944817 MCC: 0.5297929745508286\n",
      "Train Epoch: 360, Ave Loss: 0.19156714011294565 ACC: 0.9680972669645641 Top2: 0.9839080459770115 AUC: 0.9932067419402373 MCC: 0.8331754815280629\n",
      "Val Epoch: 360, Ave Loss: 1.7867690622823613 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9068640407426509 MCC: 0.5240936644687275\n",
      "Train Epoch: 361, Ave Loss: 0.1900396163583615 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9933363740592198 MCC: 0.8316471911966691\n",
      "Val Epoch: 361, Ave Loss: 1.794760624790993 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9074950038679732 MCC: 0.5256975938758675\n",
      "Train Epoch: 362, Ave Loss: 0.19007791648042913 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9933717080073861 MCC: 0.8324360874227319\n",
      "Val Epoch: 362, Ave Loss: 1.779413627029949 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9071210997937081 MCC: 0.5053692312575527\n",
      "Train Epoch: 363, Ave Loss: 0.190737284551852 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9933597814065854 MCC: 0.8280755331299433\n",
      "Val Epoch: 363, Ave Loss: 1.7620621218134356 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9069027204744713 MCC: 0.5140259328072316\n",
      "Train Epoch: 364, Ave Loss: 0.19095764352352565 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9932271397715131 MCC: 0.8315886723193354\n",
      "Val Epoch: 364, Ave Loss: 1.73192156482182 ACC: 0.9117997616209773 Top2: 0.8532110091743119 AUC: 0.9077149948427025 MCC: 0.5241078994085301\n",
      "Train Epoch: 365, Ave Loss: 0.18969840467566074 ACC: 0.9671620076899096 Top2: 0.9839080459770115 AUC: 0.9934086581771562 MCC: 0.8274524482230344\n",
      "Val Epoch: 365, Ave Loss: 1.7544829633336638 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.9065570203713254 MCC: 0.5255227532821783\n",
      "Train Epoch: 366, Ave Loss: 0.18906179600512707 ACC: 0.9683051023589317 Top2: 0.9862068965517241 AUC: 0.993549548115586 MCC: 0.8356823667206911\n",
      "Val Epoch: 366, Ave Loss: 1.7617982334474176 ACC: 0.9114024632499007 Top2: 0.8256880733944955 AUC: 0.9077399755028366 MCC: 0.5155275348936812\n",
      "Train Epoch: 367, Ave Loss: 0.1905855378972429 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9932410727163736 MCC: 0.8295300165537822\n",
      "Val Epoch: 367, Ave Loss: 1.7638112638252856 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9066110108303249 MCC: 0.5256975938758675\n",
      "Train Epoch: 368, Ave Loss: 0.19075872908219194 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9932364469786799 MCC: 0.8269681202973856\n",
      "Val Epoch: 368, Ave Loss: 1.8247080177621795 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9063877965446105 MCC: 0.511251685495539\n",
      "Train Epoch: 369, Ave Loss: 0.18995453040596863 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.993271112145493 MCC: 0.8270941452060375\n",
      "Val Epoch: 369, Ave Loss: 1.8285571028249785 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9067085159876225 MCC: 0.5144220444886393\n",
      "Train Epoch: 370, Ave Loss: 0.1898691864522632 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9933908240077347 MCC: 0.8287879091409306\n",
      "Val Epoch: 370, Ave Loss: 1.8316279368194321 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9066641954615781 MCC: 0.5213258124820263\n",
      "Train Epoch: 371, Ave Loss: 0.1893435335835249 ACC: 0.9680972669645641 Top2: 0.9885057471264368 AUC: 0.9934404252914382 MCC: 0.8331754815280629\n",
      "Val Epoch: 371, Ave Loss: 1.7815350698499808 ACC: 0.9125943583631307 Top2: 0.8256880733944955 AUC: 0.9073443140794224 MCC: 0.5228110043757691\n",
      "Train Epoch: 372, Ave Loss: 0.18962804216720433 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9934196930694857 MCC: 0.8319500316405132\n",
      "Val Epoch: 372, Ave Loss: 1.8505014157953876 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9055868037648273 MCC: 0.5328246463186501\n",
      "Train Epoch: 373, Ave Loss: 0.19002131206193487 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9933277913651856 MCC: 0.8285667688378635\n",
      "Val Epoch: 373, Ave Loss: 1.7787327384821432 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9082154138731304 MCC: 0.5327367841367721\n",
      "Train Epoch: 374, Ave Loss: 0.18887317702092632 ACC: 0.968720773147667 Top2: 0.9862068965517241 AUC: 0.9933855294886877 MCC: 0.8352519732610909\n",
      "Val Epoch: 374, Ave Loss: 1.8103580099049013 ACC: 0.9141835518474374 Top2: 0.8256880733944955 AUC: 0.9065787777204743 MCC: 0.5329394704499809\n",
      "Train Epoch: 375, Ave Loss: 0.19006320637313587 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9934223124631195 MCC: 0.8314698757330777\n",
      "Val Epoch: 375, Ave Loss: 1.7977132367738617 ACC: 0.912197059992054 Top2: 0.8532110091743119 AUC: 0.9061025335224343 MCC: 0.5227361838734625\n",
      "Train Epoch: 376, Ave Loss: 0.18951550931362343 ACC: 0.9667463369011743 Top2: 0.9862068965517241 AUC: 0.9933355380825281 MCC: 0.8264363526181225\n",
      "Val Epoch: 376, Ave Loss: 1.7998508325584166 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9060920577617327 MCC: 0.5227361838734625\n",
      "Train Epoch: 377, Ave Loss: 0.18884557154764592 ACC: 0.9688246908448509 Top2: 0.9862068965517241 AUC: 0.9934704647205574 MCC: 0.8357437956917305\n",
      "Val Epoch: 377, Ave Loss: 1.8047294756745382 ACC: 0.9117997616209773 Top2: 0.8532110091743119 AUC: 0.9053241039195461 MCC: 0.5283710878130083\n",
      "Train Epoch: 378, Ave Loss: 0.1892676663796142 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9934485621312368 MCC: 0.8327859157872184\n",
      "Val Epoch: 378, Ave Loss: 1.8070503236739226 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9080993746776689 MCC: 0.5098277776023405\n",
      "Train Epoch: 379, Ave Loss: 0.1890553993808923 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9933742716692404 MCC: 0.8301382867586021\n",
      "Val Epoch: 379, Ave Loss: 1.8234694204326567 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9049324716348633 MCC: 0.5175321395481265\n",
      "Train Epoch: 380, Ave Loss: 0.1895669815383147 ACC: 0.9682011846617479 Top2: 0.9862068965517241 AUC: 0.993470408988778 MCC: 0.8332862231093209\n",
      "Val Epoch: 380, Ave Loss: 1.818090597277685 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9074788873130479 MCC: 0.511349889644648\n",
      "Train Epoch: 381, Ave Loss: 0.18918284476192246 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9933800677743023 MCC: 0.8286670886852362\n",
      "Val Epoch: 381, Ave Loss: 1.7793531472729596 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9087440368746778 MCC: 0.5183810583403471\n",
      "Train Epoch: 382, Ave Loss: 0.18947538786838167 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9933869785149532 MCC: 0.8290368919456419\n",
      "Val Epoch: 382, Ave Loss: 1.8228661393933356 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9072919352759155 MCC: 0.5241078994085301\n",
      "Train Epoch: 383, Ave Loss: 0.18945187973614325 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.993435743821965 MCC: 0.8279236580980146\n",
      "Val Epoch: 383, Ave Loss: 1.8159302833053332 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9073596248066014 MCC: 0.5199161978331617\n",
      "Train Epoch: 384, Ave Loss: 0.1899488680547041 ACC: 0.9676815961758287 Top2: 0.9839080459770115 AUC: 0.9934077664686851 MCC: 0.8297645694478786\n",
      "Val Epoch: 384, Ave Loss: 1.7471808962425694 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9064458161423414 MCC: 0.5184308107227527\n",
      "Train Epoch: 385, Ave Loss: 0.1901675108530245 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9933797333836256 MCC: 0.8273894418397205\n",
      "Val Epoch: 385, Ave Loss: 1.825673531957397 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9066601663228468 MCC: 0.5125822496943438\n",
      "Train Epoch: 386, Ave Loss: 0.18910891160408835 ACC: 0.9673698430842772 Top2: 0.9839080459770115 AUC: 0.9933028792597749 MCC: 0.8283006207148408\n",
      "Val Epoch: 386, Ave Loss: 1.8061158611822525 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9065231756059825 MCC: 0.5300693752697087\n",
      "Train Epoch: 387, Ave Loss: 0.18836629599089408 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9934291117402114 MCC: 0.8300890324147712\n",
      "Val Epoch: 387, Ave Loss: 1.850777147435302 ACC: 0.9102105681366707 Top2: 0.8348623853211009 AUC: 0.9073555956678699 MCC: 0.5038652414233378\n",
      "Train Epoch: 388, Ave Loss: 0.18871248452331596 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9934326785740957 MCC: 0.8293329694807813\n",
      "Val Epoch: 388, Ave Loss: 1.839629800830374 ACC: 0.9090186730234406 Top2: 0.8348623853211009 AUC: 0.906262087416194 MCC: 0.5023615460752395\n",
      "Train Epoch: 389, Ave Loss: 0.1881909755048986 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9933709277624738 MCC: 0.8325572916641559\n",
      "Val Epoch: 389, Ave Loss: 1.8287620496044137 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9071243231046933 MCC: 0.5283843302901519\n",
      "Train Epoch: 390, Ave Loss: 0.19034617261167788 ACC: 0.9679933492673802 Top2: 0.9885057471264368 AUC: 0.9933763894768591 MCC: 0.8333370938899058\n",
      "Val Epoch: 390, Ave Loss: 1.8300201974526595 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9053418321299639 MCC: 0.5170130946051832\n",
      "Train Epoch: 391, Ave Loss: 0.1896893106224492 ACC: 0.9667463369011743 Top2: 0.9862068965517241 AUC: 0.99338330021751 MCC: 0.8255192360725118\n",
      "Val Epoch: 391, Ave Loss: 1.8070197176701253 ACC: 0.9102105681366707 Top2: 0.8440366972477065 AUC: 0.9065537970603403 MCC: 0.508218714863602\n",
      "Train Epoch: 392, Ave Loss: 0.18799570978369617 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9934973274382486 MCC: 0.8319595309731636\n",
      "Val Epoch: 392, Ave Loss: 1.8012854163178296 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9067173800928314 MCC: 0.5212365394845138\n",
      "Train Epoch: 393, Ave Loss: 0.1888245221732732 ACC: 0.9685129377532994 Top2: 0.9839080459770115 AUC: 0.9935250261326313 MCC: 0.8342704305244836\n",
      "Val Epoch: 393, Ave Loss: 1.7887661478241939 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9060034167096441 MCC: 0.5256150789832389\n",
      "Train Epoch: 394, Ave Loss: 0.18831464949430235 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.993471913746823 MCC: 0.8299119193242769\n",
      "Val Epoch: 394, Ave Loss: 1.7958644636964014 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9067214092315625 MCC: 0.5197934570444092\n",
      "Train Epoch: 395, Ave Loss: 0.18766942226317845 ACC: 0.9691364439364024 Top2: 0.9862068965517241 AUC: 0.99350418244712 MCC: 0.8359568140518032\n",
      "Val Epoch: 395, Ave Loss: 1.8171844134220536 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9057681150077359 MCC: 0.5368721799592288\n",
      "Train Epoch: 396, Ave Loss: 0.18929316841623997 ACC: 0.9674737607814611 Top2: 0.9885057471264368 AUC: 0.9934983863420581 MCC: 0.8320655709917382\n",
      "Val Epoch: 396, Ave Loss: 1.8128978077007276 ACC: 0.9129916567342073 Top2: 0.8256880733944955 AUC: 0.9080913164002062 MCC: 0.5243048607726423\n",
      "Train Epoch: 397, Ave Loss: 0.18960377927696612 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9934022490225203 MCC: 0.8289538907466791\n",
      "Val Epoch: 397, Ave Loss: 1.8620274119071136 ACC: 0.9102105681366707 Top2: 0.8440366972477065 AUC: 0.905231433728726 MCC: 0.5024147739716298\n",
      "Train Epoch: 398, Ave Loss: 0.18881290890676458 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9933819626548034 MCC: 0.829396637916339\n",
      "Val Epoch: 398, Ave Loss: 1.8225459616607138 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.907025206291903 MCC: 0.516954031088185\n",
      "Train Epoch: 399, Ave Loss: 0.18847517629552388 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9934012458504904 MCC: 0.8304926551178372\n",
      "Val Epoch: 399, Ave Loss: 1.860599213038098 ACC: 0.9125943583631307 Top2: 0.8165137614678899 AUC: 0.9046738009283136 MCC: 0.5256150789832389\n",
      "Train Epoch: 400, Ave Loss: 0.19046862599534564 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9933658004387651 MCC: 0.8263583715828452\n",
      "Val Epoch: 400, Ave Loss: 1.6902275487181706 ACC: 0.9102105681366707 Top2: 0.8348623853211009 AUC: 0.9053531137184115 MCC: 0.5125744367575039\n",
      "Train Epoch: 401, Ave Loss: 0.18983034368935287 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9933192086711514 MCC: 0.8285465248632237\n",
      "Val Epoch: 401, Ave Loss: 1.769899998567413 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9060509605466736 MCC: 0.5255227532821783\n",
      "Train Epoch: 402, Ave Loss: 0.1889069158350717 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9934502340846201 MCC: 0.8336843762079669\n",
      "Val Epoch: 402, Ave Loss: 1.8678306601027292 ACC: 0.912197059992054 Top2: 0.8532110091743119 AUC: 0.906376514956163 MCC: 0.5213258124820263\n",
      "Train Epoch: 403, Ave Loss: 0.18944400526145794 ACC: 0.966850254598358 Top2: 0.9862068965517241 AUC: 0.9934584823879774 MCC: 0.8258745268723163\n",
      "Val Epoch: 403, Ave Loss: 1.8488855385150866 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9069937790097988 MCC: 0.5243048607726423\n",
      "Train Epoch: 404, Ave Loss: 0.1915689001513228 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9933022104784217 MCC: 0.8303720330809086\n",
      "Val Epoch: 404, Ave Loss: 1.8370396102880042 ACC: 0.9094159713945172 Top2: 0.8440366972477065 AUC: 0.9037068076328003 MCC: 0.5096961170185199\n",
      "Train Epoch: 405, Ave Loss: 0.18954695999993285 ACC: 0.9665385015068066 Top2: 0.9885057471264368 AUC: 0.9932780786179232 MCC: 0.8237909308246726\n",
      "Val Epoch: 405, Ave Loss: 1.7964971138176642 ACC: 0.9114024632499007 Top2: 0.8256880733944955 AUC: 0.9053249097472924 MCC: 0.526952148298931\n",
      "Train Epoch: 406, Ave Loss: 0.18847266884892186 ACC: 0.9685129377532994 Top2: 0.9862068965517241 AUC: 0.9934441035888814 MCC: 0.8351052996108023\n",
      "Val Epoch: 406, Ave Loss: 1.8695832119738627 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.9064272821041774 MCC: 0.5154857682354594\n",
      "Train Epoch: 407, Ave Loss: 0.18895832486162017 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9934023604860793 MCC: 0.828215281933202\n",
      "Val Epoch: 407, Ave Loss: 1.8685380574920372 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.905277365910263 MCC: 0.5214102844759737\n",
      "Train Epoch: 408, Ave Loss: 0.18920218187553925 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9934939835314821 MCC: 0.8322544067315276\n",
      "Val Epoch: 408, Ave Loss: 1.8450765556366118 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9061782813305828 MCC: 0.5186117432374951\n",
      "Train Epoch: 409, Ave Loss: 0.18806267446874605 ACC: 0.9682011846617479 Top2: 0.9839080459770115 AUC: 0.9934527420146949 MCC: 0.8321040170898908\n",
      "Val Epoch: 409, Ave Loss: 1.8920698270783494 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9051887248581743 MCC: 0.5212365394845138\n",
      "Train Epoch: 410, Ave Loss: 0.18847281193236384 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9933950038911928 MCC: 0.829640601503413\n",
      "Val Epoch: 410, Ave Loss: 1.8910470203862533 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9055150850954099 MCC: 0.5126159047343926\n",
      "Train Epoch: 411, Ave Loss: 0.18753998559421767 ACC: 0.9677855138730126 Top2: 0.9839080459770115 AUC: 0.9934801620501805 MCC: 0.8311038454995953\n",
      "Val Epoch: 411, Ave Loss: 1.903926106176157 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9054691529138732 MCC: 0.5154857682354594\n",
      "Train Epoch: 412, Ave Loss: 0.18807063401127602 ACC: 0.9686168554504833 Top2: 0.9862068965517241 AUC: 0.9934253777109887 MCC: 0.8359568546669098\n",
      "Val Epoch: 412, Ave Loss: 1.9078552691517003 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9056955905105725 MCC: 0.5154698839252518\n",
      "Train Epoch: 413, Ave Loss: 0.18875218018155787 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9935254719868669 MCC: 0.8309799779199166\n",
      "Val Epoch: 413, Ave Loss: 1.8138477925422862 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.906818108561114 MCC: 0.5213258124820263\n",
      "Train Epoch: 414, Ave Loss: 0.1870950681096498 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9935927402446536 MCC: 0.8331924569715763\n",
      "Val Epoch: 414, Ave Loss: 1.9018614138316137 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9057971248066014 MCC: 0.5039280114346477\n",
      "Train Epoch: 415, Ave Loss: 0.18909372298498156 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9934682911811592 MCC: 0.8295300165537822\n",
      "Val Epoch: 415, Ave Loss: 1.8828975310300622 ACC: 0.9078267779102106 Top2: 0.8532110091743119 AUC: 0.9051025012893245 MCC: 0.48750740955218624\n",
      "Train Epoch: 416, Ave Loss: 0.18962598795630697 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.99336931154087 MCC: 0.8321961480748327\n",
      "Val Epoch: 416, Ave Loss: 1.7443136639273162 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9063112429087158 MCC: 0.5111388719542006\n",
      "Train Epoch: 417, Ave Loss: 0.18812076943486994 ACC: 0.9686168554504833 Top2: 0.9862068965517241 AUC: 0.9935165549021562 MCC: 0.83487672392383\n",
      "Val Epoch: 417, Ave Loss: 1.8193173329389563 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9057358818978856 MCC: 0.5198085745236014\n",
      "Train Epoch: 418, Ave Loss: 0.18807319547510146 ACC: 0.9686168554504833 Top2: 0.9862068965517241 AUC: 0.9934983306102785 MCC: 0.8355894480947065\n",
      "Val Epoch: 418, Ave Loss: 1.80183718288383 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9071501095925735 MCC: 0.5242126193427202\n",
      "Train Epoch: 419, Ave Loss: 0.19392037104870652 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.993239846617226 MCC: 0.8277035328114737\n",
      "Val Epoch: 419, Ave Loss: 1.8547258405170404 ACC: 0.9137862534763608 Top2: 0.8348623853211009 AUC: 0.9010354886539453 MCC: 0.5177378526995531\n",
      "Train Epoch: 420, Ave Loss: 0.2047360905700729 ACC: 0.966850254598358 Top2: 0.9862068965517241 AUC: 0.992602553719305 MCC: 0.82441458538637\n",
      "Val Epoch: 420, Ave Loss: 1.7551433551471003 ACC: 0.9090186730234406 Top2: 0.8348623853211009 AUC: 0.9011869842702425 MCC: 0.5038386869066614\n",
      "Train Epoch: 421, Ave Loss: 0.2694818226067392 ACC: 0.9615504520419828 Top2: 0.9816091954022989 AUC: 0.9885375891903533 MCC: 0.7965846703869385\n",
      "Val Epoch: 421, Ave Loss: 1.4406087855361065 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9012474213512119 MCC: 0.5284859557203005\n",
      "Train Epoch: 422, Ave Loss: 0.3072602737019163 ACC: 0.958329003429284 Top2: 0.9770114942528736 AUC: 0.9847974294719904 MCC: 0.7797297579246152\n",
      "Val Epoch: 422, Ave Loss: 1.1894764011558103 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9067262441980402 MCC: 0.5312152173657474\n",
      "Train Epoch: 423, Ave Loss: 0.3077620858025548 ACC: 0.9590564273095709 Top2: 0.9747126436781609 AUC: 0.9849938282627446 MCC: 0.7816776552145739\n",
      "Val Epoch: 423, Ave Loss: 1.26791118494934 ACC: 0.9062375844259039 Top2: 0.8256880733944955 AUC: 0.8854668965961836 MCC: 0.5137442451958091\n",
      "Train Epoch: 424, Ave Loss: 0.31532058471976604 ACC: 0.9573937441546295 Top2: 0.9747126436781609 AUC: 0.9828450895063523 MCC: 0.7756917950429264\n",
      "Val Epoch: 424, Ave Loss: 1.3745256184318333 ACC: 0.9014700039729837 Top2: 0.7798165137614679 AUC: 0.8926524626095926 MCC: 0.4906086409230963\n",
      "Train Epoch: 425, Ave Loss: 0.23828896640229869 ACC: 0.9635248882884755 Top2: 0.9839080459770115 AUC: 0.9903061814791705 MCC: 0.807129735208148\n",
      "Val Epoch: 425, Ave Loss: 1.4569281555045772 ACC: 0.9086213746523639 Top2: 0.8256880733944955 AUC: 0.8983964027849407 MCC: 0.5083343557749996\n",
      "Train Epoch: 426, Ave Loss: 0.21321840203141637 ACC: 0.965707159929336 Top2: 0.9862068965517241 AUC: 0.9920336437147594 MCC: 0.8208708253123805\n",
      "Val Epoch: 426, Ave Loss: 1.566742259995967 ACC: 0.9129916567342073 Top2: 0.8532110091743119 AUC: 0.9049042676637442 MCC: 0.5243048607726423\n",
      "Train Epoch: 427, Ave Loss: 0.20514808931035716 ACC: 0.9665385015068066 Top2: 0.9862068965517241 AUC: 0.9925787005177037 MCC: 0.8245567126704186\n",
      "Val Epoch: 427, Ave Loss: 1.571131764644508 ACC: 0.9149781485895908 Top2: 0.8348623853211009 AUC: 0.9044884605466736 MCC: 0.5373028960036114\n",
      "Train Epoch: 428, Ave Loss: 0.20056899594557376 ACC: 0.966850254598358 Top2: 0.9862068965517241 AUC: 0.9927409914594393 MCC: 0.8248879015077616\n",
      "Val Epoch: 428, Ave Loss: 1.6174944502482012 ACC: 0.911005164878824 Top2: 0.8256880733944955 AUC: 0.9040573427024239 MCC: 0.5284157595780545\n",
      "Train Epoch: 429, Ave Loss: 0.19781829591606792 ACC: 0.9662267484152551 Top2: 0.9862068965517241 AUC: 0.9930135198609114 MCC: 0.8240651776132603\n",
      "Val Epoch: 429, Ave Loss: 1.6539027715566965 ACC: 0.9125943583631307 Top2: 0.8532110091743119 AUC: 0.906747195719443 MCC: 0.5284223417726511\n",
      "Train Epoch: 430, Ave Loss: 0.19755070856022075 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9929717767581091 MCC: 0.8318638249288067\n",
      "Val Epoch: 430, Ave Loss: 1.6145023586531009 ACC: 0.9153754469606674 Top2: 0.8440366972477065 AUC: 0.906981691593605 MCC: 0.5388129380573912\n",
      "Train Epoch: 431, Ave Loss: 0.1948295031616567 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9931166793846586 MCC: 0.8273281186715986\n",
      "Val Epoch: 431, Ave Loss: 1.5780936496215072 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9057076779267664 MCC: 0.5341026646196773\n",
      "Train Epoch: 432, Ave Loss: 0.19483681722366172 ACC: 0.967577678478645 Top2: 0.9862068965517241 AUC: 0.9929330989031763 MCC: 0.8306487501033906\n",
      "Val Epoch: 432, Ave Loss: 1.7029573468243384 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9038470216606498 MCC: 0.5369057809798204\n",
      "Train Epoch: 433, Ave Loss: 0.19344451066978904 ACC: 0.9667463369011743 Top2: 0.9862068965517241 AUC: 0.9932046241326185 MCC: 0.8271144908201268\n",
      "Val Epoch: 433, Ave Loss: 1.7410207950840373 ACC: 0.9082240762812872 Top2: 0.8532110091743119 AUC: 0.905712512893244 MCC: 0.4830184627278952\n",
      "Train Epoch: 434, Ave Loss: 0.19352205187050747 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9932033980334707 MCC: 0.8277035328114737\n",
      "Val Epoch: 434, Ave Loss: 1.7584893276433406 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9040210804538422 MCC: 0.5186117432374951\n",
      "Train Epoch: 435, Ave Loss: 0.19228620408955135 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9931487808896174 MCC: 0.8280861579293264\n",
      "Val Epoch: 435, Ave Loss: 1.764626350910392 ACC: 0.9125943583631307 Top2: 0.8348623853211009 AUC: 0.9056133960804538 MCC: 0.5200105123722258\n",
      "Train Epoch: 436, Ave Loss: 0.19084954619924574 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9932419086930653 MCC: 0.8291779435468799\n",
      "Val Epoch: 436, Ave Loss: 1.7582379665497825 ACC: 0.9137862534763608 Top2: 0.8440366972477065 AUC: 0.9052274045899948 MCC: 0.5286935237121229\n",
      "Train Epoch: 437, Ave Loss: 0.19197279065726272 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9933401080884423 MCC: 0.8313859248176284\n",
      "Val Epoch: 437, Ave Loss: 1.7657550466491214 ACC: 0.912197059992054 Top2: 0.8348623853211009 AUC: 0.9048148207839093 MCC: 0.5199161978331617\n",
      "Train Epoch: 438, Ave Loss: 0.19225385463125588 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9931954283890105 MCC: 0.8279578222241605\n",
      "Val Epoch: 438, Ave Loss: 1.7396002139066027 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9063015729757606 MCC: 0.5271911980287856\n",
      "Train Epoch: 439, Ave Loss: 0.19121044394274422 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9934039209759036 MCC: 0.8306268295495346\n",
      "Val Epoch: 439, Ave Loss: 1.7482531886532318 ACC: 0.912197059992054 Top2: 0.8532110091743119 AUC: 0.907399110366168 MCC: 0.5326262888691773\n",
      "Train Epoch: 440, Ave Loss: 0.192613509771008 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9931814397123706 MCC: 0.8311006171726504\n",
      "Val Epoch: 440, Ave Loss: 1.744236869273113 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9056996196493037 MCC: 0.5326262888691773\n",
      "Train Epoch: 441, Ave Loss: 0.19154409330782438 ACC: 0.9673698430842772 Top2: 0.9839080459770115 AUC: 0.9933383246715002 MCC: 0.8298241324786965\n",
      "Val Epoch: 441, Ave Loss: 1.7740347587580658 ACC: 0.9114024632499007 Top2: 0.8348623853211009 AUC: 0.9060719120680762 MCC: 0.5240936644687275\n",
      "Train Epoch: 442, Ave Loss: 0.19167247599039405 ACC: 0.9666424192039904 Top2: 0.9885057471264368 AUC: 0.993266319212461 MCC: 0.8254286139005145\n",
      "Val Epoch: 442, Ave Loss: 1.786578305699608 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9058051830840639 MCC: 0.5269713801180247\n",
      "Train Epoch: 443, Ave Loss: 0.19152352365691433 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9932040110830446 MCC: 0.8297838263146962\n",
      "Val Epoch: 443, Ave Loss: 1.7539003667201158 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.906503029912326 MCC: 0.526949589880808\n",
      "Train Epoch: 444, Ave Loss: 0.19186912676304846 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9932502127282021 MCC: 0.8285667688378635\n",
      "Val Epoch: 444, Ave Loss: 1.7772621344548674 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.904931665807117 MCC: 0.5368707597402009\n",
      "Train Epoch: 445, Ave Loss: 0.19043121543557245 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.993320657697417 MCC: 0.8277362111884194\n",
      "Val Epoch: 445, Ave Loss: 1.7705257857023335 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9050380350696237 MCC: 0.5227361838734625\n",
      "Train Epoch: 446, Ave Loss: 0.1904697757878773 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9933620664095425 MCC: 0.8275696079225937\n",
      "Val Epoch: 446, Ave Loss: 1.7698924330620809 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9064498452810726 MCC: 0.5326384343964145\n",
      "Train Epoch: 447, Ave Loss: 0.19087107579946963 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.99335532286423 MCC: 0.8309809176943518\n",
      "Val Epoch: 447, Ave Loss: 1.8258812534126214 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9050146660649818 MCC: 0.5212365394845138\n",
      "Train Epoch: 448, Ave Loss: 0.19085768917344342 ACC: 0.9673698430842772 Top2: 0.9862068965517241 AUC: 0.9932386762498575 MCC: 0.8293036101461554\n",
      "Val Epoch: 448, Ave Loss: 1.783823312693831 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9055175025786488 MCC: 0.5256150789832389\n",
      "Train Epoch: 449, Ave Loss: 0.18893946574876327 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9935030678115312 MCC: 0.8313519149825783\n",
      "Val Epoch: 449, Ave Loss: 1.7942811193541341 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9070405170190821 MCC: 0.534072892709803\n",
      "Train Epoch: 450, Ave Loss: 0.19102806404883002 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9932751805653922 MCC: 0.8296565239666867\n",
      "Val Epoch: 450, Ave Loss: 1.7786178073419325 ACC: 0.9106078665077473 Top2: 0.8348623853211009 AUC: 0.9038728081485301 MCC: 0.5226924755700372\n",
      "Train Epoch: 451, Ave Loss: 0.18892354975977121 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9933485235871381 MCC: 0.8323423902073791\n",
      "Val Epoch: 451, Ave Loss: 1.774962547522249 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9054651237751419 MCC: 0.5154857682354594\n",
      "Train Epoch: 452, Ave Loss: 0.19055377046898656 ACC: 0.967577678478645 Top2: 0.9839080459770115 AUC: 0.9933670822696923 MCC: 0.8313043849198851\n",
      "Val Epoch: 452, Ave Loss: 1.859371187171721 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.905990523465704 MCC: 0.5111816740013708\n",
      "Train Epoch: 453, Ave Loss: 0.18981829876113937 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9932843205772208 MCC: 0.8269600342734261\n",
      "Val Epoch: 453, Ave Loss: 1.8097429785411137 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9050170835482207 MCC: 0.5256975938758675\n",
      "Train Epoch: 454, Ave Loss: 0.18957153250905953 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9933700917857822 MCC: 0.8326002886792823\n",
      "Val Epoch: 454, Ave Loss: 1.8289677882932431 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9058793192367199 MCC: 0.511349889644648\n",
      "Train Epoch: 455, Ave Loss: 0.1892322908645967 ACC: 0.9682011846617479 Top2: 0.9862068965517241 AUC: 0.9934033636581092 MCC: 0.8339080583670606\n",
      "Val Epoch: 455, Ave Loss: 1.843898745455298 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9045102178958226 MCC: 0.5142863239471283\n",
      "Train Epoch: 456, Ave Loss: 0.18720366754399417 ACC: 0.9685129377532994 Top2: 0.9862068965517241 AUC: 0.9935795875447053 MCC: 0.8327260101041362\n",
      "Val Epoch: 456, Ave Loss: 1.723383767144279 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9070260121196493 MCC: 0.5226924755700372\n",
      "Train Epoch: 457, Ave Loss: 0.19008766196772933 ACC: 0.9680972669645641 Top2: 0.9885057471264368 AUC: 0.993398682188636 MCC: 0.8351472858582749\n",
      "Val Epoch: 457, Ave Loss: 1.862724644551244 ACC: 0.9129916567342073 Top2: 0.8532110091743119 AUC: 0.9061960095410004 MCC: 0.5229131640968384\n",
      "Train Epoch: 458, Ave Loss: 0.18950595124314837 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9933799005789641 MCC: 0.8259893196439358\n",
      "Val Epoch: 458, Ave Loss: 1.7581406410056284 ACC: 0.912197059992054 Top2: 0.8532110091743119 AUC: 0.9067649239298609 MCC: 0.5312118210278248\n",
      "Train Epoch: 459, Ave Loss: 0.18930608543440958 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9933757764272853 MCC: 0.8261087694360952\n",
      "Val Epoch: 459, Ave Loss: 1.7791345865659483 ACC: 0.9117997616209773 Top2: 0.8348623853211009 AUC: 0.9055312016503353 MCC: 0.5297929745508286\n",
      "Train Epoch: 460, Ave Loss: 0.1894518869736009 ACC: 0.9674737607814611 Top2: 0.9885057471264368 AUC: 0.9934075435415672 MCC: 0.8303009000655305\n",
      "Val Epoch: 460, Ave Loss: 1.8346451785968416 ACC: 0.9106078665077473 Top2: 0.8532110091743119 AUC: 0.9065860301701908 MCC: 0.5140259328072316\n",
      "Train Epoch: 461, Ave Loss: 0.1894624944250147 ACC: 0.966850254598358 Top2: 0.9862068965517241 AUC: 0.993340832601575 MCC: 0.8269125656733889\n",
      "Val Epoch: 461, Ave Loss: 1.8850230388806426 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9055102501289324 MCC: 0.5256975938758675\n",
      "Train Epoch: 462, Ave Loss: 0.18903717889381938 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9934492866443695 MCC: 0.831234794665254\n",
      "Val Epoch: 462, Ave Loss: 1.8491248495462527 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9048196557503868 MCC: 0.5226877806064516\n",
      "Train Epoch: 463, Ave Loss: 0.19055397870020235 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9934369141893333 MCC: 0.8309809176943518\n",
      "Val Epoch: 463, Ave Loss: 1.7611171053434467 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9057366877256319 MCC: 0.5226649149549655\n",
      "Train Epoch: 464, Ave Loss: 0.19212477590719962 ACC: 0.9685129377532994 Top2: 0.9885057471264368 AUC: 0.9932777442272466 MCC: 0.833925158944474\n",
      "Val Epoch: 464, Ave Loss: 1.7749037948079947 ACC: 0.9137862534763608 Top2: 0.8623853211009175 AUC: 0.9068898272305311 MCC: 0.5369656619330004\n",
      "Train Epoch: 465, Ave Loss: 0.18932874773351716 ACC: 0.9674737607814611 Top2: 0.9839080459770115 AUC: 0.993466730691335 MCC: 0.8297838263146962\n",
      "Val Epoch: 465, Ave Loss: 1.8130090635464062 ACC: 0.9106078665077473 Top2: 0.8623853211009175 AUC: 0.9071895951521403 MCC: 0.5111388719542006\n",
      "Train Epoch: 466, Ave Loss: 0.1893511762653067 ACC: 0.9679933492673802 Top2: 0.9862068965517241 AUC: 0.9933925516928974 MCC: 0.8321950343044581\n",
      "Val Epoch: 466, Ave Loss: 1.784372478333528 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9058664259927798 MCC: 0.5198029986473033\n",
      "Train Epoch: 467, Ave Loss: 0.1890454290902611 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9933542639604207 MCC: 0.8325186414422492\n",
      "Val Epoch: 467, Ave Loss: 1.8287955197002543 ACC: 0.9125943583631307 Top2: 0.8440366972477065 AUC: 0.9071219056214544 MCC: 0.5242126193427202\n",
      "Train Epoch: 468, Ave Loss: 0.1887438540211948 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9934404810232176 MCC: 0.8313827643807531\n",
      "Val Epoch: 468, Ave Loss: 1.8149235439504492 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9056915613718411 MCC: 0.5241373471271298\n",
      "Train Epoch: 469, Ave Loss: 0.18816077376816503 ACC: 0.9685129377532994 Top2: 0.9862068965517241 AUC: 0.9935531149494702 MCC: 0.8364968710758474\n",
      "Val Epoch: 469, Ave Loss: 1.866200387058873 ACC: 0.9129916567342073 Top2: 0.8348623853211009 AUC: 0.9060517663744198 MCC: 0.5243048607726423\n",
      "Train Epoch: 470, Ave Loss: 0.18864835785535805 ACC: 0.9692403616335862 Top2: 0.9862068965517241 AUC: 0.9934211420957513 MCC: 0.8378285028240902\n",
      "Val Epoch: 470, Ave Loss: 1.7985990716185762 ACC: 0.9129916567342073 Top2: 0.8440366972477065 AUC: 0.9065046415678184 MCC: 0.5354714937567187\n",
      "Train Epoch: 471, Ave Loss: 0.1882187504129854 ACC: 0.9680972669645641 Top2: 0.9862068965517241 AUC: 0.9934797719277244 MCC: 0.8347392750055626\n",
      "Val Epoch: 471, Ave Loss: 1.854911651186264 ACC: 0.912197059992054 Top2: 0.8532110091743119 AUC: 0.9057947073233625 MCC: 0.524147265121951\n",
      "Train Epoch: 472, Ave Loss: 0.18803188906954935 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9934345734545968 MCC: 0.8294100565754846\n",
      "Val Epoch: 472, Ave Loss: 1.895411564336052 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.9061919804022692 MCC: 0.5212365394845138\n",
      "Train Epoch: 473, Ave Loss: 0.18872036566990538 ACC: 0.9677855138730126 Top2: 0.9862068965517241 AUC: 0.9933820183865828 MCC: 0.8322544067315276\n",
      "Val Epoch: 473, Ave Loss: 1.8751295055086519 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9068503416709646 MCC: 0.5198085745236014\n",
      "Train Epoch: 474, Ave Loss: 0.1878383136780188 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9934981634149403 MCC: 0.8311277067611339\n",
      "Val Epoch: 474, Ave Loss: 1.8334429985274538 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9069333419288292 MCC: 0.5156923969794759\n",
      "Train Epoch: 475, Ave Loss: 0.1880767881197016 ACC: 0.9686168554504833 Top2: 0.9862068965517241 AUC: 0.9934967143886748 MCC: 0.8346458255144424\n",
      "Val Epoch: 475, Ave Loss: 1.8586860450065505 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9061275141825683 MCC: 0.5227361838734625\n",
      "Train Epoch: 476, Ave Loss: 0.18797727780767354 ACC: 0.9678894315701964 Top2: 0.9862068965517241 AUC: 0.9935332187042094 MCC: 0.8314646623725322\n",
      "Val Epoch: 476, Ave Loss: 1.8503225111622719 ACC: 0.9125943583631307 Top2: 0.8532110091743119 AUC: 0.9072371389891697 MCC: 0.5284223417726511\n",
      "Train Epoch: 477, Ave Loss: 0.18776220857307513 ACC: 0.9685129377532994 Top2: 0.9862068965517241 AUC: 0.9934351307723912 MCC: 0.8341544924984485\n",
      "Val Epoch: 477, Ave Loss: 1.8341902474066372 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9067762055183084 MCC: 0.5283710878130083\n",
      "Train Epoch: 478, Ave Loss: 0.1889110712215559 ACC: 0.9684090200561155 Top2: 0.9862068965517241 AUC: 0.9934554728718876 MCC: 0.8357611318155225\n",
      "Val Epoch: 478, Ave Loss: 1.857856160972406 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.906552991232594 MCC: 0.5127644370100667\n",
      "Train Epoch: 479, Ave Loss: 0.188529565044625 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9934901380387006 MCC: 0.8273894418397205\n",
      "Val Epoch: 479, Ave Loss: 1.8453703708903315 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.9056391825683342 MCC: 0.5140506170697363\n",
      "Train Epoch: 480, Ave Loss: 0.18849040462288882 ACC: 0.9684090200561155 Top2: 0.9862068965517241 AUC: 0.9935505512876158 MCC: 0.8340169080641701\n",
      "Val Epoch: 480, Ave Loss: 1.8600244982178558 ACC: 0.9133889551052841 Top2: 0.8440366972477065 AUC: 0.9057407168643631 MCC: 0.5258074743467643\n",
      "Train Epoch: 481, Ave Loss: 0.18855023285030273 ACC: 0.9670580899927258 Top2: 0.9862068965517241 AUC: 0.9933530935930523 MCC: 0.8269681202973856\n",
      "Val Epoch: 481, Ave Loss: 1.860885616229231 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.9049501998452811 MCC: 0.5140506170697363\n",
      "Train Epoch: 482, Ave Loss: 0.18817422804607384 ACC: 0.9692403616335862 Top2: 0.9862068965517241 AUC: 0.9934796604641655 MCC: 0.8371794149193279\n",
      "Val Epoch: 482, Ave Loss: 1.8544246354685203 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.905403880866426 MCC: 0.5212299752541836\n",
      "Train Epoch: 483, Ave Loss: 0.18802842979262763 ACC: 0.9669541722955419 Top2: 0.9862068965517241 AUC: 0.9934292789355497 MCC: 0.8267390902801441\n",
      "Val Epoch: 483, Ave Loss: 1.8823843796699586 ACC: 0.911005164878824 Top2: 0.8440366972477065 AUC: 0.9057640858690047 MCC: 0.5212299752541836\n",
      "Train Epoch: 484, Ave Loss: 0.1871692891894021 ACC: 0.9684090200561155 Top2: 0.9862068965517241 AUC: 0.9935744044892171 MCC: 0.8348694740002786\n",
      "Val Epoch: 484, Ave Loss: 1.8900848912694612 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9060412906137185 MCC: 0.5156923969794759\n",
      "Train Epoch: 485, Ave Loss: 0.18694940605166377 ACC: 0.9682011846617479 Top2: 0.9862068965517241 AUC: 0.9936277955339227 MCC: 0.8319904366940958\n",
      "Val Epoch: 485, Ave Loss: 1.894942951518523 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9060711062403302 MCC: 0.5198085745236014\n",
      "Train Epoch: 486, Ave Loss: 0.18724890398716013 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9935369527334319 MCC: 0.8304959765896871\n",
      "Val Epoch: 486, Ave Loss: 1.8872611428340853 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9066408264569366 MCC: 0.5111388719542006\n",
      "Train Epoch: 487, Ave Loss: 0.18848224347682596 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9933520904210223 MCC: 0.8308726294182416\n",
      "Val Epoch: 487, Ave Loss: 1.8918260040448607 ACC: 0.911005164878824 Top2: 0.8348623853211009 AUC: 0.9053031523981434 MCC: 0.5083148392167022\n",
      "Train Epoch: 488, Ave Loss: 0.18761357546458898 ACC: 0.9686168554504833 Top2: 0.9862068965517241 AUC: 0.9934909182836128 MCC: 0.83487672392383\n",
      "Val Epoch: 488, Ave Loss: 1.8758508117235755 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9057665033522435 MCC: 0.5111388719542006\n",
      "Train Epoch: 489, Ave Loss: 0.18740972140879203 ACC: 0.9663306661124389 Top2: 0.9862068965517241 AUC: 0.993624563090715 MCC: 0.8226939386949005\n",
      "Val Epoch: 489, Ave Loss: 1.783835917542409 ACC: 0.912197059992054 Top2: 0.8440366972477065 AUC: 0.9065868359979371 MCC: 0.524147265121951\n",
      "Train Epoch: 490, Ave Loss: 0.18987162902035506 ACC: 0.9690325262392185 Top2: 0.9862068965517241 AUC: 0.9935266980860147 MCC: 0.8381316160001722\n",
      "Val Epoch: 490, Ave Loss: 1.7318756663885784 ACC: 0.9086213746523639 Top2: 0.8348623853211009 AUC: 0.9059373388344507 MCC: 0.5187127940281329\n",
      "Train Epoch: 491, Ave Loss: 0.18913152630224525 ACC: 0.9682011846617479 Top2: 0.9862068965517241 AUC: 0.9933504741994186 MCC: 0.8335325516370817\n",
      "Val Epoch: 491, Ave Loss: 1.8161581740787702 ACC: 0.911005164878824 Top2: 0.8440366972477065 AUC: 0.906413583032491 MCC: 0.5154857682354594\n",
      "Train Epoch: 492, Ave Loss: 0.18790310113599593 ACC: 0.9674737607814611 Top2: 0.9862068965517241 AUC: 0.9934389205333932 MCC: 0.8295300165537822\n",
      "Val Epoch: 492, Ave Loss: 1.9502796811120038 ACC: 0.9114024632499007 Top2: 0.8532110091743119 AUC: 0.9051943656523981 MCC: 0.5098277776023405\n",
      "Train Epoch: 493, Ave Loss: 0.18756657249140354 ACC: 0.9690325262392185 Top2: 0.9862068965517241 AUC: 0.9935506070193953 MCC: 0.8359684220714778\n",
      "Val Epoch: 493, Ave Loss: 1.809264876702976 ACC: 0.9106078665077473 Top2: 0.8440366972477065 AUC: 0.9060388731304796 MCC: 0.5241373471271298\n",
      "Train Epoch: 494, Ave Loss: 0.1878027656563732 ACC: 0.9684090200561155 Top2: 0.9862068965517241 AUC: 0.9935243573512781 MCC: 0.8370999229598908\n",
      "Val Epoch: 494, Ave Loss: 1.8670627767444976 ACC: 0.9114024632499007 Top2: 0.8440366972477065 AUC: 0.9063193011861784 MCC: 0.5126763146093402\n",
      "Train Epoch: 495, Ave Loss: 0.1871478478526276 ACC: 0.9694481970279538 Top2: 0.9862068965517241 AUC: 0.9936140297844005 MCC: 0.8386019199540544\n",
      "Val Epoch: 495, Ave Loss: 1.863072972893652 ACC: 0.911005164878824 Top2: 0.8440366972477065 AUC: 0.9059832710159876 MCC: 0.5241037412296022\n",
      "Train Epoch: 496, Ave Loss: 0.187262290656776 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9934870727908313 MCC: 0.8284393063951723\n",
      "Val Epoch: 496, Ave Loss: 1.8439845652283946 ACC: 0.911005164878824 Top2: 0.8532110091743119 AUC: 0.9071251289324394 MCC: 0.5169213174994156\n",
      "Train Epoch: 497, Ave Loss: 0.1871760625912957 ACC: 0.9676815961758287 Top2: 0.9862068965517241 AUC: 0.9936454067762264 MCC: 0.8300050862704681\n",
      "Val Epoch: 497, Ave Loss: 1.8841450821390928 ACC: 0.9125943583631307 Top2: 0.8532110091743119 AUC: 0.9065755544094893 MCC: 0.5256150789832389\n",
      "Train Epoch: 498, Ave Loss: 0.18743650940519274 ACC: 0.9686168554504833 Top2: 0.9839080459770115 AUC: 0.9935791416904697 MCC: 0.8346458255144424\n",
      "Val Epoch: 498, Ave Loss: 1.866348340019112 ACC: 0.9117997616209773 Top2: 0.8440366972477065 AUC: 0.9060453197524496 MCC: 0.526949589880808\n",
      "Train Epoch: 499, Ave Loss: 0.1873393078021665 ACC: 0.9671620076899096 Top2: 0.9862068965517241 AUC: 0.9934992223187497 MCC: 0.8301034740541097\n",
      "Val Epoch: 499, Ave Loss: 1.9207637989350361 ACC: 0.9117997616209773 Top2: 0.8532110091743119 AUC: 0.9062919030428055 MCC: 0.511349889644648\n",
      "Train Epoch: 500, Ave Loss: 0.18797126679546922 ACC: 0.9672659253870934 Top2: 0.9862068965517241 AUC: 0.9935360052931814 MCC: 0.8269731227130067\n",
      "Val Epoch: 500, Ave Loss: 1.8494675102969635 ACC: 0.9106078665077473 Top2: 0.8532110091743119 AUC: 0.9056826972666322 MCC: 0.5169140675144476\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mol2y(mol):\n",
    "    _y = []\n",
    "    som = ['PRIMARY_SOM', \n",
    "           'SECONDARY_SOM', \n",
    "           'TERTIARY_SOM', \n",
    "          ]\n",
    "    result = []\n",
    "    for k in som:\n",
    "        try:\n",
    "            _res = mol.GetProp(k)\n",
    "            if ' ' in _res:\n",
    "                res = _res.split(' ')\n",
    "                for s in res:\n",
    "                    result.append(int(s))\n",
    "                # res = [int(temp) for temp in res]\n",
    "            else:\n",
    "                # res = [int(_res)]\n",
    "                result.append(int(_res))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for data in result:\n",
    "        _y.append(data)\n",
    "    _y = list(set(_y))\n",
    "\n",
    "    y = np.zeros(len(mol.GetAtoms()))\n",
    "    for i in _y:\n",
    "        y[i-1] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = Chem.SDMolSupplier('../../raw_database/2C9.sdf')\n",
    "cyp2c9 = []\n",
    "for mol in mols:\n",
    "    g = mol2graph(mol)\n",
    "    y = _mol2y(mol)\n",
    "    graph = from_networkx(g)\n",
    "    graph.feature = graph.feature.float()\n",
    "    label = torch.tensor(y, dtype=torch.float)\n",
    "    cyp2c9.append((graph, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = Chem.SDMolSupplier('../../raw_database/2D6.sdf')\n",
    "cyp2d6 = []\n",
    "for mol in mols:\n",
    "    g = mol2graph(mol)\n",
    "    y = _mol2y(mol)\n",
    "    graph = from_networkx(g)\n",
    "    graph.feature = graph.feature.float()\n",
    "    label = torch.tensor(y, dtype=torch.float)\n",
    "    cyp2d6.append((graph, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = Chem.SDMolSupplier('../../raw_database/3A4.sdf')\n",
    "cyp3a4 = []\n",
    "for mol in mols:\n",
    "    g = mol2graph(mol)\n",
    "    y = _mol2y(mol)\n",
    "    graph = from_networkx(g)\n",
    "    graph.feature = graph.feature.float()\n",
    "    label = torch.tensor(y, dtype=torch.float)\n",
    "    cyp3a4.append((graph, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_set):\n",
    "    model.eval()\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    with torch.no_grad():\n",
    "        for mol, target in test_set:\n",
    "            mol, target = mol.to(device), target.to(device)\n",
    "            output = model(mol)\n",
    "            # squeeze\n",
    "            output = torch.squeeze(output)\n",
    "            # tracking\n",
    "            top2n += top2(output, target)\n",
    "            all_pred.append(np.rint(torch.sigmoid(output).cpu().detach().numpy()))\n",
    "            all_pred_raw.append(torch.sigmoid(output).cpu().detach().numpy())\n",
    "            all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    print(f'ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(test_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(\"cuda\")\n",
    "model.load_state_dict(torch.load(args['save_path']))\n",
    "# model.load_state_dict(torch.load('./model/model80'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9153084052412912 Top2: 0.8308823529411765 AUC: 0.8722380816443048 MCC: 0.48071529885557335\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9255989911727617 Top2: 0.8539823008849557 AUC: 0.9386742021169395 MCC: 0.5366507631346034\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", cyp2c9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9307528409090909 Top2: 0.8666666666666667 AUC: 0.9297754401497325 MCC: 0.5498012637959943\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", cyp2d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9337547408343868 Top2: 0.8610526315789474 AUC: 0.9291867990113111 MCC: 0.5497159854321518\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", cyp3a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f664d902b5a3a7a580bd8fcd8c5850e6260e2b7a5b85c9c0421d5aff64b3914b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
